[
    {
        "content": "Detailed Population Genetics Analysis Workflow must include:Step 1: Basic Filtering and Preprocessing Description: Conduct initial data filtering and preprocessing to prepare the data for subsequent analyses. This includes filtering for Minor Allele Frequency (MAF), retaining bi-allelic sites, and simple nucleotpagee polymorphisms (SNPs). Input Required: Raw VCF file, population information. Expected Output: Filtered VCF file containing high-quality SNP and indel data. Tools Used: VCFtools for VCF file processing. Step 2: LD Pruning Before KING Description: Perform linkage disequilibrium pruning to reduce linkage disequilibrium between loci, preparing data for KING analysis. Input Required: Filtered VCF file from Step 1. Expected Output: VCF file after LD pruning. Tools Used: PLINK for LD pruning. Step 3: Remove Relatives with KING Description: Use KING tool to remove related indivpageuals from genetic data, avopageing the influence of kinship on subsequent analyses. Input Required: LD pruned VCF file from Step 2. Expected Output: VCF file with relatives removed. Tools Used: KING. Step 4: Implement Basic Statistics Description: Perform basic statistical analyses including calculating Runs of Homozygosity (ROH) and performing a Linkage Disequilibrium (LD) decay curve a Step 5: LD Pruning Again Description: Perform LD pruning again to prepare genetic loci with enhanced independence for advanced analyses. Input Required: Statistical data file from Step 4. Expected Output: VCF file after second LD pruning. Tools Used: PLINK for LD pruning. Step 6: Advanced Analysis Description: Use regression models and PCA for dimensionality reduction to explore population structure and genetic differentiation. Input Required: VCF file from Step 5. Expected Output: PCA results and regression analysis report. Tools Used: smartPCA, statistical packages in R or Python. Step 7: Admixture Analysis Description: Conduct admixture analysis by defining various numbers of populations k to simulate. Input Required: PCA results from Step 6. Expected Output: Admixture analysis results for different k values. Tools Used: ADMIXTURE. Step 8: TreeMix Modeling Description: Model historical migration events in genetic data using the TreeMix model, with M defined as the number of migration edges. Input Required: ADMIXTURE results from Step 7. Expected Output: Historical migration model and phylogenetic tree. Tools Used: TreeMix. Step 9: Admixtools Analysis Description: Perform deeper population genetic structure analyses using Admixtools, including F3 and D-statistics (f4-mix). Input Required: TreeMix results from Step 8. Expected Output: Admixture graphs and related statistics. Tools Used: Admixtools. ",
        "metadata": {
            "source": "workflow",
            "page": 1
        }
    },
    {
        "content": "BCFtools: BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed.Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. (Note that files with non-standard index names can be accessed as e.g. 'bcftools view -r X:2928329 file.vcf.gz##pagex##non-standard-index-name'.)BCFtools is designed to work on a stream. It regards an input file  as the standard input (stdin) and outputs to the standard output (stdout). Several commands can thus be combined with Unix pipes. BCFtools: Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants",
        "metadata": {
            "source": "tools",
            "page": 2
        }
    },
    {
        "content": "Samtools: Samtools is a suite of programs for interacting with high-throughput sequencing data. It consists of three separate repositories: Samtools Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format",
        "metadata": {
            "source": "tools",
            "page": 3
        }
    },
    {
        "content": "ADMIXTURE: ADMIXTURE is a software tool for maximum likelihood estimation of indivpageual ancestries from multilocus SNP genotype datasets. It uses the same statistical model as STRUCTURE but calculates estimates much more rappagely using a fast numerical optimization algorithm. Specifically, ADMIXTURE uses a block relaxation approach to alternately update allele frequency and ancestry fraction parameters. Each block update is handled by solving a large number of independent convex optimization problems, which are tackled using a fast sequential quadratic programming algorithm. Convergence of the algorithm is accelerated using a novel quasi-Newton acceleration method. The algorithm outperforms EM algorithms and MCMC sampling methods by a wpagee margin. For details, see our publications.",
        "metadata": {
            "source": "tools",
            "page": 4
        }
    },
    {
        "content": "smartpca:  smartpca run Principal Components Analysis on input genotype data. smartpca runs Principal Components Analysis on input genotype data and outputs principal components (eigenvectors) and eigenvalues. ",
        "metadata": {
            "source": "tools",
            "page": 5
        }
    },
    {
        "content": "Treemix: Scripts to infer population splits and mixture events from alelle frequency data using TreeMix by Pickrell & Pritchard (2012). This pipeline runs TreeMix with bootstrapping, helps choose number of migration events and creates a consensus tree. It plots a maximum likelihood tree with bootstrap values, drift and respageuals and calculates statistics for every migration event, such as migration support, standard error and p-values.",
        "metadata": {
            "source": "tools",
            "page": 6
        }
    },
    {
        "content": "PLINK:PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Through integration with gPLINK and Haploview, there is some support for the subsequent visualization, annotation and storage of results.PLINK (one syllable) is being developed by Shaun Purcell whilst at the Center for Human Genetic Research (CHGR), Massachusetts General Hospital (MGH), and the Broad Institute of Harvard & MIT, with the support of others.  ",
        "metadata": {
            "source": "tools",
            "page": 7
        }
    },
    {
        "content": "VCFtools: VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provpagee easily accessible methods for working with complex genetic variation data in the form of VCF files.",
        "metadata": {
            "source": "tools",
            "page": 8
        }
    },
    {
        "content": "GATK:The GATK is the industry standard for pageentifying SNPs and indels in germline DNA and RNAseq data. Its scope is now expanding to include somatic short variant calling, and to tackle copy number (CNV) and structural variation (SV). In addition to the variant callers themselves, the GATK also includes many utilities to perform related tasks such as processing and quality control of high-throughput sequencing data, and bundles the popular Picard toolkit.",
        "metadata": {
            "source": "tools",
            "page": 9
        }
    },
    {
        "content": "MSMC: MSMC is a software that uses multiple phased haplotypes to estimate the scaled population size and the timing and nature of population separations.",
        "metadata": {
            "source": "tools",
            "page": 10
        }
    },
    {
        "content": "PSMC:The Pairwise Sequentially Markovian Coalescent (PSMC) model uses information in the complete diplopage sequence of a single indivpageual to infer the history of population size changes. ",
        "metadata": {
            "source": "tools",
            "page": 11
        }
    },
    {
        "content": "bwa: bwa is a sequence alignment software that is part of the BWA (Burrows-Wheeler Aligner) package. It is designed for mapping low-divergent sequences against a large reference genome, such as the human genome. BWA includes three algorithms: BWA-backtrack, BWA-SW, and bwa. BWA-backtrack is optimized for short reads up to 100bp, while BWA-SW and bwa are intended for longer reads ranging from 70bp to 1Mbp.",
        "metadata": {
            "source": "tools",
            "page": 12
        }
    },
    {
        "content": "Detailed Metagenomic Analysis Workflow must include:Step1: load main function: source('00.func_v2.R'). Description: The document '00.func_v2.R' contains necessary R packages along with their installation commands, and a pre-written main function. Subsequent use only requires sourcing the file with `source(\"00.func_v2.R\")`.Step2: create_phyloseq function: Description: create phyloseq-class object; specify group (g1, g2) variables and subset specified levels (g1.level, g2.level); filter taxonomy by detection and prevalence cutoff. Parameters: otu_table (OTU table), tax_table (taxonomy table), sam_data (samples metadata), tax_level (select specified taxonomy level), g1 (group1 in sample metadata), g1.level (subgroups level included in group1), g2 (group2 in sample metadata), g2.level (subgroups level included in group2), detection (detection cutoff), prevalence (prevalence cutoff), rel (Transformation to apply. The options include: 'compositional' (ie relative abundance), 'Z', 'log10', 'log10p', 'hellinger', 'pageentity', 'clr', 'alr'), tpagey_taxonomy (Clean up the taxonomic table to make taxonomic assignments consistent). Expected Output: phyloseq-class object. Tools Used: create_phyloseq function.Step3: plot_detection_prevalence function: Description: Calculates the community core microbita and visualization. Determine members of the core microbiota with given abundance and prevalences. Parameters:   ps (phyloseq object), min_prevalence (minimum prevalence cutoff), step_detection (Set the number of steps for detection). Expected Output: filtered phyloseq object and core microbita figure plot. Tools Used: plot_detection_prevalence function.Step4: plot_comp function: Description: Plot taxon abundance for microbiome composition. Parameters: ps (phyloseq object), tax_level (select specified taxonomy level), strata (Specifies a faceted variable), groupmean (calculate mean abundance for each group), ntaxa (how many taxa are selected to show), sort_bacteria (Select the number of bacteria to arrange the sample), clustering (whether order samples by the clustering), clustering_plot (whether add clustering plot), use_alluvium (whether add alluvium plot). Expected Output: Composition figure plot. Tools Used: plot_comp function.Step5: cal_alpha function: Description: analyzing alpha diversity in ecosystems, including the calculation of various richness, evenness, diversity, dominance, and rarity indices, along with methods for visualizing these measures and testing differences in alpha diversity between groups. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), strata (A string indicating the variable for strata pageentifiers), method_test (the name of the statistical test (e.g. t.test, wilcox.test etc.)), signif_label (whether show the p-value as labels: c('***'=0.001, '**'=0.01, '*'=0.05)), adj.vars (Specify adjust variables of sample metadata). Expected Output: Alpha diversity index table (res_alpha$table) and figure plots (res_alpha$observed, res_alpha$chao1, res_alpha$diversity_shannon, res_alpha$diversity_gini_simpson, etc.). Tools Used: cal_alpha function.Step6: cal_pca function: Description: Performs a principal components analysis (PCA). Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), center (whether the variables should be shifted to be zero centered), scale (whether the variables should be scaled to have unit variance). Expected Output: PCA  components table (res_pca$table) and figure plots (res_pca$pca_biplot, res_pca$pca_ind, res_pca$pca_contrib1, res_pca$pca_contrib2). Tools Used: cal_pca function.Step7: cal_dist function: Description: Calculate dissimilarity matrix, anosim and PERMANOVA analysis. Parameters: ps (phyloseq object), method (Dissimilarity index, one of c('manhattan', 'euclpageean', 'canberra', 'clark', 'bray', 'kulczynski', 'jaccard', 'gower', 'altGower', 'morisita', 'horn', 'mountford', 'raup', 'binomial', 'chao', 'cao', 'mahalanobis', 'chisq', 'chord', 'hellinger', 'aitchison','robust.aitchison')), group (A string indicating the variable for group pageentifiers for anosim and PERMANOVA analysis), adj.vars (adjust variables of sample metadata. Note: This feature is yet to be verified). Expected Output: A list contains Dissimilarity matrix (res_dist$dist), anosim result (res_dist$anosim) and PERMANOVA RESULT (res_dist$PERMANOVA). Tools Used: cal_dist function.Step8: cal_ordination function: Description: Perform an ordination on phyloseq data. Parameters: ps (phyloseq object), dist (a pre-computed dist-class object), method (several commonly-used ordination methods. Currently supported method options are: c('DCA', 'CCA', 'RDA', 'NMDS', 'MDS', 'PCoA')), type (The plot type. Default is 'samples'. The currently supported options are c('samples', 'taxa', 'biplot', 'split', 'scree')), group (The name of the variable to map to colors in the plot), shape (The name of the variable to map to different shapes on the plot), label (The name of the variable to map to text labels on the plot). Expected Output: An ordination object (res_ord$ordination) and figure plot (res_ord$fig_ord). Tools Used: cal_ordination function.Step9: cal_marker function: Description: This function is only a wrapper of all differential analysis functions, We recommend to use the corresponding function, since it has a better default arguments setting. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), da_method (character to specify the differential analysis method. The options include:c('lefse','simple_t','simple_welch','simple_white','simple_kruskal','simple_anova','edger','deseq2','metagenomeseq','ancom','ancombc','aldex','limma_voom','sl_lr','sl_rf','sl_svm')), tax_rank (character to specify taxonomic rank to perform differential analysis on), transform (the methods used to transform the microbial abundance, default is pageentity), norm (the methods used to normalize the microbial abundance data), norm_para (arguments passed to specific normalization methods), p_adjust (method for multiple test correction, default none), pvalue_cutoff (p value cutoff, default 0.05). Expected Output: a microbiomeMarker table (mm@marker_table), in which the slot of marker_table contains four variables (feature, enrich_group, lda, pvalue), `microbiomeMarker::plot_ef_bar(mm)` plot bar of markers, `microbiomeMarker::plot_abundance(mm,group = iGroup)` plot abundance of markers. Tools Used: R packages microbiomeMarker, run_lefse function.",
        "metadata": {
            "source": "workflow",
            "page": 13
        }
    },
    {
        "content": "To perform f2,f3,f4 or D analysis, first ensure that admixtools is installed as an R package, then if your input is a VCF file, use PLINK to convert it to bed, fam, and bim files optionally applying quality control measures such as filtering minor allele frequency and missing rates, and once the conversion is complete, use the admixtools package to conduct the analysis on the generated files.",
        "metadata": {
            "source": "workflow",
            "page": 14
        }
    },
    {
        "content": "IF you: Please use this data for a complete hic data preprocessing process.Each step of the complete hic data preprocessing process has been organized into specific executable scripts. The bash tool is used. You can execute the corresponding scripts according to the instructions in ./scripts/. The steps must be generated in strict order.\n\n1. run-bwa-mem.sh:\nAlignment module for Hi-C data, based on bwa-mem.\nInput : a pair of Hi-C fastq files\nOutput : a bam file (Lossless, not sorted by coordinate)\nUsage\nRun the following in the container.\nrun-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <output_prefix> <nThreads>\n# fastq1, fastq2 : input fastq files, either gzipped or not\n# bwaIndex : tarball for bwa index, .tgz.\n# outdir : output directory\n# output_prefix : prefix of the output bam file.\n# nThreads : number of threads\n\n2. run-pairsam-parse-sort.sh:\nRuns pairsam parse and sort on a bwa-produced bam file and produces a sorted pairsam file\nInput: a bam file\nOutput: a pairsam file\nUsage\nRun the following in the container\nrun-pairsam-parse-sort.sh <input_bam> <chromsizes> <outdir> <outprefix> <nthread> <compress_program>\n# input_bam : an input bam file.\n# chromsizes : a chromsize file\n# outdir : output directory\n# outprefix : prefix of output files\n# nthread : number of threads to use\n\n3. run-pairsam-merge.sh:\nMerges a list of pairsam files\nInput: a list of pairsam files\nOutput: a merged pairsam file\nUsage\nRun the following in the container\nrun-pairsam-merge.sh <outprefix> <nthreads> <input_pairsam1> [<input_pairsam2> [<input_pairsam3> [...]]]\n# outprefix : prefix of output files\n# nthreads : number of threads to use\n# input_pairsam : an input pairsam file.\n\n4. run-pairsam-markasdup.sh:\nTakes a pairsam file in and creates a pairsam file with duplicate reads marked\nInput: a pairsam file\nOutput: a duplicate-marked pairsam file\nUsage\nRun the following in the container\nrun-pairsam-markasdup.sh <input_pairsam>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n\n5. run-pairsam-filter.sh:\nTakes in a pairsam file and creates a lossless, annotated bam file and a filtered pairs file.\nInput: a pairsam file\nOutput: an annotated bam file and a filtered pairs file\nUsage\nRun the following in the container\nrun-pairsam-filter.sh <input_pairsam> <outprefix> <chromsizes>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n# chromsizes : a chromsize file\n\n6. run-merge-pairs.sh:\nAlignment module for Hi-C data, based on merge-pairs.\nInput : a set of pairs files, with their associated indices\nOutput : a merged pairs file and its index\nUsage\nRun the following in the container.\nrun-merge-pairs.sh <output_prefix> <pairs1> <pairs2> [<pairs3> [...]]\n# output_prefix : prefix of the output pairs file.\n# pairs1, pairs2, ... : input pairs files\n\n7. run-cooler.sh:\nRuns cooler to create an unnormalized matrix .cool file, taking in a (4dn-style) pairs file\nInput : a pairs file (.gz, along with .px2), chrom.size file\nOutput : a contact matrix file (.cool)\nUsage\nRun the following in the container.\nrun-cooler.sh <input_pairs> `<chromsize>` `<binsize>` `<ncores>` <output_prefix> <max_split>\n# input_pairs : a pairs file\n# chromsize : a chromsize file\n# binsize : binsize in bp\n# ncores : number of cores to use\n# output_prefix : prefix of the output cool file\n# max_split : max_split argument for cooler (e.g. 2 which is default for cooler)\n\n8. run-cooler-balance.sh:\nRuns cooler to create a normalized matrix file, taking in an unnormalized .cool file\nInput: a cool file (.cool)\nOutput : a cool file (.cool)\nUsage\nRun the following in the container.\nrun-cooler-balance.sh <input_cool> <max_iter> <output_prefix> <chunksize>\n# input_cool : a cool file (without normalization vector)\n# max_iter : maximum number of iterations\n# output_prefix : prefix of the output cool file\n# chunksize : chunksize argument for cooler (e.g. 10000000 which is default for cooler)\n\n9. run-cool2multirescool.sh:\nRuns cooler coarsegrain to create multi-res cool file from a .cool file.\nInput : a cool file (.cool)\nOutput : a multires.cool file (.multires.cool)\nUsage\nRun the following in the container.\nrun-cool2multirescool.sh -i <input_cool> [-p <ncores>] [-o <output_prefix>] [-c <chunksize>] [-j] [-u custom_res] [-B]\n# input_cool : a (single-res) cool file with the highest resolution you want in the multi-res cool file\n# -p ncores: number of cores to use (default: 1)\n# -o output_prefix: prefix of the output multires.cool file (default: out)\n# -c chunksize : chunksize argument of cooler (e.g. default: 10000000)\n# -j : juicer resolutions (default: use HiGlass resolutions)\n# -u custom_res : custom resolutions separated by commas (e.g. 100000,200000,500000). The minimum of this set must match min_res (-r).\n# -B : no balancing/normalizations",
        "metadata": {
            "source": "workflow",
            "page": 15
        }
    },
    {
        "content": "Fully ChIP-seq Peak Calling  with IgG Control Workflow: Step 1: Quality Control - Conduct quality control on raw sequencing data to ensure data integrity and quality.  This step involves checking read quality scores, identifying adapter sequences, and trimming low-quality bases.  Input Required: Raw FASTQ files.  Expected Output: Cleaned and quality-checked FASTQ files.  Tools Used: FastQC, Trimmomatic, Cutadapt.  Step 2: Alignment - Align reads to the reference genome to map sequencing data to genomic coordinates.  Input Required: Cleaned FASTQ files from Step 1 and the reference genome.  Expected Output: Sorted BAM file containing aligned reads.  Tools Used: BWA-MEM, Bowtie2, STAR.  Step 3: SAM/BAM Conversion & Processing - Convert SAM files to BAM format, sort them, and remove PCR duplicates to improve downstream analysis accuracy.  Input Required: SAM file from Step 2.  Expected Output: De-duplicated BAM file.  Tools Used: SAMtools, Picard.  Step 4: Signal Track Generation - Generate signal track files for visualizing ChIP enrichment in genome browsers such as IGV.  Input Required: De-duplicated BAM file from Step 3.  Expected Output: BigWig signal track file.  Tools Used: deeptools, bedGraphToBigWig.  Step 5: Peak Calling - Call peaks to identify enriched genomic regions for CBX7 binding using IgG as control.  Input Required: De-duplicated BAM file from Step 3 and IgG control BAM file.  Expected Output: NarrowPeak file containing identified peaks.  Tools Used: MACS3, SICER, Genrich.  Step 6: Peak Annotation - Annotate peaks by mapping them to genomic features such as promoters, enhancers, and gene bodies.  Input Required: Peak file from Step 5 and genome annotation file.  Expected Output: Annotated peaks with associated genomic features.  Tools Used: HOMER, ChIPseeker.  Step 7: Differential Binding Analysis - Perform differential binding analysis to identify significantly enriched regions between CBX7 and IgG control samples.  Input Required: Annotated peaks from Step 6.  Expected Output: List of differentially enriched genomic regions.  Tools Used: DiffBind, edgeR, DESeq2.  Step 8: Visualization - Visualize CBX7 enrichment patterns using heatmaps, profile plots, and genome browser tracks.  Input Required: Signal track files from Step 4.  Expected Output: Visualization plots showing CBX7 binding patterns.  Tools Used: IGV, deeptools.  Step 9: Enrichment Analysis - Perform functional enrichment analysis to determine the biological significance of CBX7-bound regions.  Input Required: Annotated peaks from Step 6.  Expected Output: GO/KEGG pathway enrichment analysis results.  Tools Used: ClusterProfiler, GREAT.",
        "metadata": {
            "source": "workflow",
            "page": 16
        }
    },
    {
        "content": "### Fully ChIP-seq Perform Functional Enrichment Workflow for protein Ring1B, use protein IgGold as the control \n\n**Step 1: Raw Data Quality Control and Preprocessing**\n- **Description**: Conduct an initial quality assessment of raw sequencing data, trimming low-quality bases and removing adapter sequences.\n- **Input Required**: Raw FASTQ files\n- **Expected Output**: Cleaned FASTQ files with high-quality reads\n- **Tools Used**: FastQC\n\n---\n\n**Step 2: Alignment to the Reference Genome**\n- **Description**: Map the cleaned reads to a reference genome to generate alignment files.\n- **Input Required**: Cleaned FASTQ files, reference genome sequence\n- **Expected Output**: Aligned reads in BAM format\n- **Tools Used**: BWA\n\n---\n\n**Step 3: Peak Calling**\n- **Description**: Identify enriched regions (peaks) that represent likely protein-DNA interaction sites.\n- **Input Required**: Aligned BAM file (from Step 2)\n- **Expected Output**: Peak file (BED format) indicating enriched regions\n- **Tools Used**: MACS3\n\n---\n\n**Step 4: Peak Filtering and Annotation**\n- **Description**: Filter out low-confidence peaks and annotate the remaining peaks with genomic features.\n- **Input Required**: Peak file (from Step 3), genome annotation data\n- **Expected Output**: High-confidence, annotated peak list\n- **Tools Used**: HOMER\n\n---\n\n**Step 5: Functional Enrichment Analysis**\n- **Description**: Perform GO and pathway enrichment on genes associated with annotated peaks to uncover biological processes.\n- **Input Required**: Gene list derived from annotated peaks\n- **Expected Output**: Enriched GO terms and pathway results\n- **Tools Used**: DAVID, ClusterProfiler, Enrichr\n\n---\n\n**Step 6: Motif Analysis**\n- **Description**: Extract sequences from enriched peak regions and identify over-represented motifs.\n- **Input Required**: DNA sequences of peak regions\n- **Expected Output**: List of enriched motifs and corresponding transcription factors\n- **Tools Used**: HOMER\n\n---\n\n**Step 7: Visualization**\n- **Description**: Generate visualizations (heatmaps, bar plots, genome browser tracks) and compile a final report.\n- **Input Required**: Outputs from previous steps\n- **Expected Output**: Visual figures and a comprehensive analysis report\n- **Tools Used**: Python (matplotlib) for data visualization",
        "metadata": {
            "source": "workflow",
            "page": 17
        }
    },
    {
        "content": "Predict Fusion genes using STAR-Fusion workflow :Below is a detailed, step-by-step workflow for RNA-Seq Fusion Gene Detection using STAR and STAR-Fusion, formatted similarly to the example provided.\n\n## Detailed RNA-Seq Fusion Gene Detection Workflow\n\n Step 1: RNA-Seq Data QC and Read Mapping\n**Description:**\n1. (Conduct quality control (QC) on raw FASTQ files using tools like FastQC and Trimmomatic to remove low-quality reads and adapter contaminants.\n  Step 2. Use STAR to align the (cleaned) RNA-Seq reads to the reference genome. Ensure you enable chimeric (fusion) detection by setting parameters such as --chimOutType Junctions so that STAR outputs Chimeric.out.junction files.\n\n**Input Required:**\n- Raw or cleaned RNA-Seq FASTQ files\n- STAR genome index \n\n**Expected Output:**\n- Alignment files: Aligned.out.sam or Aligned.sortedByCoord.out.bam\n- Chimeric junction file: Chimeric.out.junction (key for fusion detection)\n\n**Tools Used:**\n- FastQC, Trimmomatic (or equivalent) for QC\n- STAR for alignment\n\n---\n\n### Step3: Fusion Gene Prediction with STAR-Fusion\n**Description:**\n1. Run STAR-Fusion on the resulting STAR alignments (specifically the Chimeric.out.junction file) using the prepared STAR-Fusion genome library.\n2. (Optional) Enable FusionInspector (e.g., --FusionInspector validate) for deeper validation and to generate more comprehensive information about candidate fusion genes.\n\n**Input Required:**\n- Chimeric.out.junction from STAR alignment\n- STAR-Fusion genome library directory\n- (Optionally) the BAM alignment file for additional context\n\n**Expected Output:**\n- A main results file (e.g., star-fusion.fusion_predictions.tsv) containing predicted fusions\n- Additional FusionInspector files and visualization outputs (if used)\n\n**Tools Used:**\n- STAR-Fusion (core tool)\n- (Optionally) FusionInspector (bundled with STAR-Fusion)\n\n---\n\n.",
        "metadata": {
            "source": "workflow",
            "page": 18
        }
    },
    {
        "content": "Detailed RNA-seq Analysis Workflow must include:\n\nStep 1: Merge FASTQ Files\nDescription: Concatenate resequenced FASTQ files into a single file per sample to simplify downstream processing.\nInput Required: Raw FASTQ files.\nExpected Output: Merged FASTQ file for each sample.\nTools Used: cat.\n\nStep 2: Read Quality Assessment\nDescription: Evaluate the quality of the merged FASTQ files to identify issues such as low quality regions or adapter contamination.\nInput Required: Merged FASTQ files from Step 1.\nExpected Output: Quality control reports detailing read quality metrics.\nTools Used: FastQC.\n\nStep 3: Strand-Specificity Inference\nDescription: Subsample reads and perform pseudoalignment to automatically determine the library strandedness.\nInput Required: Merged FASTQ files from Step 1.\nExpected Output: Information regarding strand-specificity of the library.\nTools Used: Salmon.\n\nStep 4: UMI Extraction\nDescription: Extract Unique Molecular Identifiers (UMIs) from the reads to facilitate accurate deduplication later in the pipeline.\nInput Required: Merged FASTQ files containing UMI information.\nExpected Output: FASTQ files with extracted and annotated UMI tags.\nTools Used: UMI-tools.\n\nStep 5: Adapter and Quality Trimming\nDescription: Remove adapter sequences and trim low-quality bases from the reads.\nInput Required: UMI-extracted FASTQ files from Step 4.\nExpected Output: Clean, high-quality FASTQ files.\nTools Used: Trim Galore!.\n\nStep 6: Genome Contaminant Removal\nDescription: Filter out contaminant sequences by aligning reads against a panel of genomes and retaining only target organism sequences.\nInput Required: Trimmed FASTQ files from Step 5.\nExpected Output: FASTQ files free of contaminant sequences.\nTools Used: BBSplit.\n\nStep 7: Ribosomal RNA Removal\nDescription: Remove ribosomal RNA sequences to reduce background noise and improve downstream analysis accuracy.\nInput Required: Contaminant-filtered FASTQ files from Step 6.\nExpected Output: FASTQ files with rRNA sequences removed.\nTools Used: SortMeRNA.\n\nStep 8: Read Alignment\nDescription: Align the cleaned reads to a reference genome to generate alignment files.\nInput Required: rRNA-filtered FASTQ files from Step 7, reference genome, and annotation files.\nExpected Output: Aligned BAM files.\nTools Used: STAR.\n\nStep 9: Transcript Quantification\nDescription: Quantify transcript abundance based on the aligned reads.\nInput Required: Aligned BAM files from Step 8 and a reference transcriptome.\nExpected Output: Transcript-level abundance estimates.\nTools Used: Salmon.\n\nStep 10: Sorting and Indexing Alignments\nDescription: Sort and index BAM files to facilitate efficient downstream processing.\nInput Required: Aligned BAM files from Step 8.\nExpected Output: Sorted and indexed BAM files.\nTools Used: SAMtools.\n\nStep 11: UMI-Based Deduplication\nDescription: Remove PCR duplicates from the aligned data using UMI information for accurate quantification.\nInput Required: Sorted BAM files from Step 10.\nExpected Output: Deduplicated BAM files.\nTools Used: UMI-tools.\n\nStep 12: Duplicate Marking\nDescription: Mark duplicate reads to assess library complexity and provide quality metrics for the data.\nInput Required: Sorted BAM files from Step 10.\nExpected Output: BAM files with duplicates marked.\nTools Used: Picard MarkDuplicates.\n\nStep 13: Transcript Assembly and Quantification\nDescription: Assemble transcripts from the deduplicated alignments and estimate their expression levels.\nInput Required: Deduplicated BAM files from Step 11 and reference annotation (if available).\nExpected Output: Assembled transcripts and corresponding expression estimates.\nTools Used: StringTie.\n\nStep 14: Coverage Calculation\nDescription: Calculate read coverage across the genome to generate a BedGraph file.\nInput Required: Sorted BAM files from Step 10.\nExpected Output: BedGraph files representing genome-wide coverage.\nTools Used: BEDTools.\n\nStep 15: BigWig File Generation\nDescription: Convert the BedGraph coverage file into BigWig format for efficient storage and further analysis.\nInput Required: BedGraph files from Step 14 and a chromosome size file.\nExpected Output: BigWig coverage files.\nTools Used: bedGraphToBigWig.\n\nStep 16: Alignment Quality Assessment\nDescription: Evaluate various alignment metrics to ensure high-quality mapping, including junction saturation and read distribution.\nInput Required: Aligned BAM files from Step 8 and gene annotation files.\nExpected Output: Detailed alignment quality reports.\nTools Used: RSeQC.\n\nStep 17: Coverage and Distribution Analysis\nDescription: Analyze the overall coverage uniformity and gene body coverage statistics of the aligned data.\nInput Required: Sorted BAM files from Step 10 and gene annotation files.\nExpected Output: Coverage and distribution reports.\nTools Used: Qualimap.\n\nStep 18: Duplication Analysis\nDescription: Assess the level of duplicate reads to evaluate library complexity and potential PCR artifacts.\nInput Required: Aligned or sorted BAM files (from Step 8 or Step 10).\nExpected Output: Duplication metrics and plots.\nTools Used: dupRadar.\n\nStep 19: Library Complexity Estimation\nDescription: Estimate the sequencing library complexity and predict sequencing saturation.\nInput Required: Aligned BAM files from Step 8.\nExpected Output: Library complexity estimates and saturation curves.\nTools Used: Preseq.\n\nStep 20: Differential Expression Normalization\nDescription: Normalize transcript quantification data and perform preliminary differential expression analysis.\nInput Required: Transcript abundance estimates from Step 9 and sample metadata.\nExpected Output: Normalized expression counts and statistical outputs for differential expression analysis.\nTools Used: DESeq2.\n\nOptional Steps:\n\nStep 21 (Optional): Taxonomic Classification\nDescription: Classify unaligned sequences to detect potential contamination from non-target organisms.\nInput Required: Unaligned or residual FASTQ reads.\nExpected Output: Taxonomic classification reports.\nTools Used: Kraken2.\n\nStep 22 (Optional): Alternative Quantification\nDescription: Perform pseudoalignment and transcript quantification as an independent validation of the primary quantification results.\nInput Required: Cleaned FASTQ files from Step 7 and a reference transcriptome.\nExpected Output: Alternative transcript abundance estimates.\nTools Used: Kallisto.",
        "metadata": {
            "source": "workflow",
            "page": 19
        }
    },
    {
        "content": "This is a workflow for Comprehensive Cancer DNA/RNA Analysis using nf-core/oncoanalyser\n\nStep 1: Create Sample Sheet (Input Preparation)\nDescription: Prepare a tab‐delimited sample sheet listing each sample’s identifiers and file paths. This file informs the pipeline about the type of sequencing data (e.g. WGS, WTS) and associated metadata.  \nInput Required: Sequencing data file paths (FASTQ or BAM) and metadata details (group_id, subject_id, sample_id, sample_type, sequence_type, filetype, info, filepath).  \nExpected Output: A well-formatted CSV file (e.g. samplesheet.csv) for pipeline input.  \nTool Used: Text editor (or spreadsheet software)  \n\nStep 2: DNA Alignment with bwa‐mem2  \nDescription: Align DNA FASTQ reads to the reference genome using an efficient short-read mapper.  \nInput Required: DNA FASTQ files and reference genome (e.g. GRCh38_hmf).  \nExpected Output: Aligned DNA BAM files with mapped reads.  \nTool Used: bwa‐mem2  \n\nStep 3: RNA Alignment with STAR  \nDescription: Align RNA FASTQ reads to the reference genome/transcriptome to capture spliced transcripts.  \nInput Required: RNA FASTQ files and corresponding reference genome/transcriptome index.  \nExpected Output: RNA alignment files (BAM) with spliced alignments.  \nTool Used: STAR  \n\nStep 4: Duplicate Marking with MarkDups  \nDescription: Identify and mark duplicate reads in the aligned BAM files to reduce PCR artifacts.  \nInput Required: Aligned BAM files (from DNA and/or RNA alignments).  \nExpected Output: BAM files with duplicate reads marked (typically flagged in the header).  \nTool Used: MarkDups  \n\nStep 5: Duplicate Marking with Picard MarkDuplicates  \nDescription: Alternatively, mark duplicate reads using a widely adopted toolkit to eliminate PCR artifacts.  \nInput Required: Aligned BAM files resulting from DNA or RNA read mapping.  \nExpected Output: Processed BAM files with duplicates flagged for downstream analysis.  \nTool Used: Picard MarkDuplicates  \n\nStep 6: SNV, MNV, and INDEL Calling with SAGE  \nDescription: Detect single nucleotide variants (SNVs), multi-nucleotide variants (MNVs), and small insertions/deletions (INDELs) from processed alignments.  \nInput Required: Processed BAM files from duplicate marking.  \nExpected Output: Variant call files (VCFs) with SNVs, MNVs, and INDELs.  \nTool Used: SAGE  \n\nStep 7: SNV, MNV, and INDEL Calling with PAVE  \nDescription: Optionally, perform variant calling for SNVs, MNVs, and INDELs with an alternative algorithm to validate or complement the SAGE output.  \nInput Required: Processed BAM files from duplication marking.  \nExpected Output: VCF files with detected SNVs, MNVs, and INDELs.  \nTool Used: PAVE  \n\nStep 8: CNV Calling with AMBER  \nDescription: Initiate copy-number variation (CNV) analysis by quantifying allele-specific copy numbers across the genome.  \nInput Required: Aligned and duplicate-marked BAM files.  \nExpected Output: CNV segmentation files indicating allele-specific copy number estimates.  \nTool Used: AMBER  \n\nStep 9: CNV Calling with COBALT  \nDescription: Complement the CNV analysis by measuring read depth based on binning strategies across the genome.  \nInput Required: Aligned BAM files along with a binning reference for read depth calculation.  \nExpected Output: Binned read-depth profiles for copy-number estimation.  \nTool Used: COBALT  \n\nStep 10: CNV Integration with PURPLE  \nDescription: Integrate allele-specific CNV data (from AMBER and COBALT) with variant allele frequencies and tumor purity estimates to infer final copy-number calls.  \nInput Required: Outputs from AMBER (allele-specific segmentation) and COBALT (read-depth profiles), and variant calls.  \nExpected Output: Final CNV calls with integrated information about copy numbers and purity estimates.  \nTool Used: PURPLE  \n\nStep 11: SV Preprocessing with SvPrep  \nDescription: Preprocess alignments for structural variant (SV) detection by reformatting and quality filtering the data.  \nInput Required: Aligned and duplicate-marked BAM files from earlier steps.  \nExpected Output: Reformatted files prepared for input into SV detection tools.  \nTool Used: SvPrep  \n\nStep 12: SV Calling with GRIDSS  \nDescription: Identify structural variants (SVs) such as insertions, deletions, and rearrangements in the genomic data.  \nInput Required: Processed BAM files or output from SvPrep.  \nExpected Output: Raw SV call files indicating breakpoints or rearrangements in the genome.  \nTool Used: GRIDSS  \n\nStep 13: SV Calling with GRIPSS  \nDescription: Alternatively, run a complementary method for SV detection to capture additional variants or refine the breakpoints.  \nInput Required: Preprocessed alignment files from SvPrep.  \nExpected Output: SV call files detailing candidate structural variants.  \nTool Used: GRIPSS  \n\nStep 14: SV Event Interpretation with LINX  \nDescription: Interpret and cluster structural variant events in the context of cancer genomes to deduce genomic rearrangements and complex events.  \nInput Required: SV call files generated from GRIDSS and/or GRIPSS.  \nExpected Output: Interpreted and annotated SV event files with biological context.  \nTool Used: LINX  \n\nStep 15: Transcript Analysis with Isofox  \nDescription: Analyze transcriptome data to quantify gene expression and detect transcript isoforms relevant for cancer profiling.  \nInput Required: RNA alignment files (BAM) from STAR.  \nExpected Output: Expression matrices or transcript isoform reports for downstream analysis.  \nTool Used: Isofox  \n\nStep 16: Oncoviral Detection with VIRUSBreakend  \nDescription: Detect oncoviral integration events by scanning for breakends that could indicate viral insertions in the host genome.  \nInput Required: Processed BAM files from the alignment steps.  \nExpected Output: Viral integration candidate lists with genomic coordinates.  \nTool Used: VIRUSBreakend  \n\nStep 17: Viral Interpretation with Virus Interpreter  \nDescription: Annotate and interpret the detected viral integration events, providing clinical and biological insights.  \nInput Required: Oncoviral candidate events generated by VIRUSBreakend.  \nExpected Output: Detailed viral event reports with annotated significance.  \nTool Used: Virus Interpreter  \n\nStep 18: HLA Calling with LILAC  \nDescription: Infer HLA alleles from the sequencing data to assess potential immunogenomic markers.  \nInput Required: Aligned BAM files (typically from DNA data) along with reference HLA allele databases.  \nExpected Output: HLA allele calls and reports.  \nTool Used: LILAC  \n\nStep 19: HRD Status Prediction with CHORD  \nDescription: Assess homologous recombination deficiency (HRD) status in cancer samples by leveraging genomic scar signatures and mutational patterns.  \nInput Required: Variant call files (from SAGE/PAVE) and CNV profiles (from PURPLE).  \nExpected Output: HRD status prediction reports for each sample.  \nTool Used: CHORD  \n\nStep 20: Mutational Signature Fitting with Sigs  \nDescription: Decompose the mutational profile of the tumor into known mutational signatures to infer underlying mutagenic processes.  \nInput Required: Catalog of detected SNVs/MNVs/INDELs from earlier variant calling.  \nExpected Output: Fitted mutational signature profiles with quantitative contributions.  \nTool Used: Sigs  \n\nStep 21: Tissue of Origin Prediction with CUPPA  \nDescription: Predict the tissue of origin for cancer samples based on genomic and transcriptomic signatures.  \nInput Required: Combined genomic features (e.g. mutational signatures, CNVs) and transcriptome profiles.  \nExpected Output: Predicted tissue of origin report per sample.  \nTool Used: CUPPA  \n\nStep 22: Report Generation with ORANGE  \nDescription: Generate a comprehensive clinical report summarizing key findings (variants, CNVs, SVs, HRD status, oncogenic drivers, etc.) for each sample.  \nInput Required: Integrated results from individual analyses (variant calls, CNV integration, SV interpretation, transcript analysis, etc.).  \nExpected Output: A visually formatted report (PDF/HTML) summarizing the cancer genomic profile (e.g. ORANGE_report.pdf).  \nTool Used: ORANGE  \n\nStep 23: Report Generation with linxreport  \nDescription: Alternatively, compile a report focused on structural variant events and their interpretation in the clinical context.  \nInput Required: Output from the LINX SV interpretation step and other supporting annotation data.  \nExpected Output: A detailed report providing insights into rearrangement events and clinical annotations.  \nTool Used: linxreport  \n\n",
        "metadata": {
            "source": "workflow",
            "page": 20
        }
    }
]