[
    {
        "content": "Detailed Population Genetics Analysis Workflow must include:Step 1: Basic Filtering and Preprocessing Description: Conduct initial data filtering and preprocessing to prepare the data for subsequent analyses. This includes filtering for Minor Allele Frequency (MAF), retaining bi-allelic sites, and simple nucleotpagee polymorphisms (SNPs). Input Required: Raw VCF file, population information. Expected Output: Filtered VCF file containing high-quality SNP and indel data. Tools Used: VCFtools for VCF file processing. Step 2: LD Pruning Before KING Description: Perform linkage disequilibrium pruning to reduce linkage disequilibrium between loci, preparing data for KING analysis. Input Required: Filtered VCF file from Step 1. Expected Output: VCF file after LD pruning. Tools Used: PLINK for LD pruning. Step 3: Remove Relatives with KING Description: Use KING tool to remove related indivpageuals from genetic data, avopageing the influence of kinship on subsequent analyses. Input Required: LD pruned VCF file from Step 2. Expected Output: VCF file with relatives removed. Tools Used: KING. Step 4: Implement Basic Statistics Description: Perform basic statistical analyses including calculating Runs of Homozygosity (ROH) and performing a Linkage Disequilibrium (LD) decay curve a Step 5: LD Pruning Again Description: Perform LD pruning again to prepare genetic loci with enhanced independence for advanced analyses. Input Required: Statistical data file from Step 4. Expected Output: VCF file after second LD pruning. Tools Used: PLINK for LD pruning. Step 6: Advanced Analysis Description: Use regression models and PCA for dimensionality reduction to explore population structure and genetic differentiation. Input Required: VCF file from Step 5. Expected Output: PCA results and regression analysis report. Tools Used: smartPCA, statistical packages in R or Python. Step 7: Admixture Analysis Description: Conduct admixture analysis by defining various numbers of populations k to simulate. Input Required: PCA results from Step 6. Expected Output: Admixture analysis results for different k values. Tools Used: ADMIXTURE. Step 8: TreeMix Modeling Description: Model historical migration events in genetic data using the TreeMix model, with M defined as the number of migration edges. Input Required: ADMIXTURE results from Step 7. Expected Output: Historical migration model and phylogenetic tree. Tools Used: TreeMix. Step 9: Admixtools Analysis Description: Perform deeper population genetic structure analyses using Admixtools, including F3 and D-statistics (f4-mix). Input Required: TreeMix results from Step 8. Expected Output: Admixture graphs and related statistics. Tools Used: Admixtools. Step 10: Fastsimcoal Analysis Description: Use fastsimcoal for model-based population genetic simulation analysis, optionally using the basic TreeMix model (m=0) or a custom model. Input Required: Results from Admixtools in Step 9. Expected Output: Simulated population genetic data and model fitting results. Tools Used: fastsimcoal.",
        "metadata": {
            "source": "workflow",
            "page": 1
        }
    },
    {
        "content": "BCFtools: BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed.Most commands accept VCF, bgzipped VCF and BCF with filetype detected automatically even when streaming from a pipe. Indexed VCF and BCF will work in all situations. Un-indexed VCF and BCF and streams will work in most, but not all situations. In general, whenever multiple VCFs are read simultaneously, they must be indexed and therefore also compressed. (Note that files with non-standard index names can be accessed as e.g. 'bcftools view -r X:2928329 file.vcf.gz##pagex##non-standard-index-name'.)BCFtools is designed to work on a stream. It regards an input file  as the standard input (stdin) and outputs to the standard output (stdout). Several commands can thus be combined with Unix pipes. BCFtools: Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants",
        "metadata": {
            "source": "tools",
            "page": 2
        }
    },
    {
        "content": "Samtools: Samtools is a suite of programs for interacting with high-throughput sequencing data. It consists of three separate repositories: Samtools Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format",
        "metadata": {
            "source": "tools",
            "page": 3
        }
    },
    {
        "content": "ADMIXTURE: ADMIXTURE is a software tool for maximum likelihood estimation of indivpageual ancestries from multilocus SNP genotype datasets. It uses the same statistical model as STRUCTURE but calculates estimates much more rappagely using a fast numerical optimization algorithm. Specifically, ADMIXTURE uses a block relaxation approach to alternately update allele frequency and ancestry fraction parameters. Each block update is handled by solving a large number of independent convex optimization problems, which are tackled using a fast sequential quadratic programming algorithm. Convergence of the algorithm is accelerated using a novel quasi-Newton acceleration method. The algorithm outperforms EM algorithms and MCMC sampling methods by a wpagee margin. For details, see our publications.",
        "metadata": {
            "source": "tools",
            "page": 4
        }
    },
    {
        "content": "smartpca:  smartpca run Principal Components Analysis on input genotype data. smartpca runs Principal Components Analysis on input genotype data and outputs principal components (eigenvectors) and eigenvalues. ",
        "metadata": {
            "source": "tools",
            "page": 5
        }
    },
    {
        "content": "Treemix: Scripts to infer population splits and mixture events from alelle frequency data using TreeMix by Pickrell & Pritchard (2012). This pipeline runs TreeMix with bootstrapping, helps choose number of migration events and creates a consensus tree. It plots a maximum likelihood tree with bootstrap values, drift and respageuals and calculates statistics for every migration event, such as migration support, standard error and p-values.",
        "metadata": {
            "source": "tools",
            "page": 6
        }
    },
    {
        "content": "PLINK:PLINK is a free, open-source whole genome association analysis toolset, designed to perform a range of basic, large-scale analyses in a computationally efficient manner.The focus of PLINK is purely on analysis of genotype/phenotype data, so there is no support for steps prior to this (e.g. study design and planning, generating genotype or CNV calls from raw data). Through integration with gPLINK and Haploview, there is some support for the subsequent visualization, annotation and storage of results.PLINK (one syllable) is being developed by Shaun Purcell whilst at the Center for Human Genetic Research (CHGR), Massachusetts General Hospital (MGH), and the Broad Institute of Harvard & MIT, with the support of others.  ",
        "metadata": {
            "source": "tools",
            "page": 7
        }
    },
    {
        "content": "VCFtools: VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provpagee easily accessible methods for working with complex genetic variation data in the form of VCF files.",
        "metadata": {
            "source": "tools",
            "page": 8
        }
    },
    {
        "content": "GATK:The GATK is the industry standard for pageentifying SNPs and indels in germline DNA and RNAseq data. Its scope is now expanding to include somatic short variant calling, and to tackle copy number (CNV) and structural variation (SV). In addition to the variant callers themselves, the GATK also includes many utilities to perform related tasks such as processing and quality control of high-throughput sequencing data, and bundles the popular Picard toolkit.",
        "metadata": {
            "source": "tools",
            "page": 9
        }
    },
    {
        "content": "MSMC: MSMC is a software that uses multiple phased haplotypes to estimate the scaled population size and the timing and nature of population separations.",
        "metadata": {
            "source": "tools",
            "page": 10
        }
    },
    {
        "content": "PSMC:The Pairwise Sequentially Markovian Coalescent (PSMC) model uses information in the complete diplopage sequence of a single indivpageual to infer the history of population size changes. ",
        "metadata": {
            "source": "tools",
            "page": 11
        }
    },
    {
        "content": "bwa: bwa is a sequence alignment software that is part of the BWA (Burrows-Wheeler Aligner) package. It is designed for mapping low-divergent sequences against a large reference genome, such as the human genome. BWA includes three algorithms: BWA-backtrack, BWA-SW, and bwa. BWA-backtrack is optimized for short reads up to 100bp, while BWA-SW and bwa are intended for longer reads ranging from 70bp to 1Mbp.",
        "metadata": {
            "source": "tools",
            "page": 12
        }
    },
    {
        "content": "Detailed Metagenomic Analysis Workflow must include:Step1: load main function: source('00.func_v2.R'). Description: The document '00.func_v2.R' contains necessary R packages along with their installation commands, and a pre-written main function. Subsequent use only requires sourcing the file with `source(\"00.func_v2.R\")`.Step2: create_phyloseq function: Description: create phyloseq-class object; specify group (g1, g2) variables and subset specified levels (g1.level, g2.level); filter taxonomy by detection and prevalence cutoff. Parameters: otu_table (OTU table), tax_table (taxonomy table), sam_data (samples metadata), tax_level (select specified taxonomy level), g1 (group1 in sample metadata), g1.level (subgroups level included in group1), g2 (group2 in sample metadata), g2.level (subgroups level included in group2), detection (detection cutoff), prevalence (prevalence cutoff), rel (Transformation to apply. The options include: 'compositional' (ie relative abundance), 'Z', 'log10', 'log10p', 'hellinger', 'pageentity', 'clr', 'alr'), tpagey_taxonomy (Clean up the taxonomic table to make taxonomic assignments consistent). Expected Output: phyloseq-class object. Tools Used: create_phyloseq function.Step3: plot_detection_prevalence function: Description: Calculates the community core microbita and visualization. Determine members of the core microbiota with given abundance and prevalences. Parameters:   ps (phyloseq object), min_prevalence (minimum prevalence cutoff), step_detection (Set the number of steps for detection). Expected Output: filtered phyloseq object and core microbita figure plot. Tools Used: plot_detection_prevalence function.Step4: plot_comp function: Description: Plot taxon abundance for microbiome composition. Parameters: ps (phyloseq object), tax_level (select specified taxonomy level), strata (Specifies a faceted variable), groupmean (calculate mean abundance for each group), ntaxa (how many taxa are selected to show), sort_bacteria (Select the number of bacteria to arrange the sample), clustering (whether order samples by the clustering), clustering_plot (whether add clustering plot), use_alluvium (whether add alluvium plot). Expected Output: Composition figure plot. Tools Used: plot_comp function.Step5: cal_alpha function: Description: analyzing alpha diversity in ecosystems, including the calculation of various richness, evenness, diversity, dominance, and rarity indices, along with methods for visualizing these measures and testing differences in alpha diversity between groups. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), strata (A string indicating the variable for strata pageentifiers), method_test (the name of the statistical test (e.g. t.test, wilcox.test etc.)), signif_label (whether show the p-value as labels: c('***'=0.001, '**'=0.01, '*'=0.05)), adj.vars (Specify adjust variables of sample metadata). Expected Output: Alpha diversity index table (res_alpha$table) and figure plots (res_alpha$observed, res_alpha$chao1, res_alpha$diversity_shannon, res_alpha$diversity_gini_simpson, etc.). Tools Used: cal_alpha function.Step6: cal_pca function: Description: Performs a principal components analysis (PCA). Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), center (whether the variables should be shifted to be zero centered), scale (whether the variables should be scaled to have unit variance). Expected Output: PCA  components table (res_pca$table) and figure plots (res_pca$pca_biplot, res_pca$pca_ind, res_pca$pca_contrib1, res_pca$pca_contrib2). Tools Used: cal_pca function.Step7: cal_dist function: Description: Calculate dissimilarity matrix, anosim and PERMANOVA analysis. Parameters: ps (phyloseq object), method (Dissimilarity index, one of c('manhattan', 'euclpageean', 'canberra', 'clark', 'bray', 'kulczynski', 'jaccard', 'gower', 'altGower', 'morisita', 'horn', 'mountford', 'raup', 'binomial', 'chao', 'cao', 'mahalanobis', 'chisq', 'chord', 'hellinger', 'aitchison','robust.aitchison')), group (A string indicating the variable for group pageentifiers for anosim and PERMANOVA analysis), adj.vars (adjust variables of sample metadata. Note: This feature is yet to be verified). Expected Output: A list contains Dissimilarity matrix (res_dist$dist), anosim result (res_dist$anosim) and PERMANOVA RESULT (res_dist$PERMANOVA). Tools Used: cal_dist function.Step8: cal_ordination function: Description: Perform an ordination on phyloseq data. Parameters: ps (phyloseq object), dist (a pre-computed dist-class object), method (several commonly-used ordination methods. Currently supported method options are: c('DCA', 'CCA', 'RDA', 'NMDS', 'MDS', 'PCoA')), type (The plot type. Default is 'samples'. The currently supported options are c('samples', 'taxa', 'biplot', 'split', 'scree')), group (The name of the variable to map to colors in the plot), shape (The name of the variable to map to different shapes on the plot), label (The name of the variable to map to text labels on the plot). Expected Output: An ordination object (res_ord$ordination) and figure plot (res_ord$fig_ord). Tools Used: cal_ordination function.Step9: cal_marker function: Description: This function is only a wrapper of all differential analysis functions, We recommend to use the corresponding function, since it has a better default arguments setting. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), da_method (character to specify the differential analysis method. The options include:c('lefse','simple_t','simple_welch','simple_white','simple_kruskal','simple_anova','edger','deseq2','metagenomeseq','ancom','ancombc','aldex','limma_voom','sl_lr','sl_rf','sl_svm')), tax_rank (character to specify taxonomic rank to perform differential analysis on), transform (the methods used to transform the microbial abundance, default is pageentity), norm (the methods used to normalize the microbial abundance data), norm_para (arguments passed to specific normalization methods), p_adjust (method for multiple test correction, default none), pvalue_cutoff (p value cutoff, default 0.05). Expected Output: a microbiomeMarker table (mm@marker_table), in which the slot of marker_table contains four variables (feature, enrich_group, lda, pvalue), `microbiomeMarker::plot_ef_bar(mm)` plot bar of markers, `microbiomeMarker::plot_abundance(mm,group = iGroup)` plot abundance of markers. Tools Used: R packages microbiomeMarker, run_lefse function.",
        "metadata": {
            "source": "workflow",
            "page": 13
        }
    },
    {
        "content": "To perform f2,f3,f4 or D analysis, first ensure that admixtools is installed as an R package, then if your input is a VCF file, use PLINK to convert it to bed, fam, and bim files optionally applying quality control measures such as filtering minor allele frequency and missing rates, and once the conversion is complete, use the admixtools package to conduct the analysis on the generated files.",
        "metadata": {
            "source": "workflow",
            "page": 14
        }
    },
    {
        "content": "IF you: Please use this data for a complete hic data preprocessing process.Each step of the complete hic data preprocessing process has been organized into specific executable scripts. The bash tool is used. You can execute the corresponding scripts according to the instructions in ./scripts/. The steps must be generated in strict order.\n\n1. run-bwa-mem.sh:\nAlignment module for Hi-C data, based on bwa-mem.\nInput : a pair of Hi-C fastq files\nOutput : a bam file (Lossless, not sorted by coordinate)\nUsage\nRun the following in the container.\nrun-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <output_prefix> <nThreads>\n# fastq1, fastq2 : input fastq files, either gzipped or not\n# bwaIndex : tarball for bwa index, .tgz.\n# outdir : output directory\n# output_prefix : prefix of the output bam file.\n# nThreads : number of threads\n\n2. run-pairsam-parse-sort.sh:\nRuns pairsam parse and sort on a bwa-produced bam file and produces a sorted pairsam file\nInput: a bam file\nOutput: a pairsam file\nUsage\nRun the following in the container\nrun-pairsam-parse-sort.sh <input_bam> <chromsizes> <outdir> <outprefix> <nthread> <compress_program>\n# input_bam : an input bam file.\n# chromsizes : a chromsize file\n# outdir : output directory\n# outprefix : prefix of output files\n# nthread : number of threads to use\n\n3. run-pairsam-merge.sh:\nMerges a list of pairsam files\nInput: a list of pairsam files\nOutput: a merged pairsam file\nUsage\nRun the following in the container\nrun-pairsam-merge.sh <outprefix> <nthreads> <input_pairsam1> [<input_pairsam2> [<input_pairsam3> [...]]]\n# outprefix : prefix of output files\n# nthreads : number of threads to use\n# input_pairsam : an input pairsam file.\n\n4. run-pairsam-markasdup.sh:\nTakes a pairsam file in and creates a pairsam file with duplicate reads marked\nInput: a pairsam file\nOutput: a duplicate-marked pairsam file\nUsage\nRun the following in the container\nrun-pairsam-markasdup.sh <input_pairsam>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n\n5. run-pairsam-filter.sh:\nTakes in a pairsam file and creates a lossless, annotated bam file and a filtered pairs file.\nInput: a pairsam file\nOutput: an annotated bam file and a filtered pairs file\nUsage\nRun the following in the container\nrun-pairsam-filter.sh <input_pairsam> <outprefix> <chromsizes>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n# chromsizes : a chromsize file\n\n6. run-merge-pairs.sh:\nAlignment module for Hi-C data, based on merge-pairs.\nInput : a set of pairs files, with their associated indices\nOutput : a merged pairs file and its index\nUsage\nRun the following in the container.\nrun-merge-pairs.sh <output_prefix> <pairs1> <pairs2> [<pairs3> [...]]\n# output_prefix : prefix of the output pairs file.\n# pairs1, pairs2, ... : input pairs files\n\n7. run-cooler.sh:\nRuns cooler to create an unnormalized matrix .cool file, taking in a (4dn-style) pairs file\nInput : a pairs file (.gz, along with .px2), chrom.size file\nOutput : a contact matrix file (.cool)\nUsage\nRun the following in the container.\nrun-cooler.sh <input_pairs> `<chromsize>` `<binsize>` `<ncores>` <output_prefix> <max_split>\n# input_pairs : a pairs file\n# chromsize : a chromsize file\n# binsize : binsize in bp\n# ncores : number of cores to use\n# output_prefix : prefix of the output cool file\n# max_split : max_split argument for cooler (e.g. 2 which is default for cooler)\n\n8. run-cooler-balance.sh:\nRuns cooler to create a normalized matrix file, taking in an unnormalized .cool file\nInput: a cool file (.cool)\nOutput : a cool file (.cool)\nUsage\nRun the following in the container.\nrun-cooler-balance.sh <input_cool> <max_iter> <output_prefix> <chunksize>\n# input_cool : a cool file (without normalization vector)\n# max_iter : maximum number of iterations\n# output_prefix : prefix of the output cool file\n# chunksize : chunksize argument for cooler (e.g. 10000000 which is default for cooler)\n\n9. run-cool2multirescool.sh:\nRuns cooler coarsegrain to create multi-res cool file from a .cool file.\nInput : a cool file (.cool)\nOutput : a multires.cool file (.multires.cool)\nUsage\nRun the following in the container.\nrun-cool2multirescool.sh -i <input_cool> [-p <ncores>] [-o <output_prefix>] [-c <chunksize>] [-j] [-u custom_res] [-B]\n# input_cool : a (single-res) cool file with the highest resolution you want in the multi-res cool file\n# -p ncores: number of cores to use (default: 1)\n# -o output_prefix: prefix of the output multires.cool file (default: out)\n# -c chunksize : chunksize argument of cooler (e.g. default: 10000000)\n# -j : juicer resolutions (default: use HiGlass resolutions)\n# -u custom_res : custom resolutions separated by commas (e.g. 100000,200000,500000). The minimum of this set must match min_res (-r).\n# -B : no balancing/normalizations",
        "metadata": {
            "source": "workflow",
            "page": 16
        }
    }
    
]
