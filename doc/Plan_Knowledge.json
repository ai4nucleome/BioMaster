[
    {
        "content": "VCFtools: VCFtools is a program package designed for working with VCF files, such as those generated by the 1000 Genomes Project. The aim of VCFtools is to provpagee easily accessible methods for working with complex genetic variation data in the form of VCF files.",
        "metadata": {
            "source": "tools",
            "page": 8
        }
    },
    {
        "content": "GATK:The GATK is the industry standard for pageentifying SNPs and indels in germline DNA and RNAseq data. Its scope is now expanding to include somatic short variant calling, and to tackle copy number (CNV) and structural variation (SV). In addition to the variant callers themselves, the GATK also includes many utilities to perform related tasks such as processing and quality control of high-throughput sequencing data, and bundles the popular Picard toolkit.",
        "metadata": {
            "source": "tools",
            "page": 9
        }
    },
    {
        "content": "bwa: bwa is a sequence alignment software that is part of the BWA (Burrows-Wheeler Aligner) package. It is designed for mapping low-divergent sequences against a large reference genome, such as the human genome. BWA includes three algorithms: BWA-backtrack, BWA-SW, and bwa. BWA-backtrack is optimized for short reads up to 100bp, while BWA-SW and bwa are intended for longer reads ranging from 70bp to 1Mbp.",
        "metadata": {
            "source": "tools",
            "page": 12
        }
    },
    {
        "content": "Detailed Metagenomic Analysis Workflow must include:Step1: load main function: source('00.func_v2.R'). Description: The document '00.func_v2.R' contains necessary R packages along with their installation commands, and a pre-written main function. Subsequent use only requires sourcing the file with `source(\"00.func_v2.R\")`.Step2: create_phyloseq function: Description: create phyloseq-class object; specify group (g1, g2) variables and subset specified levels (g1.level, g2.level); filter taxonomy by detection and prevalence cutoff. Parameters: otu_table (OTU table), tax_table (taxonomy table), sam_data (samples metadata), tax_level (select specified taxonomy level), g1 (group1 in sample metadata), g1.level (subgroups level included in group1), g2 (group2 in sample metadata), g2.level (subgroups level included in group2), detection (detection cutoff), prevalence (prevalence cutoff), rel (Transformation to apply. The options include: 'compositional' (ie relative abundance), 'Z', 'log10', 'log10p', 'hellinger', 'pageentity', 'clr', 'alr'), tpagey_taxonomy (Clean up the taxonomic table to make taxonomic assignments consistent). Expected Output: phyloseq-class object. Tools Used: create_phyloseq function.Step3: plot_detection_prevalence function: Description: Calculates the community core microbita and visualization. Determine members of the core microbiota with given abundance and prevalences. Parameters:   ps (phyloseq object), min_prevalence (minimum prevalence cutoff), step_detection (Set the number of steps for detection). Expected Output: filtered phyloseq object and core microbita figure plot. Tools Used: plot_detection_prevalence function.Step4: plot_comp function: Description: Plot taxon abundance for microbiome composition. Parameters: ps (phyloseq object), tax_level (select specified taxonomy level), strata (Specifies a faceted variable), groupmean (calculate mean abundance for each group), ntaxa (how many taxa are selected to show), sort_bacteria (Select the number of bacteria to arrange the sample), clustering (whether order samples by the clustering), clustering_plot (whether add clustering plot), use_alluvium (whether add alluvium plot). Expected Output: Composition figure plot. Tools Used: plot_comp function.Step5: cal_alpha function: Description: analyzing alpha diversity in ecosystems, including the calculation of various richness, evenness, diversity, dominance, and rarity indices, along with methods for visualizing these measures and testing differences in alpha diversity between groups. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), strata (A string indicating the variable for strata pageentifiers), method_test (the name of the statistical test (e.g. t.test, wilcox.test etc.)), signif_label (whether show the p-value as labels: c('***'=0.001, '**'=0.01, '*'=0.05)), adj.vars (Specify adjust variables of sample metadata). Expected Output: Alpha diversity index table (res_alpha$table) and figure plots (res_alpha$observed, res_alpha$chao1, res_alpha$diversity_shannon, res_alpha$diversity_gini_simpson, etc.). Tools Used: cal_alpha function.Step6: cal_pca function: Description: Performs a principal components analysis (PCA). Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), center (whether the variables should be shifted to be zero centered), scale (whether the variables should be scaled to have unit variance). Expected Output: PCA  components table (res_pca$table) and figure plots (res_pca$pca_biplot, res_pca$pca_ind, res_pca$pca_contrib1, res_pca$pca_contrib2). Tools Used: cal_pca function.Step7: cal_dist function: Description: Calculate dissimilarity matrix, anosim and PERMANOVA analysis. Parameters: ps (phyloseq object), method (Dissimilarity index, one of c('manhattan', 'euclpageean', 'canberra', 'clark', 'bray', 'kulczynski', 'jaccard', 'gower', 'altGower', 'morisita', 'horn', 'mountford', 'raup', 'binomial', 'chao', 'cao', 'mahalanobis', 'chisq', 'chord', 'hellinger', 'aitchison','robust.aitchison')), group (A string indicating the variable for group pageentifiers for anosim and PERMANOVA analysis), adj.vars (adjust variables of sample metadata. Note: This feature is yet to be verified). Expected Output: A list contains Dissimilarity matrix (res_dist$dist), anosim result (res_dist$anosim) and PERMANOVA RESULT (res_dist$PERMANOVA). Tools Used: cal_dist function.Step8: cal_ordination function: Description: Perform an ordination on phyloseq data. Parameters: ps (phyloseq object), dist (a pre-computed dist-class object), method (several commonly-used ordination methods. Currently supported method options are: c('DCA', 'CCA', 'RDA', 'NMDS', 'MDS', 'PCoA')), type (The plot type. Default is 'samples'. The currently supported options are c('samples', 'taxa', 'biplot', 'split', 'scree')), group (The name of the variable to map to colors in the plot), shape (The name of the variable to map to different shapes on the plot), label (The name of the variable to map to text labels on the plot). Expected Output: An ordination object (res_ord$ordination) and figure plot (res_ord$fig_ord). Tools Used: cal_ordination function.Step9: cal_marker function: Description: This function is only a wrapper of all differential analysis functions, We recommend to use the corresponding function, since it has a better default arguments setting. Parameters: ps (phyloseq object), group (A string indicating the variable for group pageentifiers), da_method (character to specify the differential analysis method. The options include:c('lefse','simple_t','simple_welch','simple_white','simple_kruskal','simple_anova','edger','deseq2','metagenomeseq','ancom','ancombc','aldex','limma_voom','sl_lr','sl_rf','sl_svm')), tax_rank (character to specify taxonomic rank to perform differential analysis on), transform (the methods used to transform the microbial abundance, default is pageentity), norm (the methods used to normalize the microbial abundance data), norm_para (arguments passed to specific normalization methods), p_adjust (method for multiple test correction, default none), pvalue_cutoff (p value cutoff, default 0.05). Expected Output: a microbiomeMarker table (mm@marker_table), in which the slot of marker_table contains four variables (feature, enrich_group, lda, pvalue), `microbiomeMarker::plot_ef_bar(mm)` plot bar of markers, `microbiomeMarker::plot_abundance(mm,group = iGroup)` plot abundance of markers. Tools Used: R packages microbiomeMarker, run_lefse function.",
        "metadata": {
            "source": "workflow",
            "page": 13
        }
    },
    {
        "content": "IF you: Please use this data for a complete hic data preprocessing process.Each step of the complete hic data preprocessing process has been organized into specific executable scripts. The bash tool is used. You can execute the corresponding scripts according to the instructions in ./scripts/. The steps must be generated in strict order.\n\n1. run-bwa-mem.sh:\nAlignment module for Hi-C data, based on bwa-mem.\nInput : a pair of Hi-C fastq files\nOutput : a bam file (Lossless, not sorted by coordinate)\nUsage\nRun the following in the container.\nrun-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <output_prefix> <nThreads>\n# fastq1, fastq2 : input fastq files, either gzipped or not\n# bwaIndex : tarball for bwa index, .tgz.\n# outdir : output directory\n# output_prefix : prefix of the output bam file.\n# nThreads : number of threads. Also for the first step run-bwa-mem, if there are two pairs of data, one set of data should be output. If there are four pairs data, two pairs of data should be generated. Six makes three.Example 4DNFI15H1RVG.fastq.gz,4DNFIZHUKESO.fastq.gz,4DNFIEQ58J6G.fastq.gz,4DNFIKVDGNJN.fastq.gz should generate two of 4DNFI95DH3QJ.fastq.gz and 4DNFIHMY7KMH.fastq.gz output, for reference only\n\n2. run-pairsam-parse-sort.sh:\nRuns pairsam parse and sort on a bwa-produced bam file and produces a sorted pairsam file\nInput: a bam file\nOutput: a pairsam file\nUsage\nRun the following in the container\nrun-pairsam-parse-sort.sh <input_bam> <chromsizes> <outdir> <outprefix> <nthread> <compress_program>\n# input_bam : an input bam file.\n# chromsizes : a chromsize file\n# outdir : output directory\n# outprefix : prefix of output files\n# nthread : number of threads to use\n\n3. run-pairsam-merge.sh:\nMerges a list of pairsam files\nInput: a list of pairsam files\nOutput: a merged pairsam file\nUsage\nRun the following in the container\nrun-pairsam-merge.sh <outprefix> <nthreads> <input_pairsam1> [<input_pairsam2> [<input_pairsam3> [...]]]\n# outprefix : prefix of output files\n# nthreads : number of threads to use\n# input_pairsam : an input pairsam file.\n\n4. run-pairsam-markasdup.sh:\nTakes a pairsam file in and creates a pairsam file with duplicate reads marked\nInput: a pairsam file\nOutput: a duplicate-marked pairsam file\nUsage\nRun the following in the container\nrun-pairsam-markasdup.sh <input_pairsam>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n\n5. run-pairsam-filter.sh:\nTakes in a pairsam file and creates a lossless, annotated bam file and a filtered pairs file.\nInput: a pairsam file\nOutput: an annotated bam file and a filtered pairs file\nUsage\nRun the following in the container\nrun-pairsam-filter.sh <input_pairsam> <outprefix> <chromsizes>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n# chromsizes : a chromsize file\n\n6. run-merge-pairs.sh:\nAlignment module for Hi-C data, based on merge-pairs.\nInput : a set of pairs files, with their associated indices\nOutput : a merged pairs file and its index\nUsage\nRun the following in the container.\nrun-merge-pairs.sh <output_prefix> <pairs1> <pairs2> [<pairs3> [...]]\n# output_prefix : prefix of the output pairs file.\n# pairs1, pairs2, ... : input pairs files\n\n7. run-cooler.sh:\nRuns cooler to create an unnormalized matrix .cool file, taking in a (4dn-style) pairs file\nInput : a pairs file (.gz, along with .px2), chrom.size file\nOutput : a contact matrix file (.cool)\nUsage\nRun the following in the container.\nrun-cooler.sh <input_pairs> `<chromsize>` `<binsize>` `<ncores>` <output_prefix> <max_split>\n# input_pairs : a pairs file\n# chromsize : a chromsize file\n# binsize : binsize in bp\n# ncores : number of cores to use\n# output_prefix : prefix of the output cool file\n# max_split : max_split argument for cooler (e.g. 2 which is default for cooler)\n\n8. run-cooler-balance.sh:\nRuns cooler to create a normalized matrix file, taking in an unnormalized .cool file\nInput: a cool file (.cool)\nOutput : a cool file (.cool)\nUsage\nRun the following in the container.\nrun-cooler-balance.sh <input_cool> <max_iter> <output_prefix> <chunksize>\n# input_cool : a cool file (without normalization vector)\n# max_iter : maximum number of iterations\n# output_prefix : prefix of the output cool file\n# chunksize : chunksize argument for cooler (e.g. 10000000 which is default for cooler)\n\n9. run-cool2multirescool.sh:\nRuns cooler coarsegrain to create multi-res cool file from a .cool file.\nInput : a cool file (.cool)\nOutput : a multires.cool file (.multires.cool)\nUsage\nRun the following in the container.\nrun-cool2multirescool.sh -i <input_cool> [-p <ncores>] [-o <output_prefix>] [-c <chunksize>] [-j] [-u custom_res] [-B]\n# input_cool : a (single-res) cool file with the highest resolution you want in the multi-res cool file\n# -p ncores: number of cores to use (default: 1)\n# -o output_prefix: prefix of the output multires.cool file (default: out)\n# -c chunksize : chunksize argument of cooler (e.g. default: 10000000)\n# -j : juicer resolutions (default: use HiGlass resolutions)\n# -u custom_res : custom resolutions separated by commas (e.g. 100000,200000,500000). The minimum of this set must match min_res (-r).\n# -B : no balancing/normalizations",
        "metadata": {
            "source": "workflow",
            "page": 15
        }
    },
    {
        "content": "Fully ChIP-seq Peak Calling  with IgG Control Workflow: Step 1: Quality Control - Conduct quality control on raw sequencing data to ensure data integrity and quality.  This step involves checking read quality scores, identifying adapter sequences, and trimming low-quality bases.  Input Required: Raw FASTQ files.  Expected Output: Cleaned and quality-checked FASTQ files.  Tools Used: FastQC, Trimmomatic, Cutadapt.  Step 2: Alignment - Align reads to the reference genome to map sequencing data to genomic coordinates.  Input Required: Cleaned FASTQ files from Step 1 and the reference genome.  Expected Output: Sorted BAM file containing aligned reads.  Tools Used: BWA-MEM, Bowtie2, STAR.  Step 3: SAM/BAM Conversion & Processing - Convert SAM files to BAM format, sort them, and remove PCR duplicates to improve downstream analysis accuracy.  Input Required: SAM file from Step 2.  Expected Output: De-duplicated BAM file.  Tools Used: SAMtools, Picard.  Step 4: Signal Track Generation - Generate signal track files for visualizing ChIP enrichment in genome browsers such as IGV.  Input Required: De-duplicated BAM file from Step 3.  Expected Output: BigWig signal track file.  Tools Used: deeptools, bedGraphToBigWig.  Step 5: Peak Calling - Call peaks to identify enriched genomic regions for CBX7 binding using IgG as control.  Input Required: De-duplicated BAM file from Step 3 and IgG control BAM file.  Expected Output: NarrowPeak file containing identified peaks.  Tools Used: MACS3.",
        "metadata": {
            "source": "workflow",
            "page": 16
        }
    },
    {
        "content": "### Fully ChIP-seq Perform Functional Enrichment Workflow for protein Ring1B, use protein IgGold as the control \n\n**Step 1: Raw Data Quality Control and Preprocessing**\n- **Description**: Conduct an initial quality assessment of raw sequencing data, trimming low-quality bases and removing adapter sequences.\n- **Input Required**: Raw FASTQ files\n- **Expected Output**: Cleaned FASTQ files with high-quality reads\n- **Tools Used**: FastQC\n\n---\n\n**Step 2: Alignment to the Reference Genome**\n- **Description**: Map the cleaned reads to a reference genome to generate alignment files.\n- **Input Required**: Cleaned FASTQ files, reference genome sequence\n- **Expected Output**: Aligned reads in BAM format\n- **Tools Used**: BWA\n\n---\n\n**Step 3: Peak Calling**\n- **Description**: Identify enriched regions (peaks) that represent likely protein-DNA interaction sites.\n- **Input Required**: Aligned BAM file (from Step 2)\n- **Expected Output**: Peak file (BED format) indicating enriched regions\n- **Tools Used**: MACS3\n\n---\n\n**Step 4: Peak Filtering and Annotation**\n- **Description**: Filter out low-confidence peaks and annotate the remaining peaks with genomic features.\n- **Input Required**: Peak file (from Step 3), genome annotation data\n- **Expected Output**: High-confidence, annotated peak list\n- **Tools Used**: HOMER\n\n---\n\n**Step 5: Functional Enrichment Analysis**\n- **Description**: Perform GO and pathway enrichment on genes associated with annotated peaks to uncover biological processes.\n- **Input Required**: Gene list derived from annotated peaks\n- **Expected Output**: Enriched GO terms and pathway results\n- **Tools Used**: DAVID, ClusterProfiler, Enrichr\n\n---\n\n**Step 6: Motif Analysis**\n- **Description**: Extract sequences from enriched peak regions and identify over-represented motifs.\n- **Input Required**: DNA sequences of peak regions\n- **Expected Output**: List of enriched motifs and corresponding transcription factors\n- **Tools Used**: HOMER\n\n---\n\n**Step 7: Visualization**\n- **Description**: Generate visualizations (heatmaps, bar plots, genome browser tracks) and compile a final report.\n- **Input Required**: Outputs from previous steps\n- **Expected Output**: Visual figures and a comprehensive analysis report\n- **Tools Used**: Python (matplotlib) for data visualization",
        "metadata": {
            "source": "workflow",
            "page": 17
        }
    },
    {
        "content": "Predict Fusion genes using STAR-Fusion workflow :Below is a detailed, step-by-step workflow for RNA-Seq Fusion Gene Detection using STAR and STAR-Fusion, formatted similarly to the example provided.\n\n## Detailed RNA-Seq Fusion Gene Detection Workflow\n\n Step 1: RNA-Seq Data QC and Read Mapping\n**Description:**\n1. (Conduct quality control (QC) on raw FASTQ files using tools like FastQC and Trimmomatic to remove low-quality reads and adapter contaminants.\n  Step 2. Use STAR to align the (cleaned) RNA-Seq reads to the reference genome. Ensure you enable chimeric (fusion) detection by setting parameters such as --chimOutType Junctions so that STAR outputs Chimeric.out.junction files.\n\n**Input Required:**\n- Raw or cleaned RNA-Seq FASTQ files\n- STAR genome index \n\n**Expected Output:**\n- Alignment files: Aligned.out.sam or Aligned.sortedByCoord.out.bam\n- Chimeric junction file: Chimeric.out.junction (key for fusion detection)\n\n**Tools Used:**\n- FastQC, Trimmomatic (or equivalent) for QC\n- STAR for alignment\n\n---\n\n### Step3: Fusion Gene Prediction with STAR-Fusion\n**Description:**\n1. Run STAR-Fusion on the resulting STAR alignments (specifically the Chimeric.out.junction file) using the prepared STAR-Fusion genome library.\n2. (Optional) Enable FusionInspector (e.g., --FusionInspector validate) for deeper validation and to generate more comprehensive information about candidate fusion genes.\n\n**Input Required:**\n- Chimeric.out.junction from STAR alignment\n- STAR-Fusion genome library directory\n- (Optionally) the BAM alignment file for additional context\n\n**Expected Output:**\n- A main results file (e.g., star-fusion.fusion_predictions.tsv) containing predicted fusions\n- Additional FusionInspector files and visualization outputs (if used)\n\n**Tools Used:**\n- STAR-Fusion (core tool)\n- (Optionally) FusionInspector (bundled with STAR-Fusion)\n\n---\n\n.",
        "metadata": {
            "source": "workflow",
            "page": 18
        }
    },
    {
        "content": "Detailed RNA-seq Analysis Workflow must include:\n\nStep 1: Quality Control and Preprocessing\nDescription: Perform quality control (QC) on the raw RNA-seq data. This includes checking the quality of raw reads, trimming adapters, and filtering low-quality sequences. FastQC is commonly used for assessing quality, and Cutadapt can be used for trimming.\nInput Required: Raw FASTQ files, sample metadata.\nExpected Output: Cleaned and trimmed FASTQ files ready for alignment.\nTools Used: FastQC, Cutadapt.\n\nStep 2: Alignment to Reference Genome\nDescription: Align the cleaned RNA-seq reads to a reference genome using a suitable aligner, such as STAR. The alignment process produces a SAM or BAM file, which contains the mapped reads.\nInput Required: Trimmed FASTQ files from Step 1, reference genome (FASTA file).\nExpected Output: Aligned SAM/BAM files.\nTools Used: STAR.\n\nStep 3: Quality Control of Aligned Reads\nDescription: Perform a second round of quality control on the aligned reads to assess the alignment efficiency, the number of uniquely mapped reads, and potential biases in the alignment. Tools like SAMtools can be used for this analysis.\nInput Required: Aligned SAM/BAM files from Step 2.\nExpected Output: Alignment QC reports, such as mapping percentage, coverage distribution, and biases.\nTools Used: SAMtools.\n\nStep 4: Gene Quantification\nDescription: Quantify gene expression levels by counting the number of reads mapped to each gene using tools like HTSeq. These tools output gene-level counts that are used for downstream differential expression analysis.\nInput Required: Aligned BAM files from Step 3, gene annotation file (GTF or GFF).\nExpected Output: Gene count matrix.\nTools Used: HTSeq.\n\nStep 5: Differential Expression Analysis\nDescription: Perform differential gene expression analysis to identify genes that are significantly upregulated or downregulated between different conditions. This is typically done using DESeq2.\nInput Required: Gene count matrix from Step 4, experimental design information.\nExpected Output: Differentially expressed genes (DEGs), p-values, fold changes.\nTools Used: DESeq2.\n\nStep 6: Data Normalization\nDescription: Normalize the raw gene counts to account for differences in sequencing depth and library size. This step is necessary to ensure accurate differential expression results. Methods like TMM (Trimmed Mean of M-values) or RPKM/FPKM (Reads/Fragments Per Kilobase of transcript per Million mapped reads) can be used.\nInput Required: Gene count matrix from Step 4.\nExpected Output: Normalized gene count matrix.\nTools Used: DESeq2, or custom scripts.\n\nStep 7: Gene Ontology and Pathway Enrichment Analysis\nDescription: Conduct functional annotation and enrichment analysis to determine which biological processes, molecular functions, or pathways are enriched in the differentially expressed genes. Tools like DAVID, ClusterProfiler can be used for this analysis.\nInput Required: List of differentially expressed genes (from Step 5), gene annotation information.\nExpected Output: Enriched Gene Ontology terms and pathways, plots for visualization.\nTools Used: DAVID, ClusterProfiler.\n\nStep 8: Clustering and Visualization\nDescription: Perform unsupervised clustering to group similar gene expression patterns and visualize the results using techniques like Principal Component Analysis (PCA), hierarchical clustering, or t-SNE.\nInput Required: Normalized gene count matrix from Step 6.\nExpected Output: Clustering results, visualizations (e.g., PCA plots, heatmaps).\nTools Used: Python (scikit-learn, Seaborn).\n\nStep 9: Alternative Splicing Analysis\nDescription: Analyze alternative splicing events to identify differences in splicing patterns between conditions. Tools like rMATS or DEXSeq can be used for this analysis.\nInput Required: Aligned BAM files from Step 3, gene annotation file (GTF or GFF).\nExpected Output: Splicing event counts, significant splicing events.\nTools Used: rMATS, DEXSeq.\n\nStep 10: Visualization and Reporting\nDescription: Generate publication-quality visualizations of the analysis results, such as volcano plots for differential expression, heatmaps for gene clustering, and pathway enrichment plots. Prepare a final report summarizing the RNA-seq analysis.\nInput Required: Differential expression results, clustering results, pathway analysis results.\nExpected Output: Final analysis report, visualizations for publication.\nTools Used: Python (matplotlib, Seaborn).",
        "metadata": {
            "source": "workflow",
            "page": 19
        }
    },
    {
        "content": "This is a workflow for Comprehensive Cancer DNA/RNA Analysis using nf-core/oncoanalyser\n\nStep 1: Create Sample Sheet (Input Preparation)\nDescription: Prepare a tab‐delimited sample sheet listing each sample’s identifiers and file paths. This file informs the pipeline about the type of sequencing data (e.g. WGS, WTS) and associated metadata.  \nInput Required: Sequencing data file paths (FASTQ or BAM) and metadata details (group_id, subject_id, sample_id, sample_type, sequence_type, filetype, info, filepath).  \nExpected Output: A well-formatted CSV file (e.g. samplesheet.csv) for pipeline input.  \nTool Used: Text editor (or spreadsheet software)  \n\nStep 2: DNA Alignment with bwa‐mem2  \nDescription: Align DNA FASTQ reads to the reference genome using an efficient short-read mapper.  \nInput Required: DNA FASTQ files and reference genome (e.g. GRCh38_hmf).  \nExpected Output: Aligned DNA BAM files with mapped reads.  \nTool Used: bwa‐mem2  \n\nStep 3: RNA Alignment with STAR  \nDescription: Align RNA FASTQ reads to the reference genome/transcriptome to capture spliced transcripts.  \nInput Required: RNA FASTQ files and corresponding reference genome/transcriptome index.  \nExpected Output: RNA alignment files (BAM) with spliced alignments.  \nTool Used: STAR  \n\nStep 4: Duplicate Marking with MarkDups  \nDescription: Identify and mark duplicate reads in the aligned BAM files to reduce PCR artifacts.  \nInput Required: Aligned BAM files (from DNA and/or RNA alignments).  \nExpected Output: BAM files with duplicate reads marked (typically flagged in the header).  \nTool Used: MarkDups  \n\nStep 5: Duplicate Marking with Picard MarkDuplicates  \nDescription: Alternatively, mark duplicate reads using a widely adopted toolkit to eliminate PCR artifacts.  \nInput Required: Aligned BAM files resulting from DNA or RNA read mapping.  \nExpected Output: Processed BAM files with duplicates flagged for downstream analysis.  \nTool Used: Picard MarkDuplicates  \n\nStep 6: SNV, MNV, and INDEL Calling with SAGE  \nDescription: Detect single nucleotide variants (SNVs), multi-nucleotide variants (MNVs), and small insertions/deletions (INDELs) from processed alignments.  \nInput Required: Processed BAM files from duplicate marking.  \nExpected Output: Variant call files (VCFs) with SNVs, MNVs, and INDELs.  \nTool Used: SAGE  \n\nStep 7: SNV, MNV, and INDEL Calling with PAVE  \nDescription: Optionally, perform variant calling for SNVs, MNVs, and INDELs with an alternative algorithm to validate or complement the SAGE output.  \nInput Required: Processed BAM files from duplication marking.  \nExpected Output: VCF files with detected SNVs, MNVs, and INDELs.  \nTool Used: PAVE  \n\nStep 8: CNV Calling with AMBER  \nDescription: Initiate copy-number variation (CNV) analysis by quantifying allele-specific copy numbers across the genome.  \nInput Required: Aligned and duplicate-marked BAM files.  \nExpected Output: CNV segmentation files indicating allele-specific copy number estimates.  \nTool Used: AMBER  \n\nStep 9: CNV Calling with COBALT  \nDescription: Complement the CNV analysis by measuring read depth based on binning strategies across the genome.  \nInput Required: Aligned BAM files along with a binning reference for read depth calculation.  \nExpected Output: Binned read-depth profiles for copy-number estimation.  \nTool Used: COBALT  \n\nStep 10: CNV Integration with PURPLE  \nDescription: Integrate allele-specific CNV data (from AMBER and COBALT) with variant allele frequencies and tumor purity estimates to infer final copy-number calls.  \nInput Required: Outputs from AMBER (allele-specific segmentation) and COBALT (read-depth profiles), and variant calls.  \nExpected Output: Final CNV calls with integrated information about copy numbers and purity estimates.  \nTool Used: PURPLE  \n\nStep 11: SV Preprocessing with SvPrep  \nDescription: Preprocess alignments for structural variant (SV) detection by reformatting and quality filtering the data.  \nInput Required: Aligned and duplicate-marked BAM files from earlier steps.  \nExpected Output: Reformatted files prepared for input into SV detection tools.  \nTool Used: SvPrep  \n\nStep 12: SV Calling with GRIDSS  \nDescription: Identify structural variants (SVs) such as insertions, deletions, and rearrangements in the genomic data.  \nInput Required: Processed BAM files or output from SvPrep.  \nExpected Output: Raw SV call files indicating breakpoints or rearrangements in the genome.  \nTool Used: GRIDSS  \n\nStep 13: SV Calling with GRIPSS  \nDescription: Alternatively, run a complementary method for SV detection to capture additional variants or refine the breakpoints.  \nInput Required: Preprocessed alignment files from SvPrep.  \nExpected Output: SV call files detailing candidate structural variants.  \nTool Used: GRIPSS  \n\nStep 14: SV Event Interpretation with LINX  \nDescription: Interpret and cluster structural variant events in the context of cancer genomes to deduce genomic rearrangements and complex events.  \nInput Required: SV call files generated from GRIDSS and/or GRIPSS.  \nExpected Output: Interpreted and annotated SV event files with biological context.  \nTool Used: LINX  \n\nStep 15: Transcript Analysis with Isofox  \nDescription: Analyze transcriptome data to quantify gene expression and detect transcript isoforms relevant for cancer profiling.  \nInput Required: RNA alignment files (BAM) from STAR.  \nExpected Output: Expression matrices or transcript isoform reports for downstream analysis.  \nTool Used: Isofox  \n\nStep 16: Oncoviral Detection with VIRUSBreakend  \nDescription: Detect oncoviral integration events by scanning for breakends that could indicate viral insertions in the host genome.  \nInput Required: Processed BAM files from the alignment steps.  \nExpected Output: Viral integration candidate lists with genomic coordinates.  \nTool Used: VIRUSBreakend  \n\nStep 17: Viral Interpretation with Virus Interpreter  \nDescription: Annotate and interpret the detected viral integration events, providing clinical and biological insights.  \nInput Required: Oncoviral candidate events generated by VIRUSBreakend.  \nExpected Output: Detailed viral event reports with annotated significance.  \nTool Used: Virus Interpreter  \n\nStep 18: HLA Calling with LILAC  \nDescription: Infer HLA alleles from the sequencing data to assess potential immunogenomic markers.  \nInput Required: Aligned BAM files (typically from DNA data) along with reference HLA allele databases.  \nExpected Output: HLA allele calls and reports.  \nTool Used: LILAC  \n\nStep 19: HRD Status Prediction with CHORD  \nDescription: Assess homologous recombination deficiency (HRD) status in cancer samples by leveraging genomic scar signatures and mutational patterns.  \nInput Required: Variant call files (from SAGE/PAVE) and CNV profiles (from PURPLE).  \nExpected Output: HRD status prediction reports for each sample.  \nTool Used: CHORD  \n\nStep 20: Mutational Signature Fitting with Sigs  \nDescription: Decompose the mutational profile of the tumor into known mutational signatures to infer underlying mutagenic processes.  \nInput Required: Catalog of detected SNVs/MNVs/INDELs from earlier variant calling.  \nExpected Output: Fitted mutational signature profiles with quantitative contributions.  \nTool Used: Sigs  \n\nStep 21: Tissue of Origin Prediction with CUPPA  \nDescription: Predict the tissue of origin for cancer samples based on genomic and transcriptomic signatures.  \nInput Required: Combined genomic features (e.g. mutational signatures, CNVs) and transcriptome profiles.  \nExpected Output: Predicted tissue of origin report per sample.  \nTool Used: CUPPA  \n\nStep 22: Report Generation with ORANGE  \nDescription: Generate a comprehensive clinical report summarizing key findings (variants, CNVs, SVs, HRD status, oncogenic drivers, etc.) for each sample.  \nInput Required: Integrated results from individual analyses (variant calls, CNV integration, SV interpretation, transcript analysis, etc.).  \nExpected Output: A visually formatted report (PDF/HTML) summarizing the cancer genomic profile (e.g. ORANGE_report.pdf).  \nTool Used: ORANGE  \n\nStep 23: Report Generation with linxreport  \nDescription: Alternatively, compile a report focused on structural variant events and their interpretation in the clinical context.  \nInput Required: Output from the LINX SV interpretation step and other supporting annotation data.  \nExpected Output: A detailed report providing insights into rearrangement events and clinical annotations.  \nTool Used: linxreport  \n\n",
        "metadata": {
            "source": "workflow",
            "page": 20
        }
    },
    {
        "content": "Gene-Centric Functional Enrichment Analysis Workflow using GeneFEAST must include:Step1: Prepare Configuration File. Description: Based on the available gene-level quantitative data, a configuration file run.yml should be created and edited. This file specifies the input gene list, expression matrices, and analysis parameters for GeneFEAST. Expected Output: A completed configuration file named run.yml. Tools Used: Manual editing or script-based generation of run.yml. Step2: Run GeneFEAST Analysis. Description: Invoke GeneFEAST with the run.yml configuration file. GeneFEAST will perform gene-centric functional enrichment analysis (FEA) and generate a series of visualizations and a navigable HTML report. Expected Output: A result directory containing: Multiple visualization outputs (e.g., expression heatmaps, enrichment summaries); A system-organized and navigable HTML report summarizing the analysis. Key Features of GeneFEAST: Gene-Centric Enrichment Interpretation: Enables easy identification of gene sets driving repeated enrichment signals, with direct reference to the gene-level quantitative data used to define the input. Cross-Study FEA Integration: Supports the comparison of FEA results from multiple studies, highlighting expression patterns of genes that are differentially expressed under at least one condition, and identifying shared enriched terms across conditions. HTML Report Generation: Produces an interactive and systematized report for exploration and hypothesis generation. Advancement of Gene-Centric Hypotheses: Helps connect overlapping enrichment results with their underlying genes and datasets, facilitating hypothesis development and providing key insights for downstream experimental validation.",
        "metadata": {
          "source": "GeneFEAST_workflow",
          "page": 21
        }
      },
      {
        "content": "MAFin: Motif Detection in Multiple Alignment Files must include:Step1: Run MAFin for motif analysis. Description: This CLI tool allows motif detection within MAF (Multiple Alignment Format) files using one of the following search methods: regular expression (regex), K-mers, or Position Weight Matrices (PWMs) in JASPAR format. Users can configure the tool to search within only the reference genome or across all aligned genomes. It also supports scanning both forward and reverse strands. The tool is optimized with parallel processing to accelerate large-scale searches. Expected Input: One or more `.maf` files containing multiple sequence alignments. Expected Output: Default output includes a BED-formatted file named `regex_search_motif_hits.bed`. If `--detailed_report` is specified, the tool additionally produces a JSON file (`ref_genome_regex_search_AGT_motif_hits.json`) and a CSV file (`ref_genome_regex_search_AGT_motif_hits.csv`) that provide an analytical summary of motif hits. Tools Used: MAFin command-line interface, supporting parallel processing and flexible motif formats.",
        "metadata": {
          "source": "MAFin_workflow",
          "page": 22
        }
      },
      {
        "content": "MAFin Usage Instructions must include:Step1: Navigate to Output Directory. Description: Before running MAFin, use `cd` to enter the target directory where output files are expected to be saved. This directory should be empty or cleaned as necessary. Step2: Adjust Input Path If Needed. Description: If the input MAF file is referenced with a relative path (e.g., `./data/file.maf`), ensure to update it according to the current working directory (e.g., `../../data/file.maf`) to guarantee correct file access. Step3: Run MAFin Command. Description: Use the `MAFin` command-line interface with specified arguments. You must choose exactly one motif search type: `--regexes`, `--kmers`, or `--jaspar_file`. Other options include target genome, strand orientation, parallel processing, and reporting format.\n\nExample Command (Regex):\nMAFin path/to/file.maf \\\n    --regexes \"CTGCCCGCA\" \"AGT\" \\\n    --search_in reference \\\n    --reverse_complement no \\\n    --processes 4 \\\n    --detailed_report\n\nCommand-Line Arguments:\n- maf_file: (Required) Path to the uncompressed MAF file.\n- --genome_ids: Path to the genome IDs file (optional).\n- --search_in: Genome to search motifs in. Options: 'reference' (default) or 'all'.\n- --reverse_complement: Search both strands if 'yes'. Default is 'no'.\n- --pvalue_threshold: P-value threshold for PWM matches. Default: 1e-4.\n- --processes: Number of parallel processes. Default: 1.\n- --background_frequencies: Four space-separated floats for A, C, G, T frequencies. Must sum to 1.\n- --purge_results_dir: Remove temporary result directories before running.\n- --verbose, -v: Enable detailed logging to `debug.log`.\n- --detailed_report, -d: Generate additional CSV and JSON reports.\n\nSearch Types (choose one only):\n- --regexes: Provide inline regex patterns (e.g., \"AGT\", \"CTG[CC]+CGCA\").\n- --kmers: Path to file with K-mers, one per line.\n- --jaspar_file: Path to a motif file in JASPAR PWM format.\n\nAdditional Examples:\n• Regex search across all genomes and both strands:\nMAFin path/to/file.maf \\\n    --regexes \"CTG[CC]+CGCA\" \"AGT\" \\\n    --search_in all \\\n    --verbose \\\n    --reverse_complement yes \\\n    --processes 8 \\\n    --purge_results_dir\n\n• K-mer search:\nMAFin path/to/file.maf \\\n    --kmers data/kmers.txt \\\n    --search_in reference \\\n    --processes 2 \\\n    --detailed_report\n\n• PWM motif search:\nMAFin path/to/file.maf \\\n    --jaspar_file data/motif.jaspar \\\n    --search_in all \\\n    --pvalue_threshold 1e-5 \\\n    --background_frequencies 0.25 0.15 0.35 0.25 \\\n    --processes 4",
        "metadata": {
          "source": "MAFin_usage",
          "page": 23
        }
      },
      {
        "content": "Transcriptome Annotation Comparison Planning using rnalib must include:Step1: Prepare Annotation Resources. Description: Gather annotation files (e.g., Gencode, Chess, FlyBase, MirGeneDB) and define filters for target gene types or genomic regions. Expected Input: Gene annotation files (.gff/.gtf), reference genome sequences, optional alias mapping files. Expected Output: Configured transcriptome datasets ready for comparative analysis. Step2: Perform Comparative Analysis. Description: Compare gene sets or transcript structures across annotations, assessing differences in gene naming, genomic coordinates, splice junction motifs, and CDS lengths. Expected Input: Prepared transcriptome datasets from Step1. Expected Output: Statistical summaries, comparative plots, lists of unique/shared annotations, and optionally visualizations (e.g., IGV tracks).",
        "metadata": {
          "source": "rnalib_analysis_planning",
          "page": 24
        }
      },
      {
        "content": "NichePCA: PCA-based Spatial Domain Identification for Single-Cell Spatial Transcriptomics Data must include: Step1: Run NichePCA Analysis. Description: NichePCA is designed for identifying spatial domains in single-cell spatial transcriptomics data through Principal Component Analysis (PCA). It performs dimensionality reduction on spatially resolved gene expression profiles and identifies distinct spatial niches or domains. Expected Input: A single `.h5ad` file containing spatial transcriptomics data, including gene expression matrices and spatial coordinates. Expected Output: Visualization results, typically as spatial plots highlighting domain structures inferred from PCA. Tools Used: NichePCA module or script, typically integrated within a Python analysis pipeline (e.g., using Scanpy or custom scripts).",
        "metadata": {
          "source": "NichePCA_workflow",
          "page": 25
        }
      },
      {
        "content": "WGATools: A Cross-Platform and Ultrafast Toolkit for Whole Genome Alignment File Manipulation must include: Step1: Select Command Based on Task. Description: WGATools provides a suite of high-performance subcommands for converting, analyzing, and visualizing alignment files in MAF, PAF, and CHAIN formats. Users can convert between formats (e.g., `maf2paf`, `paf2chain`), extract and view MAF content (`maf-ext`, `tview`), perform dotplot visualizations (`dotplot`), call variants (`call`), compute statistics (`stat`), filter or rename records, and calculate alignment coverage (`pafcov`). WGATools also supports chaining commands with stdin/stdout, indexing MAF files, generating shell completions, and fixing invalid PAF files. Expected Input: A variety of alignment files in MAF, PAF, or CHAIN format. Optional inputs include reference and query FASTA files when required by format conversion. Expected Output: Converted alignment files, filtered or renamed alignments, interactive HTML dotplots, statistical summaries, extracted region outputs, and VCF variant calls. Tools Used: WGATools command-line interface with multi-threading support, compatible with Linux, macOS, and Windows. Example Commands: `wgatools maf2paf test.maf > test.paf` for format conversion; `wgatools dotplot test.maf -m overview > out.html` for visualization; `wgatools call test.paf -s -l0 --target target.fa --query query.fa -f paf` for variant calling. Advanced Features: Soft-filtering, navigation-based terminal viewing, and output compression support.",
        "metadata": {
          "source": "wgatools_manual",
          "page": 26
        }
      },
      {
        "content":"Squidpy is a tool for the analysis and visualization of spatial molecular data. It builds on top of scanpy and anndata, from which it inherits modularity and scalability. It provides analysis tools that leverages the spatial coordinates of the data, as well as tissue images if available.",
         "metadata": {
            "source": "tools",
            "page": 27
        }
    },
    {
        "content": "Spatial Transcriptomics Analysis Workflow with Scanpy\n\n**Step 1: Data Reading and Initial Setup**\n- **Description**: Import necessary libraries and read spatial transcriptomics data into an AnnData object.\n- **Input Required**: Spatial transcriptomics dataset (e.g., 10x Genomics Visium data)\n- **Expected Output**: AnnData object containing gene expression matrix and spatial coordinates\n- **Tools Used**: Scanpy, pandas, matplotlib\n\n---\n\n**Step 2: Quality Control and Metrics Calculation**\n- **Description**: Calculate quality metrics and visualize their distributions to guide filtering decisions.\n- **Input Required**: AnnData object with raw counts\n- **Expected Output**: AnnData object with calculated quality metrics (n_genes_by_counts, total_counts, pct_counts_mt)\n- **Tools Used**: Scanpy (pp.calculate_qc_metrics)\n\n---\n\n**Step 3: Cell and Gene Filtering**\n- **Description**: Filter out low-quality spots and rarely expressed genes based on quality metrics.\n- **Input Required**: AnnData object with quality metrics\n- **Expected Output**: Filtered AnnData object with high-quality spots and genes\n- **Tools Used**: Scanpy (pp.filter_cells, pp.filter_genes)\n\n---\n\n**Step 4: Normalization and Feature Selection**\n- **Description**: Normalize gene expression data and identify highly variable genes for downstream analysis.\n- **Input Required**: Filtered AnnData object\n- **Expected Output**: Normalized AnnData object with identified highly variable genes\n- **Tools Used**: Scanpy (pp.normalize_total, pp.log1p, pp.highly_variable_genes)\n\n---\n\n**Step 5: Dimensionality Reduction and Clustering**\n- **Description**: Perform PCA, compute nearest neighbors, run UMAP for visualization, and cluster spots based on transcriptional similarity.\n- **Input Required**: Normalized AnnData object with highly variable genes\n- **Expected Output**: AnnData object with PCA, UMAP embeddings, and cluster assignments\n- **Tools Used**: Scanpy (pp.pca, pp.neighbors, tl.umap, tl.leiden)\n\n---\n\n**Step 6: Spatial Visualization**\n- **Description**: Visualize quality metrics, cluster assignments, and gene expression in spatial coordinates.\n- **Input Required**: AnnData object with cluster assignments\n- **Expected Output**: Spatial plots showing the distribution of clusters and gene expression\n- **Tools Used**: Scanpy (pl.spatial)\n\n---\n\n**Step 7: Marker Gene Identification and Analysis**\n- **Description**: Identify marker genes for each cluster and visualize their expression patterns.\n- **Input Required**: AnnData object with cluster assignments\n- **Expected Output**: Ranked lists of marker genes for each cluster, heatmaps, and spatial plots of marker gene expression\n- **Tools Used**: Scanpy (tl.rank_genes_groups, pl.rank_genes_groups_heatmap)\n\n---\n\n**Step 8: Spatial Gene Expression Pattern Analysis**\n- **Description**: Identify genes with spatial expression patterns using specialized tools.\n- **Input Required**: Normalized counts and spatial coordinates\n- **Expected Output**: List of genes with significant spatial expression patterns\n- **Tools Used**: SpatialDE, SPARK, or other spatial statistics tools",
         "metadata": {
          "source": "workflow",
          "page": 28
        }
      },
    {
        "content":"spaceranger:SpaceRanger is a set of analysis pipelines that process 10x Genomics Visium data with brightfield or fluorescence microscope images, allowing users to map the whole transcriptome in a variety of tissues. This tool has been installed,just use spaceranger command dierectly,do not download spaceranger repeatly.e.g., spaceranger count --id=Visium_FFPE_Mouse_Brain_RUN1 \n--create-bam=false \n --transcriptome=/path/to/refdata-gex-mm10-2020-A \n—-probe-set=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_probe_set.csv \n --fastqs=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_fastqs \n--image=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_image.jpg \n--slide=V11J26-127 \n--area=B1",
        "metadata":{
            "source": "tools",
            "page":29
        }
    },
    {
        "content": "This is a pipeline for rna splicing analysis of rna sequencing data from organisms with a reference genome and annotation. Follow these steps in order, and adjust file paths and parameters as needed:\n\n1. **Merge Re-sequenced FastQ Files**\n   - **Tool:** Unix `cat`\n   - **Purpose:** Concatenate multiple FastQ files into a single file for further processing.\n   - **Example:**\n     ```bash\n     cat sample_part1.fastq sample_part2.fastq > merged_sample.fastq\n     ```\n\n2. **Read Quality Control**\n   - **Tool:** FastQC\n   - **Purpose:** Assess the quality of raw sequencing reads and generate quality reports.\n   - **Example:**\n     ```bash\n     fastqc merged_sample.fastq -o qc_reports/\n     ```\n\n3. **Adapter and Quality Trimming**\n   - **Tool:** TrimGalore\n   - **Purpose:** Remove adapter sequences and trim low-quality bases from the reads.\n   - **Example:**\n     ```bash\n     trim_galore --quality 20 --fastqc merged_sample.fastq -o trimmed_reads/\n     ```\n\n4. **Alignment with STAR**\n   - **Tool:** STAR\n   - **Purpose:** Align trimmed reads to the reference genome.\n   - **Example:**\n     ```bash\n     STAR --runThreadN 8 --genomeDir /path/to/STAR_index --readFilesIn trimmed_reads/merged_sample_trimmed.fq --outFileNamePrefix alignments/sample_\n     ```\n\n5. **Quantification Options Depending on Analysis Type**\n   - **Option A:** Quantification with Salmon using STAR alignments\n     - **Example:**\n       ```bash\n       salmon quant -t transcripts.fa -l A -a alignments/sample_Aligned.out.sam -o quant_salmon/\n       ```\n   - **Option B:** Quantification with featureCounts using STAR alignments\n     - **Example:**\n       ```bash\n       featureCounts -T 8 -a annotation.gtf -o counts_featureCounts.txt alignments/sample_Aligned.out.sam\n       ```\n   - **Option C:** Quantification with HTSeq for DEXSeq counts\n     - **Example:**\n       ```bash\n       htseq-count -f sam -r pos -s no -t exon -i gene_id alignments/sample_Aligned.out.sam annotation.gtf > counts_htseq.txt\n       ```\n\n6. **Sort and Index Alignments**\n   - **Tool:** SAMtools\n   - **Purpose:** Convert SAM to BAM, sort, and index the alignments for efficient downstream analysis.\n   - **Example:**\n     ```bash\n     samtools view -bS alignments/sample_Aligned.out.sam | samtools sort -o alignments/sample_sorted.bam\n     samtools index alignments/sample_sorted.bam\n     ```\n\n7. **Create bigWig Coverage Files**\n   - **Tools:** BEDTools and bedGraphToBigWig\n   - **Purpose:** Generate bigWig files for visualization of read coverage across the genome.\n   - **Example:**\n     ```bash\n     bedtools genomecov -ibam alignments/sample_sorted.bam -bg > coverage/sample.bedGraph\n     bedGraphToBigWig coverage/sample.bedGraph /path/to/chrom.sizes coverage/sample.bw\n     ```\n ```\n\n9. **Summarize Quality Control**\n   - **Tool:** MultiQC\n   - **Purpose:** Aggregate and summarize QC reports from FastQC, TrimGalore, and other tools into a single report.\n   - **Example:**\n     ```bash\n     multiqc qc_reports/ trimmed_reads/ alignments/ -o multiqc_report/\n     ```\n\n10. **Differential Exon Usage (DEU) Analysis**\n    - **Using HTSeq:**\n      - Quantify with HTSeq and perform DEU analysis with DEXSeq.\n    - **Using featureCounts:**\n      - Quantify with featureCounts and perform DEU analysis with edgeR.\n    - **Example Commands:**\n      (The quantification step is shown in step 5, and downstream DEU analysis is conducted using R packages such as DEXSeq or edgeR.)\n\n11. **Differential Transcript Usage (DTU) Analysis**\n    - **Workflow:**\n      - Quantify with Salmon → Filter and analyze with DRIMSeq → Perform DTU analysis with DEXSeq.\n    - **Example:**\n      (DTU analysis is performed within an R environment using the output from Salmon and subsequent DRIMSeq filtering, followed by differential testing with DEXSeq.)\n\n12. **Event-Based Splicing Analysis**\n    - **Option A:** Use STAR alignments as input for rMATS to detect alternative splicing events.\n      - **Example:**\n        ```bash\n        rmats.py --b1 sample1_bam_list.txt --b2 sample2_bam_list.txt --gtf annotation.gtf --od rmats_output/ --tmp rmats_tmp/ -t paired --readLength 100 --nthread 8\n        ```\n    - **Option B:** Use Salmon quantification as input for SUPPA2 to analyze splicing events.\n      - **Example:**\n        ```bash\n        suppa.py generateEvents -i annotation.gtf -o suppa_events -f ioe -e SE SS MX RI\n        suppa.py psiPerEvent -i suppa_events.ioe -e quant_salmon/quant.sf -o suppa_psi\n        ```\n\nThis pipeline outlines the major steps and options available for alternative splicing analysis of RNA-seq data using a reference genome and annotation. Adjust each command and parameter to suit your specific data and analysis requirements.",
        "metadata": {
            "source": "workflow",
            "page": 30
        }
    },
    {
        "content": "rMATS (replicate Multivariate Analysis of Transcript Splicing) is a bioinformatics tool used to identify and analyze alternative splicing events from RNA-Seq data. It supports biological replicates and can detect five types of splicing events", 
        "metadata": {
            "source": "tools",
            "page": 31
        }
    },
    {
        "content": "Ribo-seq Data Analysis Workflow for Identifying Translated ORFs:\n\nStep 1: Adapter and Quality Trimming (Cutadapt)\nDescription: Remove adapter sequences and low-quality bases from raw Ribo-seq reads.\nInput Required: Raw FASTQ files containing Ribo-seq reads.\nExpected Output: Cleaned FASTQ files with adapter sequences and low-quality bases removed.\nTools Used: Cutadapt.\n\nStep 2: rRNA/tRNA Filtering (optional if without rRNA/tRNA file) (Bowtie)\nDescription: Align reads to rRNA and tRNA sequences to remove contamination.\nInput Required: Cleaned FASTQ files from Step 1.\nExpected Output: FASTQ files with non-rRNA/tRNA reads.\nTools Used: Bowtie.\n\nStep 3: Genome or Transcriptome Mapping\nDescription: Align filtered reads to the reference genome or transcriptome to determine ribosome positions.\nInput Required: Filtered FASTQ files from Step 2.\nExpected Output: BAM files with mapped ribosome footprints.\n\nStep 4: Indendetection of short and long active ORFs \n Tools Used: ribotricer",
        "metadata": {
          "source": "workflow",
          "page": 32
        }
    },
    {
        "content": "CLIP-seq Data Analysis Workflow for Identifying Protein-RNA Cross-Link Sites: Step 1: Adapter and Quality Trimming (Cutadapt) Description: Remove adapter sequences and perform quality trimming to prepare high-quality reads for downstream analysis. Input Required: Raw FASTQ files containing sequencing reads. Expected Output: Trimmed FASTQ files with adapter sequences removed and low-quality bases trimmed. Tools Used: Cutadapt. Step 2: Pre-mapping to rRNA and tRNA Sequences (Bowtie 2) Description: Align reads to rRNA and tRNA sequences to filter out non-specific reads. Input Required: Trimmed FASTQ files from Step 1. Expected Output: BAM files with aligned reads to rRNA and tRNA sequences. Tools Used: Bowtie 2. Step 3: Genome Mapping (STAR) Description: Align filtered reads to the reference genome for accurate mapping. Input Required: BAM files from Step 2. Expected Output: BAM files with reads mapped to the genome. Tools Used: STAR aligner. Step 4: UMI-based Deduplication (UMI-tools) Description: Identify and collapse PCR duplicates based on Unique Molecular Identifiers (UMIs) to ensure data accuracy. Input Required: BAM files from Step 3. Expected Output: Deduplicated BAM files. Tools Used: UMI-tools. Step 5: Crosslink Identification (BEDTools) Description: Identify crosslink sites by analyzing the deduplicated reads. Input Required: Deduplicated BAM files from Step 4. Expected Output: BED files containing identified crosslink sites. Tools Used: BEDTools.",
        "metadata": {
            "source": "workflow",
            "page": 33
        }
    },
    {
        "content": "RIP-seq Data Analysis Workflow for Identifying Enriched Genes Bound by RNA-Binding Proteins (RBPs): Step 1: Adapter and Quality Trimming (Cutadapt) Description: Remove adapter sequences and perform quality trimming to ensure high-quality reads for accurate downstream analysis. Input Required: Raw FASTQ files from RIP and input control samples. Expected Output: Cleaned and trimmed FASTQ files. Tools Used: Cutadapt, FastQC. Step 2: Read Alignment to Reference Genome (STAR) Description: Align cleaned reads to the reference genome to obtain mapped reads. Input Required: Trimmed FASTQ files from Step 1. Expected Output: BAM files with genome-aligned reads. Tools Used: STAR aligner. Step 3: Peak Calling to Identify RBP Binding Regions (exomePeak or CLIPper) Description: Detect RBP-bound regions by comparing RIP samples to input controls. Input Required: BAM files from Step 2. Expected Output: BED or peak files representing significant RBP binding sites. Tools Used: exomePeak (R), CLIPper. Step 4: Annotation of Peaks to Genes (ChIPseeker or BEDTools) Description: Map the detected peaks to genomic features (e.g., UTRs, CDS, promoters) to associate with genes. Input Required: Peak files and genome annotation (GTF). Expected Output: Annotated peak-gene pairs. Tools Used: ChIPseeker, BEDTools. Step 5: Differential Enrichment Analysis (DESeq2) Description: Compare RIP vs input read counts over genes or peaks to find enriched targets. Input Required: Count matrix of reads over genes or peaks. Expected Output: List of significantly enriched genes bound by RBP. Tools Used: DESeq2. Step 6: Visualization and Validation Description: Visualize binding patterns and validate enriched genes. Input Required: Enrichment results and BAM files. Expected Output: Genome browser tracks, heatmaps, and ranked gene lists. Tools Used: IGV, R packages (ggplot2, pheatmap).",
        "metadata": {
          "source": "workflow",
          "page": 34
        }
    },
    {
        "content": "This is a workflow for Comprehensive Cancer DNA/RNA Analysis using nf-core/oncoanalyser\n\nStep 1: Create Sample Sheet (Input Preparation)\nDescription: Prepare a tab‐delimited sample sheet listing each sample’s identifiers and file paths. This file informs the pipeline about the type of sequencing data (e.g. WGS, WTS) and associated metadata.  \nInput Required: Sequencing data file paths (FASTQ or BAM) and metadata details (group_id, subject_id, sample_id, sample_type, sequence_type, filetype, info, filepath).  \nExpected Output: A well-formatted CSV file (e.g. samplesheet.csv) for pipeline input.  \nTool Used: Text editor (or spreadsheet software)  \n\nStep 2: DNA Alignment with bwa‐mem2  \nDescription: Align DNA FASTQ reads to the reference genome using an efficient short-read mapper.  \nInput Required: DNA FASTQ files and reference genome (e.g. GRCh38_hmf).  \nExpected Output: Aligned DNA BAM files with mapped reads.  \nTool Used: bwa‐mem2  \n\nStep 3: RNA Alignment with STAR  \nDescription: Align RNA FASTQ reads to the reference genome/transcriptome to capture spliced transcripts.  \nInput Required: RNA FASTQ files and corresponding reference genome/transcriptome index.  \nExpected Output: RNA alignment files (BAM) with spliced alignments.  \nTool Used: STAR  \n\nStep 4: Duplicate Marking with MarkDups  \nDescription: Identify and mark duplicate reads in the aligned BAM files to reduce PCR artifacts.  \nInput Required: Aligned BAM files (from DNA and/or RNA alignments).  \nExpected Output: BAM files with duplicate reads marked (typically flagged in the header).  \nTool Used: MarkDups  \n\nStep 5: Duplicate Marking with Picard MarkDuplicates  \nDescription: Alternatively, mark duplicate reads using a widely adopted toolkit to eliminate PCR artifacts.  \nInput Required: Aligned BAM files resulting from DNA or RNA read mapping.  \nExpected Output: Processed BAM files with duplicates flagged for downstream analysis.  \nTool Used: Picard MarkDuplicates  \n\nStep 6: SNV, MNV, and INDEL Calling with SAGE  \nDescription: Detect single nucleotide variants (SNVs), multi-nucleotide variants (MNVs), and small insertions/deletions (INDELs) from processed alignments.  \nInput Required: Processed BAM files from duplicate marking.  \nExpected Output: Variant call files (VCFs) with SNVs, MNVs, and INDELs.  \nTool Used: SAGE  \n\nStep 7: SNV, MNV, and INDEL Calling with PAVE  \nDescription: Optionally, perform variant calling for SNVs, MNVs, and INDELs with an alternative algorithm to validate or complement the SAGE output.  \nInput Required: Processed BAM files from duplication marking.  \nExpected Output: VCF files with detected SNVs, MNVs, and INDELs.  \nTool Used: PAVE  \n\nStep 8: CNV Calling with AMBER  \nDescription: Initiate copy-number variation (CNV) analysis by quantifying allele-specific copy numbers across the genome.  \nInput Required: Aligned and duplicate-marked BAM files.  \nExpected Output: CNV segmentation files indicating allele-specific copy number estimates.  \nTool Used: AMBER  \n\nStep 9: CNV Calling with COBALT  \nDescription: Complement the CNV analysis by measuring read depth based on binning strategies across the genome.  \nInput Required: Aligned BAM files along with a binning reference for read depth calculation.  \nExpected Output: Binned read-depth profiles for copy-number estimation.  \nTool Used: COBALT  \n\nStep 10: CNV Integration with PURPLE  \nDescription: Integrate allele-specific CNV data (from AMBER and COBALT) with variant allele frequencies and tumor purity estimates to infer final copy-number calls.  \nInput Required: Outputs from AMBER (allele-specific segmentation) and COBALT (read-depth profiles), and variant calls.  \nExpected Output: Final CNV calls with integrated information about copy numbers and purity estimates.  \nTool Used: PURPLE  \n\nStep 11: SV Preprocessing with SvPrep  \nDescription: Preprocess alignments for structural variant (SV) detection by reformatting and quality filtering the data.  \nInput Required: Aligned and duplicate-marked BAM files from earlier steps.  \nExpected Output: Reformatted files prepared for input into SV detection tools.  \nTool Used: SvPrep  \n\nStep 12: SV Calling with GRIDSS  \nDescription: Identify structural variants (SVs) such as insertions, deletions, and rearrangements in the genomic data.  \nInput Required: Processed BAM files or output from SvPrep.  \nExpected Output: Raw SV call files indicating breakpoints or rearrangements in the genome.  \nTool Used: GRIDSS  \n\nStep 13: SV Calling with GRIPSS  \nDescription: Alternatively, run a complementary method for SV detection to capture additional variants or refine the breakpoints.  \nInput Required: Preprocessed alignment files from SvPrep.  \nExpected Output: SV call files detailing candidate structural variants.  \nTool Used: GRIPSS  \n\nStep 14: SV Event Interpretation with LINX  \nDescription: Interpret and cluster structural variant events in the context of cancer genomes to deduce genomic rearrangements and complex events.  \nInput Required: SV call files generated from GRIDSS and/or GRIPSS.  \nExpected Output: Interpreted and annotated SV event files with biological context.  \nTool Used: LINX  \n\nStep 15: Transcript Analysis with Isofox  \nDescription: Analyze transcriptome data to quantify gene expression and detect transcript isoforms relevant for cancer profiling.  \nInput Required: RNA alignment files (BAM) from STAR.  \nExpected Output: Expression matrices or transcript isoform reports for downstream analysis.  \nTool Used: Isofox  \n\nStep 16: Oncoviral Detection with VIRUSBreakend  \nDescription: Detect oncoviral integration events by scanning for breakends that could indicate viral insertions in the host genome.  \nInput Required: Processed BAM files from the alignment steps.  \nExpected Output: Viral integration candidate lists with genomic coordinates.  \nTool Used: VIRUSBreakend  \n\nStep 17: Viral Interpretation with Virus Interpreter  \nDescription: Annotate and interpret the detected viral integration events, providing clinical and biological insights.  \nInput Required: Oncoviral candidate events generated by VIRUSBreakend.  \nExpected Output: Detailed viral event reports with annotated significance.  \nTool Used: Virus Interpreter  \n\nStep 18: HLA Calling with LILAC  \nDescription: Infer HLA alleles from the sequencing data to assess potential immunogenomic markers.  \nInput Required: Aligned BAM files (typically from DNA data) along with reference HLA allele databases.  \nExpected Output: HLA allele calls and reports.  \nTool Used: LILAC  \n\nStep 19: HRD Status Prediction with CHORD  \nDescription: Assess homologous recombination deficiency (HRD) status in cancer samples by leveraging genomic scar signatures and mutational patterns.  \nInput Required: Variant call files (from SAGE/PAVE) and CNV profiles (from PURPLE).  \nExpected Output: HRD status prediction reports for each sample.  \nTool Used: CHORD  \n\nStep 20: Mutational Signature Fitting with Sigs  \nDescription: Decompose the mutational profile of the tumor into known mutational signatures to infer underlying mutagenic processes.  \nInput Required: Catalog of detected SNVs/MNVs/INDELs from earlier variant calling.  \nExpected Output: Fitted mutational signature profiles with quantitative contributions.  \nTool Used: Sigs  \n\nStep 21: Tissue of Origin Prediction with CUPPA  \nDescription: Predict the tissue of origin for cancer samples based on genomic and transcriptomic signatures.  \nInput Required: Combined genomic features (e.g. mutational signatures, CNVs) and transcriptome profiles.  \nExpected Output: Predicted tissue of origin report per sample.  \nTool Used: CUPPA  \n\nStep 22: Report Generation with ORANGE  \nDescription: Generate a comprehensive clinical report summarizing key findings (variants, CNVs, SVs, HRD status, oncogenic drivers, etc.) for each sample.  \nInput Required: Integrated results from individual analyses (variant calls, CNV integration, SV interpretation, transcript analysis, etc.).  \nExpected Output: A visually formatted report (PDF/HTML) summarizing the cancer genomic profile (e.g. ORANGE_report.pdf).  \nTool Used: ORANGE  \n\nStep 23: Report Generation with linxreport  \nDescription: Alternatively, compile a report focused on structural variant events and their interpretation in the clinical context.  \nInput Required: Output from the LINX SV interpretation step and other supporting annotation data.  \nExpected Output: A detailed report providing insights into rearrangement events and clinical annotations.  \nTool Used: linxreport  \n\n",
        "metadata": {
            "source": "workflow",
            "page": 35
        }
    },
    {
        "content": "Small RNA sequencing data analysis workflow consists of several essential steps:\n\nStep 1: Quality Control and Trimming\nDescription: Perform quality control and adapter trimming on raw sequencing reads.\nInput Required: Raw FASTQ files.\nExpected Output: Trimmed and filtered FASTQ files.\nTools Used: FastQC, fastp.\nCommands:\n  ```bash\n  # Run FastQC to assess raw reads quality\n  fastqc ./data/*.fastq.gz -o ./output/008/\n  \n  # Trim adapters and filter low-quality reads using fastp\n  fastp -i ./data/C1-N1-R1_S4_L001_R1_001.fastq.gz -o ./output/008/trimmed_C1-N1-R1_S4_L001_R1_001.fastq.gz\n  fastp -i ./data/C9-N1-R1_S7_L001_R1_001.fastq.gz -o ./output/008/trimmed_C9-N1-R1_S7_L001_R1_001.fastq.gz\n  fastp -i ./data/Ctl-N1-R1_S1_L001_R1_001.fastq.gz -o ./output/008/trimmed_Ctl-N1-R1_S1_L001_R1_001.fastq.gz\n  ```\n\nStep 2: Prepare miRBase Reference and Convert Annotation Format\nDescription: Download miRBase reference sequences and convert GFF3 annotation to GTF format for downstream analysis.\nInput Required: hsa.gff3 and miRBase_hairpin.fasta from miRBase.\nExpected Output: miRNA annotation in GTF format and reference sequences for alignment.\nTools Used: wget, gffread.\nCommands:\n  ```bash\n  # Download miRBase annotation for human miRNAs (GFF3)\n  wget -O ./data/hsa.gff3 https://www.mirbase.org/download/CURRENT/genomes/hsa.gff3\n  \n  # Download miRNA reference sequences (FASTA)\n  wget -O ./data/miRBase_hairpin.fasta https://www.mirbase.org/download/CURRENT/hairpin.fa\n  \n  # Convert GFF3 to GTF format for featureCounts and downstream analysis\n  gffread ./data/hsa.gff3 -T -o ./data/miRNA_annotation.gtf\n  \n  # Verify output\n  ls -lh ./data/miRNA_annotation.gtf\n  ```\n\nStep 3: Build Bowtie Index for miRBase\nDescription: Build Bowtie1 index for miRBase reference to enable efficient read alignment.\nInput Required: `miRBase_hairpin.fasta`.\nExpected Output: Bowtie index files.\nTools Used: bowtie-build.\nCommands:\n  ```bash\n  # Build Bowtie index for miRBase hairpin sequences\n  bowtie-build ./data/miRBase_hairpin.fasta ./data/miRBase_index\n  \n  # Verify index files\n  ls -lh ./data/miRBase_index*\n  ```\n  **Checkpoint:** If `ls ./data/miRBase_index*` returns empty, the indexing step failed.\n\nStep 4: Read Alignment\nDescription: Align reads to the miRNA reference database (miRBase).\nInput Required: Filtered FASTQ files, miRBase Bowtie index.\nExpected Output: Aligned BAM files.\nTools Used: Bowtie1, SAMtools.\nCommands:\n  ```bash\n  # Align reads to miRBase using Bowtie1\n  bowtie -v 1 -p 16 -x ./data/miRBase_index -U ./output/008/trimmed_C1-N1-R1_S4_L001_R1_001.fastq.gz -S ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.sam\n  bowtie -v 1 -p 16 -x ./data/miRBase_index -U ./output/008/trimmed_C9-N1-R1_S7_L001_R1_001.fastq.gz -S ./output/008/aligned_C9-N1-R1_S7_L001_R1_001.sam\n  bowtie -v 1 -p 16 -x ./data/miRBase_index -U ./output/008/trimmed_Ctl-N1-R1_S1_L001_R1_001.fastq.gz -S ./output/008/aligned_Ctl-N1-R1_S1_L001_R1_001.sam\n  \n  # Convert SAM to BAM, sort, and index\n  samtools view -bS ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.sam | samtools sort -o ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.bam\n  samtools index ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.bam\n  ```\n  **Checkpoint:** Run `samtools flagstat ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.bam`. If `0 + 0 mapped`, alignment failed.\n\nStep 5: miRNA Quantification\nDescription: Quantify miRNA expression levels.\nInput Required: Aligned BAM files and miRNA annotation in GTF format.\nExpected Output: Expression matrix.\nTools Used: featureCounts, HTSeq.\nCommands:\n  ```bash\n  # Run featureCounts to quantify miRNA expression levels\n  featureCounts -a ./data/miRNA_annotation.gtf -o ./output/008/mirna_quantification_results.txt ./output/008/aligned_C1-N1-R1_S4_L001_R1_001.bam ./output/008/aligned_C9-N1-R1_S7_L001_R1_001.bam ./output/008/aligned_Ctl-N1-R1_S1_L001_R1_001.bam\n  ```\n  **Checkpoint:** If `featureCounts` output has all `0` counts, check BAM files.",
        "metadata": {
            "source": "workflow",  
            "page": 36
        }
    },
    {
        "content": "Small RNA sequencing and miRNA prediction workflow consists of the following key steps:\n\nStep 1: Quality Control and Trimming\nDescription: Perform quality control on raw sequencing reads, remove low-quality reads, and trim sequencing adapters.\nCommands:\n  fastqc ./data/*.fastq.gz -o ./output/010/qc_reports/\n  fastp -i ./data/C1.fastq.gz -o ./output/010/trimmed_C1.fastq.gz\n\nStep 2: Genome Alignment\nDescription: Build index from genome and align reads with Bowtie1. Convert to sorted BAM.\nCommands:\n  bowtie-build ./data/genome.fa ./output/010/genome_index\n  bowtie -S -p 16 ./output/010/genome_index ./output/010/trimmed_C1.fastq.gz > ./output/010/C1.sam\n  samtools view -bS ./output/010/C1.sam | samtools sort -o ./output/010/aligned_C1.bam\n  samtools index ./output/010/aligned_C1.bam\nCheckpoint: Use `samtools flagstat` to verify alignments are not empty.\n\nStep 3: miRNA Quantification\nDescription: Use featureCounts with a SAF annotation file.\nCommands:\n  python convert_fasta_to_saf.py -i ./data/miRBase_mature.fasta -o ./data/miRBase_mature.saf\n  featureCounts -T 4 -F SAF -a ./data/miRBase_mature.saf -o ./output/010/miRNA_expression_matrix.txt ./output/010/aligned_C1.bam\nCheckpoint: Ensure expression matrix has non-zero counts.\n\nStep 4: Novel miRNA Discovery\nDescription: Collapse reads and predict novel miRNAs with miRDeep2.\nCommands:\n  mapper.pl ./output/010/trimmed_C1.fastq.gz -e -h -m -l 18 -s ./output/010/reads_collapsed.fa -t ./output/010/reads.arf -r ./data/genome.fa\n  miRDeep2.pl ./output/010/reads_collapsed.fa ./data/genome.fa ./output/010/reads.arf ./data/miRBase_mature.fasta ./data/miRBase_hairpin.fasta -t Human > ./output/010/miRDeep2_results.txt\nCheckpoint: Ensure result file contains predicted miRNAs.",
        "metadata": {
            "source": "workflow",
            "page": 37
        }
    },
    {
        "content": "Small RNA sequencing and miRNA prediction workflow consists of the following key steps:\n\nStep 1: Quality Control and Trimming\nDescription: Perform quality control on raw sequencing reads, remove low-quality reads, and trim sequencing adapters.\nCommands:\n  fastqc ./data/*.fastq.gz -o ./output/010/qc_reports/\n  fastp -i ./data/C1.fastq.gz -o ./output/010/trimmed_C1.fastq.gz\n\nStep 2: Genome Alignment\nDescription: Build index from genome and align reads with Bowtie1. Convert to sorted BAM.\nCommands:\n  bowtie-build ./data/genome.fa ./output/010/genome_index\n  bowtie -S -p 16 ./output/010/genome_index ./output/010/trimmed_C1.fastq.gz > ./output/010/C1.sam\n  samtools view -bS ./output/010/C1.sam | samtools sort -o ./output/010/aligned_C1.bam\n  samtools index ./output/010/aligned_C1.bam\nCheckpoint: Use `samtools flagstat` to verify alignments are not empty.\n\nStep 3: miRNA Quantification\nDescription: Use featureCounts with a SAF annotation file. If the SAF file does not exist, generate it from the miRBase mature FASTA using the provided Python script.\nCommands:\n  python convert_fasta_to_saf.py -i ./data/miRBase_mature.fasta -o ./data/miRBase_mature.saf\n  featureCounts -T 4 -F SAF -a ./data/miRBase_mature.saf -o ./output/010/miRNA_expression_matrix.txt ./output/010/aligned_C1.bam\nCheckpoint: Ensure expression matrix has non-zero counts.\n\nStep 4: Novel miRNA Discovery\nDescription: Collapse reads and predict novel miRNAs with miRDeep2.\nCommands:\n  mapper.pl ./output/010/trimmed_C1.fastq.gz -e -h -m -l 18 -s ./output/010/reads_collapsed.fa -t ./output/010/reads.arf -r ./data/genome.fa\n  miRDeep2.pl ./output/010/reads_collapsed.fa ./data/genome.fa ./output/010/reads.arf ./data/miRBase_mature.fasta ./data/miRBase_hairpin.fasta -t Human > ./output/010/miRDeep2_results.txt\nCheckpoint: Ensure result file contains predicted miRNAs.",
        "metadata": {
            "source": "workflow",
            "page": 38
        }
    },
    {
        "content": "Workflow: Reference-free 3′ end-seq analysis for PAS (polyadenylation site) identification. Step 1: Quality Control and Adapter Trimming Tool: fastp Description: Remove sequencing adapters and low-quality bases from raw 3′ end-seq FASTQ reads. Input: Raw FASTQ file (single-end) Output: Trimmed FASTQ file Step 2: 3′ End Extraction Tool: extract_3p_end Description: Extract the last 20 nucleotides from each read without alignment, producing a FASTA file of 3′ ends. Input: Trimmed FASTQ file Output: FASTA file containing 3′ end reads Note: This is an internal logic operation, not a script or command. Step 3: PAS Clustering Tool: internal logic Description: Cluster identical or similar 3′ end sequences to identify high-confidence polyadenylation sites (PAS). Input: FASTA file with 3′ end reads Output: PAS_sites.tsv listing representative PAS sequences and their abundance Note: This is an internal reasoning step and should not be linked to an external tool.", 
        "metadata": {
            "source": "workflow", 
            "page": 39
        }
    },
    {
        "content": "Step 1: Perform internal logic quality control and adapter trimming on long-read RNA-seq reads (e.g., PacBio) to remove low-quality bases (Q < 10), N bases, and known adapter sequences. Input: raw FASTQ file. Output: trimmed FASTQ file. Tool: internal logic. Step 2: Align the trimmed reads to the mouse reference genome using minimap2 with long-read RNA-seq parameters (-ax splice -uf -k14). Input: trimmed FASTQ file and reference genome (FASTA). Output: SAM file. Tool: minimap2. Step 3: Convert the SAM file to BAM format using samtools. Input: SAM file. Output: BAM file. Tool: samtools. Step 4: Sort and index the BAM file using samtools. Input: BAM file. Output: sorted BAM file. Tool: samtools. Step 5: Quantify transcript isoform expression using featureCounts with GTF annotation. Use options -t exon -g transcript_id to perform isoform-level counting. Input: sorted BAM file and GTF annotation file. Output: isoform expression matrix. Tool: featureCounts. Note: This is a reference-based pipeline optimized for long-read RNA-seq and avoids R entirely.",
        "metadata": {
          "source": "workflow",
          "page": 40
        }
      },
      {
        "content": "CAGE-seq Core Analysis Workflow must include:Step 1: BAM File Validation and Preprocessing Description: Verify BAM file integrity, perform coordinate sorting, and generate index file.Input Required: Raw BAM file with genomic coordinates.Expected Output: Sorted BAM file (sorted.bam) and index file (sorted.bam.bai).Tools Used: samtools.Step 2: TSS Signal Extraction Description: Extract 5' end positions of sequencing reads from sorted BAM file. Input Required: Sorted BAM file (sorted.bam) from Step 1. Expected Output: Text file containing 5' end positions (5prime_positions.txt).Tools Used: samtools + awk. Step 3: TSS Clustering Analysis Description: Perform density-based clustering of 5' end positions to identify transcription start site (TSS) clusters. Input Required: 5' end position file (5prime_positions.txt) from Step2. Expected Output: TSS cluster file in BED format (tss_clusters.bed). Tools Used: paraclu.",
        "metadata": {
            "source": "workflow",
            "page": 41
        }
    },
    
    {
        "content": "DNase-seq analysis workflow must include: Step 1: Data Preprocessing and Quality Control. Description: Remove low-quality sequences, adapter contaminants, and short reads from raw FASTQ files to ensure data reliability. This involves quality assessment and filtering. Input Required: Raw paired-end FASTQ files . Expected Output: Cleaned paired-end FASTQ files, FastQC quality control reports (HTML files in ./fastqc_results). Tools Used: FastQC for quality assessment, Trimmomatic for adapter trimming and quality filtering. Step 2: Sequence Alignment. Description: Align filtered reads to the reference genome using precomputed indices. Input Required: Cleaned FASTQ files from Step 1, reference genome index files. Expected Output: Unsorted SAM file containing aligned reads (e.g., sample_aligned.sam). Tools Used: Bowtie2 for sequence alignment. Step 3: BAM File Processing and Duplicate Removal. Description: Convert SAM to BAM format, sort reads by genomic coordinates, remove PCR duplicates, and generate BAM indices. Input Required: Unsorted SAM file (sample_aligned.sam). Expected Output: Deduplicated and sorted BAM file (sample_dedup.bam), BAM index file (sample_dedup.bam.bai). Tools Used: samtools for format conversion, sorting, and indexing, Picard MarkDuplicates for PCR duplicate removal. Step 4: DHSs Identification (Peak Calling). Description: Identify chromatin open regions (DNase I hypersensitive sites, DHSs) and generate significant peak files. Input Required: Deduplicated BAM file (`sample_dedup.bam`). Expected Output: Peak files (`.narrowPeak`), containing genomic coordinates and statistical significance scores. Tools Used: MACS2 (parameters: `-nomodel --extsize 200`).",
        "metadata": {
            "source": "workflow",
            "page": 42
        }
    },
    {
        "content": "Simplified WGS Bioinformatics Analysis Pipeline (SV Detection) must include: Step 1: Raw Data Quality Control. Description: Remove low-quality sequences and adapter contamination to ensure the reliability of subsequent analyses. Input Required: Raw sequencing files (*.fq.gz). Expected Output: Cleaned high-quality sequencing files (clean_*.fq.gz). Tools Used: fastp, a multi-threaded tool that performs quality control and generates statistical reports in one step. Step 2: Sequence Alignment. Description: Align quality-controlled sequencing reads to the reference genome and generate a sorted BAM file. Input Required: Quality-controlled sequencing files (clean_*.fq.gz), reference genome file. Expected Output: Sorted alignment file (sample.sorted.bam). Tools Used: BWA-MEM. Step 3: Post-alignment Processing. Description: Remove PCR duplicates and calibrate base quality to enhance SV detection accuracy. Input Required: Sorted BAM file (sample.sorted.bam). Expected Output: Deduplicated and recalibrated BAM file (sample.dedup.recal.bam). Tools Used: GATK MarkDuplicates for PCR duplicate removal and GATK BaseRecalibrator for base quality score recalibration (BQSR). Step 4: SV Detection (Core Step). Description: Detect structural variations (SVs) and generate raw VCF results. Input Required: Processed BAM file (sample.dedup.recal.bam). Expected Output: VCF file containing SV loci (e.g., cutesv.vcf). Recommended Tools: cuteSV.",
        "metadata": {
            "source": "workflow",
            "page": 43
        }
    },
    {
        "content": "Protein Mass Spectrometry Analysis Workflow must include:Step 1: Data Preprocessing. Description: Perform denoising, peak alignment, normalization, and quality control (QC) on raw mzML files. Input Required: Raw mass spectrometry file (input.mzML). Expected Output: Preprocessed mass spectrometry file (processed.mzML), Quality control report (QC_report.pdf, including peak intensity distribution and missing value ratio). Tools Used: pyOpenMS (for peak detection and alignment), pandas (for data cleaning). Step 2: Peptide Identification & Quantification. Description: Identify peptides from preprocessed mzML files and perform quantification (e.g., Label-free or TMT labeling). Input Required: Preprocessed mass spectrometry file (processed.mzML), Protein sequence database (e.g., human.fasta). Expected Output: Peptide list file (peptides.tsv, containing peptide sequences and quantitative values). Tools Used: MSFragger (for database searching), pyteomics (for result parsing). Step 3: Protein Identification. Description: Infer proteins from peptides and control false discovery rate (FDR < 1%). Input Required: Peptide list file (peptides.tsv). Expected Output: Identified protein list (protein_ids.csv, with unique protein IDs). Tools Used: Percolator (for FDR validation), pyteomics (for data parsing). Step 4: Protein Quantification. Description: Calculate protein abundances based on peptide-level quantitative results (e.g., Top3 method or intensity summation). Input Required: Identified protein list (protein_ids.csv), Peptide quantification file (peptides.tsv). Expected Output: Protein abundance matrix (protein_quant.csv, rows = proteins, columns = samples). Tools Used: pandas (for data integration), numpy (for abundance calculation). Step 5: Differential Analysis. Description: Identify differentially expressed proteins using statistical tests and correct for multiple hypothesis testing. Input Required: Protein abundance matrix (protein_quant.csv), Sample group information (group_info.csv). Expected Output: List of differentially expressed proteins (diff_proteins.csv, containing log2FC and FDR-corrected p-values). Tools Used: scipy.stats.ttest_ind (for t-tests), statsmodels.stats.multitest (for FDR correction). Step 6: Functional Annotation. Description: Map differentially expressed proteins to Gene Ontology (GO) terms and KEGG pathways. Input Required: List of differentially expressed proteins (diff_proteins.csv), Functional annotation databases (e.g., GO.csv, KEGG_terms.csv). Expected Output: Annotated protein table (annotated_proteins.csv). Tools Used: gseapy (for functional mapping), pandas (for data merging). Step 7: Pathway Enrichment Analysis. Description: Perform pathway enrichment analysis on annotated proteins using hypergeometric or Fisher's exact tests. Input Required: Annotated protein table (annotated_proteins.csv), Pathway database (e.g., KEGG_pathways.gmt). Expected Output: Pathway enrichment results table (pathway_enrichment.csv, including p-values and enriched gene counts). Tools Used: gseapy.enrichr (for enrichment analysis), statsmodels (for statistical testing). Step 8: Protein-Protein Interaction (PPI) Network Analysis. Description: Construct PPI networks for differentially expressed proteins and identify hub proteins. Input Required: List of differentially expressed proteins (diff_proteins.csv), PPI database (e.g., STRING_interactions.tsv). Expected Output: PPI network file (PPI_network.graphml), List of hub proteins (hub_proteins.csv). Tools Used: NetworkX (for network analysis), Cytoscape (for visualization).",
        "metadata": {
            "source": "workflow",
            "page": 44
        }
    },
    {
        "content": "Protein Mass Spectrometry Analysis Workflow must include: Step 1: Read .mzML File. Description: Parse raw mass spectrometry data to extract MS1 spectrum information, including retention time, m/z values, and intensities. Input Required: Raw mass spectrometry file (sample.mzML). Expected Output: A list of Python dictionaries, each containing retention time (rt), m/z values (mz), and intensity values (intensity). Tools Used: pymzml for parsing .mzML files. Step 2: Noise Filtering and Baseline Correction. Description: Remove noise peaks with low signal-to-noise ratio (SNR) through smoothing and baseline thresholding. Input Required: Processed dictionary list from Step 1. Expected Output: Updated dictionary list with only valid peaks (intensity > baseline threshold). Tools Used: scipy.signal for filtering. Step 3: Load Protein Database. Description: Read a FASTA file to generate a dictionary of protein sequences. Input Required: Protein database file (uniprot_human.fasta). Expected Output: Python dictionary {ProteinID: amino_acid_sequence}. Tools Used: pyteomics.fasta for FASTA file parsing. Step 4: Enzymatic Digestion to Generate Theoretical Peptides. Description: Simulate trypsin digestion to generate theoretical peptides from protein sequences. Input Required: Protein sequence dictionary from Step 3. Expected Output: List of theoretical peptides. Tools Used: pyteomics.parser.cleave for enzymatic digestion. Step 5: Generate Theoretical Spectra. Description: Calculate fragment ion m/z values (b and y ions) for each theoretical peptide. Input Required: List of theoretical peptides. Expected Output: Dictionary mapping peptides to their fragment ion m/z values. Tools Used: pyteomics.mass.calculate_masses for mass spectrometry calculations. Step 6: Experimental Spectrum Matching. Description: Match experimental spectra to theoretical spectra to identify peptide sequences. Input Required: Preprocessed spectra from Step 2 and theoretical spectra from Step 5. Expected Output: CSV file (peptide_matches.csv) with spectrum IDs, peptides, and matching scores. Tools Used: numpy for array calculations. Step 7: Peptide-Protein Mapping. Description: Associate identified peptides with proteins and filter low-confidence proteins. Input Required: peptide_matches.csv and protein sequence dictionary from Step 3. Expected Output: CSV file (protein_list.csv) listing proteins and their supporting peptide counts. Tools Used: pandas for data aggregation. Step 8: Protein Quantification. Description: Calculate protein abundance using the top three highest-intensity peptides. Input Required: peptide_matches.csv and preprocessed spectra from Step 2. Expected Output: CSV file (protein_quant.csv) with protein abundances. Tools Used: pandas for data aggregation.",
        "metadata": {
            "source": "workflow",
            "page": 45
        }
    },
    {
        "content": "Protein Expression Quantitative Analysis Workflow must include: Step 1: Data Preparation and Loading. Description: Read an Excel file containing protein quantification data and conduct an initial check on its structure, including column names, distribution of missing values, and content of the initial rows. Input Required: An Excel file (e.g., ms220042-dia-20220111.xls) containing columns such as PG.ProteinGroups and PG.Quantity. Expected Output: A preliminarily loaded DataFrame with columns for raw protein IDs, quantitative values, etc., and a console output preview of the data structure (df.head()). Tools Used: Pandas for data reading (pd.read_excel) and preliminary inspection. Step 2: Data Cleaning. Description: Rename complex column names to more readable ones, handle missing values by deleting rows with missing protein IDs, and split rows containing multiple protein IDs into separate rows. Input Required: The preliminarily loaded DataFrame that may contain complex column names or missing values. Expected Output: A cleaned DataFrame with standardized column names and complete data without missing IDs. Tools Used: Pandas for column renaming (rename), deleting rows with missing values (dropna), and splitting multiple IDs (str.split + explode). Step 3: Data Preprocessing. Description: Perform log2(x + 1) transformation on quantitative values to address the skewed distribution of data and use quantile normalization to eliminate technical biases and make data distributions consistent across different samples. Input Required: The cleaned DataFrame with original quantitative values. Expected Output: A preprocessed DataFrame containing a quantitative matrix after log2 transformation and normalization. Tools Used: NumPy for numerical calculation (np.log2) and scikit-learn for quantile normalization (QuantileTransformer). Step 4: Differential Expression Analysis. Description: Conduct a t - test to calculate the significance difference (p - value) between the experimental group (Group2) and the control group (Group1) for each protein, use the Benjamini - Hochberg method for multiple test correction to generate adjusted p - values (adj_p_value), and calculate the log2(mean of experimental group / mean of control group) to obtain the fold change. Input Required: The preprocessed quantitative matrix with grouped data. Expected Output: A differential analysis results table with columns for log2FC, p_value, and adj_p_value, and a list of significantly differentially expressed proteins (filtered by adj_p_value < 0.05 and |log2FC| > 1). Tools Used: SciPy for the t - test (ttest_ind), statsmodels for multiple test correction (multipletests), and NumPy/Pandas for fold change calculation (mean + log2).",
        "metadata": {
            "source": "workflow",
            "page": 46
        }
    },
    {
        "content": "The single-cell transcriptomics differential expression gene analysis workflow includes the following steps: Step 1: Data Loading and Initialization, reading the raw single-cell data file (kang.h5ad) using omicverse.read to generate an AnnData object containing the raw gene expression matrix and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using omicverse.pp.qc, outputting filtered high-quality data; Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA (50 principal components) with scanpy.tl.pca and nonlinear MDE reduction using ov.utils.mde, yielding an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, outputting cell cluster labels and visualization plots; Step 6: Whole-Cell Differential Expression Analysis, applying tools like Seurat.FindMarkers, DESeq2, MAST, or edgeR to compare group differences at the whole-cell level, inputting normalized expression matrices and grouping information, outputting a list of differentially expressed genes (DEGs) with log2FC, p-values, and adjusted p-values.", 
               "metadata": {
            "source": "workflow",
            "page": 47
        }
    },
    {
        "content": "The single-cell data marker gene identification workflow includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using omicverse.pp.qc, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbor, yielding an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, outputting cell cluster labels and visualization plots; Step 6: Whole-Cell Differential Expression Analysis, applying tools to compare group differences at the whole-cell level, inputting normalized expression matrices and grouping information, outputting a table of differentially expressed genes (DEGs) with with colums of log2FC, p-values, and adjusted p-values, and respectively rename the columns to logFC, pvals, and adj_pvals; Step 7: Result Filtering, filtering marker genes based on significance thresholds adj_pvals, logFC, output all high-confidence marker gene lists with adj_pvals < 0.05 and logFC > 1", 
               "metadata": {
            "source": "workflow",
            "page": 48
        }
    },
    {
        "content": "Single-cell full-process analysis includes the following steps:Single cell full-process analysis includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using omicverse.pp.qc, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbor, yielding an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, outputting cell cluster labels and visualization plots; Step 6: Whole-Cell Differential Expression Analysis, applying tools to compare group differences at the whole-cell level, inputting normalized expression matrices and grouping information, outputting a table of differentially expressed genes (DEGs) with with colums of log2FC, p-values, and adjusted p-values, and respectively rename the columns to logFC, pvals, and adj_pvals; Step 7: Result Filtering, filtering marker genes based on significance thresholds adj_pvals, logFC, output all high-confidence marker gene lists with adj_pvals < 0.05 and logFC > 1, or output top n marker genes; Step 8 Cell Type Annotation Analysis:Inputs include a high-confidence marker gene list (.csv with logFC and adj_pval fields) and a raw AnnData object (containing a normalized expression matrix and cluster labels). using python tools like scanoy, celltypistand and pandas to do further annotation this step outputs biologically annotated cell subpopulation labels (e.g., adata.obs['cell_type']) and output verification umap plots using plot(.png files);  Step 9 Pseudotime trajectory analysis, input the Annotated AnnData (with cell_type column) and subpopulation labels (e.g., leiden_clusters), then use scanpy tool to deal with it. sc.tl.diffmap(adata), sc.pl.diffmap(adata, color='leiden_clusters'), Output: UMAP plot overlaid with Diffusion Map trajectory(.png); Step 10 Purity Analysis (Subpopulation Homogeneity Evaluation), This module evaluates subpopulation homogeneity using an annotated AnnData object (containing cell types and Leiden cluster labels). Inputs include the preprocessed expression matrix and cluster annotations. Tools involve the Gini coefficient to measure gene expression dispersion (genes with Gini >0.7 indicate high heterogeneity) and Wilcoxon tests to quantify high-dispersion gene proportions per cluster. Outputs are a cluster purity table (e.g., cluster_purity.csv); Step 11 Functional Heterogeneity Analysis, Inputs include the filtered AnnData object and predefined gene sets . Tools include Wilcoxon tests to select cluster-specific markers and diffusion pseudotime analysis to infer differentiation trajectories. Outputs are a marker gene list (cluster_markers.csv); Step 12 Regulatory Network Analysis, Inputs include the annotated AnnData object and TF-target databases. Tools use Decoupler for TF activity scoring and correlation analysis (|r|>0.3) to build networks. Outputs are a TF activity matrix (tf_activity.csv)", 
               "metadata": {
            "source": "workflow",
            "page": 49
        }
    },
    {
        "content": "Single-cell perform clustering analysis includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using omicverse.pp.qc, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbor, yielding an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, sc.tl.leiden(adata, resolution=), sc.pl.umap(adata, color='leiden', show=False, save='cluster_plots.png'), adata.write('./output/019/adata_clustered.h5ad'), outputting 2 files, one is the result with cell cluster labels(.h5ad) and one is visualization plot(.png); ", 
               "metadata": {
            "source": "workflow",
            "page": 50
        }
    },
    {
        "content": "IF you: Please use this data to extract signal (or do eventalign, which is equivalent to the previous one) from nanopore files. Each step of the complete process has been organized into specific executable scripts(3 steps). The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order.\n\n1. map_fastq2_ref.sh:\nMap basecalling file to reference genome(fa file). Generates sam file.\nInput : a basecalling file in format fastq and a reference genome fa file\nOutput : a sam file \nUsage\nRun the following in the container.\n./scripts/map_fastq2_ref.sh <fq> <output_sam> <ref> <nThreads>\n# fq : basecalling fastq file.\n# output_sam : name of output sam file.\n# ref : reference genome fa file\n# nThreads : number of threads\n\n2. indexbam.sh:\nindex bam and generate statistics - coverage, error rate, number of reads.\nInput: a sam file\nOutput: a sorted bam file\nUsage\nRun the following in the container\n./scripts/indexbam.sh <nthread> <input_sam> <outputbam>\n# nthread : number of threads to use\n# input_sam : an input sam file\n# outputbam : name of output bam file\n\n3. eventalign.sh:\nindex fast5 to basecalling fastq with nanopolish index tool and then run nanopolish for basecalling fastq file to compute an improved consensus sequence from the draft genome assembly produced by minimap2, thus, get the signal.\nInput: a basecalling nanopore fastq file , a file folder containing nanopore files in fast5 format and a reference genome fa file\nOutput: a summary file in format sum and a output txt file\nUsage\nRun the following in the container\n./scripts/eventalign.sh <fq> <fast5dir> <sortedBam> <ref> <npsummary> <nThreads> <np_out>\n# fq: a basecalling fastq file.\n# fast5dir: Path of a file folder containing nanopore files in fast5 format.\n# sortedBam: a sorted bam file.\n# ref : reference genome fa file\n# nThreads : number of threads.\n# npsummary : name of merged summary file .\n# np_out : name of output txt file.",
            "metadata": {
            "source": "workflow",
            "page": 51
        }
    },
    {
        "content": "Before performing nanopore data process (except for modification identification, for example, identifying methylation) from nanopore files. If basecalling file (fastq/bam) is not supplied, firstly using dorado's basecaller command to do simplex basecalling to generate basecalling file before other steps.\nInput : a file folder containing nanopore files in fast5/pod5 format and a reference genome fa file\nOutput : a bam file.\nUsage\nRun the following in the container\n../dorado-0.9.1-linux-x64/bin/dorado basecaller hac <pod5dir> --reference <ref> > <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# ref : reference genome fa file\n# out : name of output bam file.",
            "metadata": {
            "source": "workflow",
            "page": 52
        }
    },
    {
        "content": "IF you: Please use this data to identify methylation from nanopore files. Each step of the process has been organized into specific executable scripts(2 steps). The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order. \n\n1. methy_iden.sh:\ndo modified basecalling from nanopore pod5 files. \nInput : a file folder containing nanopore files in fast5/pod5 format and a reference genome fa file\nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/methy_iden.sh <pod5dir> <ref> <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# ref : reference genome fa file\n# out : name of output bam file.\n 2. basecall_sum.sh:\n output a tab-separated file with read level sequencing information from the bam file generated during basecalling. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n./scripts/basecall_sum.sh <bam> <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.",
            "metadata": {
            "source": "workflow",
            "page": 53
        }
    },
    {
        "content": "After using dorado's basecaller command(simplex basecalling or modified basecalling), use dorado's summary command to output a tab-separated file with read level sequencing information from the bam file generated during basecalling. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n../dorado-0.9.1-linux-x64/bin/dorado summary <bam> > <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.",
            "metadata": {
            "source": "workflow",
            "page": 54
        }
    },
    {
        "content": "After using dorado's basecaller command(simplex basecalling or modified basecalling), use dorado's summary command to output a tab-separated file with read level sequencing information from the bam file generated during basecalling. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n../dorado-0.9.1-linux-x64/bin/dorado summary <bam> > <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.",
            "metadata": {
            "source": "workflow",
            "page": 55
        }
    },
    {
        "content": "IF you: Please use this data to do complete quantitative  analysis for nanopore files. Each step of the process has been organized into specific executable scripts(13 steps). The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order. \n1. basecaller.sh:\ndo simplex basecalling. \nInput : a file folder containing nanopore files in fast5/pod5 format\nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/basecaller.sh <pod5dir> <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# out : name of output bam file.\n 2. basecall_sum.sh:\n output a tab-separated file with read level sequencing information from the bam file generated during basecalling. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n./scripts/basecall_sum.sh <bam> <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.\n3. bam2fq.sh:\n transfer bam file to fastq format.\nInput : basecaller’s output bam file \nOutput : a fastq file.\nUsage\nRun the following in the container\n./scripts/bam2fq.sh <numcore> <bam> <out>  \n# numcore : number of threads\n# bam : basecaller’s output bam file.\n# out : name of output fastq file. \n4. raw_plot.sh:\n plot QC imformation from the basecall fastq file(raw_plot).\nInput : basecall fastq file \nOutput : sevaral png file and a report html file containing QC imformation.\nUsage\nRun the following in the container\n./scripts/raw_plot.sh <fastq> <outdir>  \n# fastq : basecall fastq file.\n# outdir : directory to store QC imformation files. \n5. filter.sh:\n filter low_quality reads in basecall fastq file.\nInput : basecall fastq file \nOutput : a filtered fastq file.\nUsage\nRun the following in the container\n./scripts/filter.sh <fastq> <out>  \n# fastq : basecall fastq file.\n# out : name of filtered fastq file. \n6. host_removal.sh:\n remove host gene in filtered fastq file.\nInput : filtered fastq file \nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/host_removal.sh <fa> <fastq> <out>  # fa:  host gene fasta file.\n# fastq : filtered fastq file.\n# out : name of output bam file. \n7. bam2fq.sh:\n transfer bam file to fastq format.\nInput : host-gene-removal bam file \nOutput : a fastq file.\nUsage\nRun the following in the container\n./scripts/bam2fq.sh <numcore> <bam> <out>  \n# numcore : number of threads\n# bam : host-gene-removal bam file.\n# out : name of output fastq file.  \n8. filtered_plot.sh:\n plot QC imformation from the host-gene-removal fastq file(filtered_plot).\nInput : host-gene-removal fastq file \nOutput : sevaral png file and a report html file containing QC imformation.\nUsage\nRun the following in the container\n./scripts/filtered_plot.sh <fastq> <outdir>  \n# fastq : host-gene-removal fastq file.\n# outdir : directory to store QC imformation files.  \n9. map_fastq2_ref.sh:\nMap host-gene-removal fastq file to reference genome(fa file). Generates sam file.\nInput : a  host-gene-removal fastq file and a reference genome fa file\nOutput : a sam file \nUsage\nRun the following in the container.\n./scripts/map_fastq2_ref.sh <fq> <output_sam> <ref> <nThreads>\n# fq :  host-gene-removal fastq file.\n# output_sam : name of output sam file.\n# ref : reference genome fa file\n# nThreads : number of threads\n 10. indexbam.sh:\nindex bam and generate statistics - coverage, error rate, number of reads.\nInput: a sam file\nOutput: a sorted bam file\nUsage\nRun the following in the container\n./scripts/indexbam.sh <nthread> <input_sam> <outputbam>\n# nthread : number of threads to use\n# input_sam : an input sam file\n# outputbam : name of output bam file\n \n11. alignment_stats.sh:\nGet the alignment_stats report.\nInput : the sorted bam file\nOutput : a alignment_stats txt file \nUsage\nRun the following in the container.\n./scripts/alignment_stats.sh <bam> <out> \n# bam :  the sorted bam file.\n# out : name of alignment_stats report txt file.\n# ref : reference genome fa file\n# nThreads : number of threads  \n12. quantififation.sh:\nGenerate transcripts and  gene_abundance tsv file from the sorted bam file.\nInput : the sorted bam file\nOutput : a transcripts gtf file and a gene_abundance tsv file. \nUsage\nRun the following in the container.\n./scripts/quantififation.sh <bam> <transcripts_gtf> <out> \n# bam :  the sorted bam file. \n# transcripts_gtf :  name of output transcripts gtf file.\n# out : name of output gene_abundance tsv file.\n13. result_sum.sh:\n make a new diroctory to store all the readable result. \nInput : path to raw_plot dir, filterd_plot dir, alignment_stats report txt file, gene_abundance tsv file and basecall summary tsv file, a new diroctory path. \nUsage\nRun the following in the container.\n./scripts/result_sum.sh <dir1> <dir2> <alignment_stats> <gene_abundance> <basecall> <report_dir>\n# dir1 : path to raw_plot QC imformation dir \n# dir2 : path to filtered_plot QC imformation dir\n# alignment_stats : path to alignment_stats report txt file.\n# gene_abundance : path to gene_abundance tsv file.\n# basecall : path to  basecall summary tsv file. .\n# report_dir : path of new diroctory to store the results.",
            "metadata": {
            "source": "workflow",
            "page": 56
        }
    }
      
                    
]