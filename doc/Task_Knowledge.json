[
    {
        "content": "EIGENSOFT 5.0 changes include:\n- New option lsqproject for PCA projection with large amounts of missing data.\n- New options grmoutname and grmbinary to output genetic relationship matrix, compatible with GCTA software (v1.13).\n- Expanded options for LD regression in computing genetic relationship matrix.\n\nSmartPCA description:\nSmartPCA runs Principal Components Analysis on input genotype data and outputs principal components (eigenvectors) and eigenvalues. The principal components are useful for a variety of tasks, including identifying population structure and correcting for population stratification in genetic association studies.\n\nProgram-specific details and input formats:\nSmartPCA supports five different input formats. See ../CONVERTF/README for documentation on using the convertf program to convert between formats.\n\nSyntax for SmartPCA:\n../bin/smartpca -p parfile\n\nInput format details:\n- genotypename: Input genotype file (in any format: see ../CONVERTF/README).\n- snpname: Input SNP file (in any format: see ../CONVERTF/README).\n- indivname: Input individual file (in any format: see ../CONVERTF/README).\n- evecoutname: Output file of eigenvectors (see numoutevec parameter below).\n- evaloutname: Output file of all eigenvalues.\n\nOptional parameters:\n- numoutevec: Number of eigenvectors to output. Default is 10.\n- numoutlieriter: Maximum number of outlier removal iterations. Default is 5. To turn off outlier removal, set this parameter to 0.\n- grmoutname: Output file prefix containing the lower-triangular of the genetic relatedness matrix, compatible with GCTA software.\n\nOther relevant programs include:\n- ploteig (for plotting the top two principal components).\n- twstats (for computing Tracy-Widom statistics to evaluate statistical significance of each principal component).A complete example of implementation includes:which python \n conda install -y eigensoft\necho 'genotypename: ./data/1000GP_pruned. bed' > ./output/098/smartpca.par\necho 'snpname: ./data/1000GP_pruned. bim' >> ./output/098/smartpca.par\necho 'indivname: ./data/1000GP_pruned. fam' >> ./output/098/smartpca.par\necho 'evecoutname: ./output/098/smartpca. evec' >> ./output/098/smartpca.par\necho 'evaloutname: ./output/098/smartpca. eval' >> ./output/098/smartpca.par\necho 'numoutevec: 10' >> ./output/098/smartpca.par\necho 'numoutlieriter: 5' >> ./output/098/smartpca.par\nsmartpca -p ./output/098/smartpca.par",
        "metadata": {
            "source": "smartPCA",
            "page": 1
        }
    },
    {
        "content": "Detailed Metagenomic Analysis Workflow must include:Step1: load main function: source('00.func_v2.R'). Description: The document '00.func_v2.R' contains necessary R packages along with their installation commands, and a pre-written main function. Subsequent use only requires sourcing the file with `source(\"00.func_v2.R\")`.Step2: create_phyloseq function: Description: create phyloseq-class object; specify group (g1, g2) variables and subset specified levels (g1.level, g2.level); filter taxonomy by detection and prevalence cutoff. Parameters: otu_table (OTU table), tax_table (taxonomy table), sam_data (samples metadata), tax_level (select specified taxonomy level), g1 (group1 in sample metadata), g1.level (subgroups level included in group1), g2 (group2 in sample metadata), g2.level (subgroups level included in group2), detection (detection cutoff), prevalence (prevalence cutoff), rel (Transformation to apply. The options include: 'compositional' (ie relative abundance), 'Z', 'log10', 'log10p', 'hellinger', 'identity', 'clr', 'alr'), tidy_taxonomy (Clean up the taxonomic table to make taxonomic assignments consistent). Expected Output: phyloseq-class object. Tools Used: create_phyloseq function.Step3: plot_detection_prevalence function: Description: Calculates the community core microbita and visualization. Determine members of the core microbiota with given abundance and prevalences. Parameters:   ps (phyloseq object), min_prevalence (minimum prevalence cutoff), step_detection (Set the number of steps for detection). Expected Output: filtered phyloseq object and core microbita figure plot. Tools Used: plot_detection_prevalence function.Step4: plot_comp function: Description: Plot taxon abundance for microbiome composition. Parameters: ps (phyloseq object), tax_level (select specified taxonomy level), strata (Specifies a faceted variable), groupmean (calculate mean abundance for each group), ntaxa (how many taxa are selected to show), sort_bacteria (Select the number of bacteria to arrange the sample), clustering (whether order samples by the clustering), clustering_plot (whether add clustering plot), use_alluvium (whether add alluvium plot). Expected Output: Composition figure plot. Tools Used: plot_comp function.Step5: cal_alpha function: Description: analyzing alpha diversity in ecosystems, including the calculation of various richness, evenness, diversity, dominance, and rarity indices, along with methods for visualizing these measures and testing differences in alpha diversity between groups. Parameters: ps (phyloseq object), group (A string indicating the variable for group identifiers), strata (A string indicating the variable for strata identifiers), method_test (the name of the statistical test (e.g. t.test, wilcox.test etc.)), signif_label (whether show the p-value as labels: c('***'=0.001, '**'=0.01, '*'=0.05)), adj.vars (Specify adjust variables of sample metadata). Expected Output: Alpha diversity index table (res_alpha$table) and figure plots (res_alpha$observed, res_alpha$chao1, res_alpha$diversity_shannon, res_alpha$diversity_gini_simpson, etc.). Tools Used: cal_alpha function.Step6: cal_pca function: Description: Performs a principal components analysis (PCA). Parameters: ps (phyloseq object), group (A string indicating the variable for group identifiers), center (whether the variables should be shifted to be zero centered), scale (whether the variables should be scaled to have unit variance). Expected Output: PCA  components table (res_pca$table) and figure plots (res_pca$pca_biplot, res_pca$pca_ind, res_pca$pca_contrib1, res_pca$pca_contrib2). Tools Used: cal_pca function.Step7: cal_dist function: Description: Calculate dissimilarity matrix, anosim and PERMANOVA analysis. Parameters: ps (phyloseq object), method (Dissimilarity index, one of c('manhattan', 'euclidean', 'canberra', 'clark', 'bray', 'kulczynski', 'jaccard', 'gower', 'altGower', 'morisita', 'horn', 'mountford', 'raup', 'binomial', 'chao', 'cao', 'mahalanobis', 'chisq', 'chord', 'hellinger', 'aitchison','robust.aitchison')), group (A string indicating the variable for group identifiers for anosim and PERMANOVA analysis), adj.vars (adjust variables of sample metadata. Note: This feature is yet to be verified). Expected Output: A list contains Dissimilarity matrix (res_dist$dist), anosim result (res_dist$anosim) and PERMANOVA RESULT (res_dist$PERMANOVA). Tools Used: cal_dist function.Step8: cal_ordination function: Description: Perform an ordination on phyloseq data. Parameters: ps (phyloseq object), dist (a pre-computed dist-class object), method (several commonly-used ordination methods. Currently supported method options are: c('DCA', 'CCA', 'RDA', 'NMDS', 'MDS', 'PCoA')), type (The plot type. Default is 'samples'. The currently supported options are c('samples', 'taxa', 'biplot', 'split', 'scree')), group (The name of the variable to map to colors in the plot), shape (The name of the variable to map to different shapes on the plot), label (The name of the variable to map to text labels on the plot). Expected Output: An ordination object (res_ord$ordination) and figure plot (res_ord$fig_ord). Tools Used: cal_ordination function.Step9: cal_marker function: Description: This function is only a wrapper of all differential analysis functions, We recommend to use the corresponding function, since it has a better default arguments setting. Parameters: ps (phyloseq object), group (A string indicating the variable for group identifiers), da_method (character to specify the differential analysis method. The options include:c('lefse','simple_t','simple_welch','simple_white','simple_kruskal','simple_anova','edger','deseq2','metagenomeseq','ancom','ancombc','aldex','limma_voom','sl_lr','sl_rf','sl_svm')), tax_rank (character to specify taxonomic rank to perform differential analysis on), transform (the methods used to transform the microbial abundance, default is identity), norm (the methods used to normalize the microbial abundance data), norm_para (arguments passed to specific normalization methods), p_adjust (method for multiple test correction, default none), pvalue_cutoff (p value cutoff, default 0.05). Expected Output: a microbiomeMarker table (mm@marker_table), in which the slot of marker_table contains four variables (feature, enrich_group, lda, pvalue), `microbiomeMarker::plot_ef_bar(mm)` plot bar of markers, `microbiomeMarker::plot_abundance(mm,group = iGroup)` plot abundance of markers. Tools Used: R packages microbiomeMarker, run_lefse function.",
        "metadata": {
            "source": "R",
            "page": 2
        }
    },
    {
        "content": "admixtools is an R package, and once installed, you can begin using it by entering the R environment and running the relevant commands. When using it, create another /admixtools/ folder under the target output path and put all the output into it. You can enter the R environment by entering an R as a line in the command line also can use Rscript. not install any package. If you don't have the admixtools package, check case and try library(admixtools) ,library(tidyverse) is imported as follows.f2 in ADMIXTOOLS 2\nIn ADMIXTOOLS 2, f2-statistics are the foundation for all further analyses. They can be computed from genotype data and saved to disk with this command:\n\nprefix = '/path/to/geno'\nmy_f2_dir = '/store/f2data/here/'\n\nextract_f2(prefix, my_f2_dir)\nThis will look for genotype files in packedancestrymap or PLINK format, compute allele frequencies and blocked f4-statistics for all pairs of populations defined in the .ind or .fam file, and write them to my_f2_dir. It is also possible to extract only a subset of the samples or populations by passing IDs to the inds and pops arguments in extract_f2(). To get a description of the arguments and to see examples of how to use it, type\n\n?extract_f2\nBy default, extract_f2() will be very cautious and exclude all SNPs which are missing in any population (maxmiss = 0). If you lose too many SNPs this way, you can either\n\nlimit the number of populations for which to extract f2-statistics,\ncompute f3- and f4-statistics directly from genotype files, or\nincrease the maxmiss parameter (maxmiss = 1 means no SNPs will be excluded).\nThe advantages and disadvantages of the different approaches are described here. Briefly, when running qpadm() and qpdstat() it can be better to choose the safer but slower options 1 and 2, while for qpgraph(), which is not centered around hypothesis testing, it is usually fine choose option 3. Since the absolute difference in f-statistics between these approaches is usually small, it can also make sense to use option 3 for exploratory analyses, and confirm key results using options 1 or 2.\n\nOnce extract_f2() has finished, f2-statistics for the populations of interest can be loaded using f2_from_precomp():\n\nf2_blocks = f2_from_precomp(my_f2_dir)\nOr you can load only a subset of the populations:\n\nmypops = c('Denisova.DG', 'Altai_Neanderthal.DG', 'Vindija.DG')\nf2_blocks = f2_from_precomp(my_f2_dir, pops = mypops)\nIf your data is so small that computing f2-statistics doesn’t take very long, you can skip writing the data to disk with extract_f2() and do everything in one step using f2_from_geno():\n\nf2_blocks = f2_from_geno(my_f2_dir, pops = mypops)\nf2_blocks is now a 3d-array with f2-statistics for each population pair along dimensions 1 and 2, and each SNP block along the 3rd dimension.\n\ndim(f2_blocks)\n## [1]   7   7 708\nThe purpose of having separate estimates for each SNP block is to compute jackknife or bootstrap standard errors for f-statistics, and for any statistics derived from them.\n\nf2_blocks can be used like this:\n\nf2_blocks[,,1]                # f2-statistics of the 1st SNP block\napply(f2_blocks, 1:2, mean)   # average across all blocks\nf2_blocks[pop1, pop2, ]       # f2(pop1, pop2) for all blocks\nThe names along the 3rd dimension contain the SNP block lengths:\n\nblock_lengths = parse_number(dimnames(f2_blocks)[[3]])\nhead(block_lengths)\n## [1] 424 772 795 835 574 842\nTo see the total number of SNPs across all blocks, you can use count_snps()\n\ncount_snps(f2_blocks)\n## [1] 780009\nIf you want to try any of this without extracting and loading your own f2-statistics, you can instead use example_f2_blocks which becomes available after running library(admixtools).\n\n\nMore information on f-statistics in ADMIXTOOLS 2\n\n\n\nf3 and qp3Pop\nThere are three main uses of f3-statistics:\n\nTesting whether a population is admixed: If f3(A;B,C) is negative, this suggests that A is admixed between a population related to B and one related to C.\nEstimating the relative divergence time for pairs of populations (outgroup f3-statistics): Pairwise FST and f2 are simpler estimates of genetic distance or divergence time, but they are affected by differences in population size. If O is an outgroup relative to all populations i and j, then f3(O;i,j) will estimate the genetic distance between O and the points of separation between i and j without being affected by drift that is specific to any population i or j.\nFitting admixture graphs: f3-statistics of the form f3(O;i,j) for an arbitrary population O, and all pairs of i and j are used in qpGraph, which is described below.\nThe original ADMIXTOOLS program for computing f3-statistics is called qp3Pop. In ADMIXTOOLS 2, you can compute f3-statistics like this:\n\npop1 = 'Denisova.DG'\npop2 = c('Altai_Neanderthal.DG', 'Vindija.DG')\npop3 = c('Chimp.REF', 'Mbuti.DG', 'Russia_Ust_Ishim.DG')\nqp3pop(f2_blocks, pop1, pop2, pop3)\nOr, equivalently\n\nf3(f2_blocks, pop1, pop2, pop3)\n## # A tibble: 6 × 7\n##   pop1        pop2                 pop3                  est      se     z     p\n##   <chr>       <chr>                <chr>               <dbl>   <dbl> <dbl> <dbl>\n## 1 Denisova.DG Altai_Neanderthal.DG Chimp.REF          0.0591 5.98e-4  98.7     0\n## 2 Denisova.DG Altai_Neanderthal.DG Mbuti.DG           0.0720 6.47e-4 111.      0\n## 3 Denisova.DG Altai_Neanderthal.DG Russia_Ust_Ishim.… 0.0742 7.02e-4 106.      0\n## 4 Denisova.DG Vindija.DG           Chimp.REF          0.0594 5.92e-4 100.      0\n## 5 Denisova.DG Vindija.DG           Mbuti.DG           0.0724 6.34e-4 114.      0\n## 6 Denisova.DG Vindija.DG           Russia_Ust_Ishim.… 0.0750 6.96e-4 108.      0\nThis will compute f3-statistics for all combinations of pop1, pop2, and pop3. f3(f2_blocks) will compute all possible combinations (which can be a large number). If only pop1 is supplied, all combinations of populations in pop1 will be computed.\n\n\nf4 and qpDstat\nThe original ADMIXTOOLS program for computing f4-statistics is called qpDstat. As the name suggests, it computes D-statistics by default. To get f4-statistics instead, the f4mode argument needs to set to YES. In ADMIXTOOLS 2, almost everything starts with f2-statistics, so the qpdstat/f4 function computes f4-statistics by default.\n\npop4 = 'Switzerland_Bichon.SG'\nf4(f2_blocks, pop1, pop2, pop3, pop4)\nqpdstat(f2_blocks, pop1, pop2, pop3, pop4)\n# two names for the same function\n## # A tibble: 6 × 8\n##   pop1        pop2                 pop3  pop4       est      se      z         p\n##   <chr>       <chr>                <chr> <chr>    <dbl>   <dbl>  <dbl>     <dbl>\n## 1 Denisova.DG Altai_Neanderthal.DG Chim… Swit…  1.50e-2 4.64e-4 32.3   6.06e-229\n## 2 Denisova.DG Altai_Neanderthal.DG Mbut… Swit…  2.03e-3 3.53e-4  5.75  8.85e-  9\n## 3 Denisova.DG Altai_Neanderthal.DG Russ… Swit… -2.17e-4 3.73e-4 -0.580 5.62e-  1\n## 4 Denisova.DG Vindija.DG           Chim… Swit…  1.54e-2 4.78e-4 32.2   5.81e-228\n## 5 Denisova.DG Vindija.DG           Mbut… Swit…  2.33e-3 3.63e-4  6.42  1.40e- 10\n## 6 Denisova.DG Vindija.DG           Russ… Swit… -2.40e-4 3.87e-4 -0.620 5.35e-  1\nThe differences between f4-statistics and D-statistics are usually negligible. However, it is still possible to compute D-statistics in ADMIXTOOLS 2, by providing genotype data as the first argument, and setting f4mode = FALSE:\n\nprefix = '/path/to/geno'\nf4(prefix, pop1, pop2, pop3, pop4, f4mode = FALSE)\nComputing f4- or D-statistics from genotype data directly is slower, but it has the advantage that it avoids any problems that may arise from large amounts of missing data. More on this here.\n\n\n\nFST\nFST is closely related to f2, but unlike f2, it doesn’t function as a building block for other tools in ADMIXTOOLS 2. However, it is the most widely used metric to estimate the genetic distance between populations. Running extract_f2() will create files which don’t only contain f2 estimates for each population pair, but also separate FST estimates. The function fst() can either read these pre-computed estimates, or compute them directly from genotype files:\n\nfst(my_f2_dir)\nfst(prefix, pop1 = \"Altai_Neanderthal.DG\", pop2 = c(\"Denisova.DG\", \"Vindija.DG\"))\nTo estimate FST without bias, we need at least two independent observations in each population. With pseudohaploid data, we only get one independent observation per sample, and so for populations consisting of only one pseudohaploid sample, FST cannot be estimated without bias. If we want to ignore that bias and get estimates anyway, we can pretend the pseudohaploid samples are actually diploid using the option adjust_pseudohaploid = FALSE.\n\nfst(prefix, pop1 = \"Altai_Neanderthal.DG\", pop2 = c(\"Denisova.DG\", \"Vindija.DG\"),\n    adjust_pseudohaploid = FALSE)\n\n\nqpWave and qpAdm\nqpWave and qpAdm are two programs with different goals - qpWave is used for estimating the number of admixture events, and qpAdm is used for estimating admixture weights - but they perform almost the same computations. The key difference is that qpWave compares two sets of populations (left and right), while qpAdm tests how a single target population (which can be one of the left populations) relates to left and right. In ADMIXTOOLS 2, both qpadm() and qpwave() require at least three arguments:\n\nf2-statistics\nA set of left populations\nA set of right populations\nqpadm() additionally requires a target population as the 4th argument, which will be modeled as a mixture of left populations.\n\nleft = c('Altai_Neanderthal.DG', 'Vindija.DG')\nright = c('Chimp.REF', 'Mbuti.DG', 'Russia_Ust_Ishim.DG', 'Switzerland_Bichon.SG')\ntarget = 'Denisova.DG'\npops = c(left, right, target)\nBoth functions will return f4-statistics, and a data frame that shows how well the f4-matrix can be approximated by lower rank matrices. The last line tests for rank 0, which is equivalent to testing whether the left populations form a clade with respect to the right populations.\n\nresults = qpwave(f2_blocks, left, right)\nresults$f4\n## # A tibble: 3 × 8\n##   pop1                 pop2       pop3      pop4       est      se     z       p\n##   <chr>                <chr>      <chr>     <chr>    <dbl>   <dbl> <dbl>   <dbl>\n## 1 Altai_Neanderthal.DG Vindija.DG Chimp.REF Mbuti… 1.24e-4 1.35e-4 0.920 0.358  \n## 2 Altai_Neanderthal.DG Vindija.DG Chimp.REF Russi… 4.45e-4 1.64e-4 2.72  0.00653\n## 3 Altai_Neanderthal.DG Vindija.DG Chimp.REF Switz… 4.22e-4 1.72e-4 2.45  0.0144\nresults$rankdrop\n## # A tibble: 1 × 7\n##   f4rank   dof chisq       p dofdiff chisqdiff p_nested\n##    <int> <int> <dbl>   <dbl>   <int>     <dbl>    <dbl>\n## 1      0     3  11.9 0.00768      NA        NA       NA\nqpadm() will also compute admixture weights and nested models:\n\nweights: These are the admixture weights, or estimates of the relative contributions of the left population to the target population.\npopdrop: popdrop shows the fits of all models generated by dropping a specific subset of left populations, and will only be returned if a target population is specified.\nf4: The estimated f4-statistics will now also include lines with fitted f4-statistics, where the target population is in the first column, and a weighted sum of the left populations, fit, in the second column.\nresults = qpadm(f2_blocks, left, right, target)\nresults$weights\n## # A tibble: 2 × 5\n##   target      left                 weight    se     z\n##   <chr>       <chr>                 <dbl> <dbl> <dbl>\n## 1 Denisova.DG Altai_Neanderthal.DG   49.6  23.3  2.13\n## 2 Denisova.DG Vindija.DG            -48.6  23.3 -2.08\nresults$popdrop\n## # A tibble: 3 × 13\n##   pat      wt   dof    chisq      p f4rank Altai_Neanderthal.DG Vindija.DG\n##   <chr> <dbl> <dbl>    <dbl>  <dbl>  <dbl>                <dbl>      <dbl>\n## 1 00        0     2     7.15 0.0280      1                 49.6      -48.6\n## 2 01        1     3 11412.   0           0                  1         NA  \n## 3 10        1     3 11449.   0           0                 NA          1  \n## # ℹ 5 more variables: feasible <lgl>, best <lgl>, dofdiff <dbl>,\n## #   chisqdiff <dbl>, p_nested <dbl>\n\n\nRunning many models\nThere are several functions that can be used to run many qpWave or qpAdm models at the same time.\n\nPairwise cladality tests\nqpwave_pairs() forms all pairs of left populations and tests whether they form a clade with respect to the right populations.\n\nqpwave_pairs(f2_blocks, left = c(target, left), right = right)\n## # A tibble: 6 × 4\n##   pop1                 pop2                  chisq       p\n##   <chr>                <chr>                 <dbl>   <dbl>\n## 1 Altai_Neanderthal.DG Denisova.DG          1507.  0      \n## 2 Altai_Neanderthal.DG Vindija.DG             11.9 0.00768\n## 3 Denisova.DG          Altai_Neanderthal.DG 1507.  0      \n## 4 Denisova.DG          Vindija.DG           1510.  0      \n## 5 Vindija.DG           Altai_Neanderthal.DG   11.9 0.00768\n## 6 Vindija.DG           Denisova.DG          1510.  0\nRotating outgroups\nqpadm_rotate() tests many qpadm() models at a time. For each model, the leftright populations will be split into two groups: The first group will be the left populations passed to qpadm(), while the second group will be added to rightfix and become the set of right populations. By default, this function will only compute p-values but not weights for each model (which makes it faster). If you want the full output for each model, set full_results = TRUE.\n\nqpadm_rotate(f2_blocks, leftright = pops[3:7], target = pops[1], rightfix = pops[1:2])\n## ℹ Evaluating 25 models...\n## \n## # A tibble: 25 × 7\n##    left      right     f4rank   dof   chisq        p feasible\n##    <list>    <list>     <dbl> <dbl>   <dbl>    <dbl> <lgl>   \n##  1 <chr [1]> <chr [6]>      0     5 26194.  0        TRUE    \n##  2 <chr [1]> <chr [6]>      0     5 53965.  0        TRUE    \n##  3 <chr [1]> <chr [6]>      0     5 43564.  0        TRUE    \n##  4 <chr [1]> <chr [6]>      0     5 54909.  0        TRUE    \n##  5 <chr [1]> <chr [6]>      0     5 13167.  0        TRUE    \n##  6 <chr [2]> <chr [5]>      1     3 10602.  0        FALSE   \n##  7 <chr [2]> <chr [5]>      1     3 13506.  0        FALSE   \n##  8 <chr [2]> <chr [5]>      1     3 13447.  0        FALSE   \n##  9 <chr [2]> <chr [5]>      1     3    72.2 1.46e-15 FALSE   \n## 10 <chr [2]> <chr [5]>      1     3  4030.  0        FALSE   \n## # ℹ 15 more rows\nMany qpadm models\nSwapping some populations between the left and the right set is one common way to run multiple qpadm() models. There is also a more general function, in which you can specify any models that you want to run. This is faster than looping over several calls to the qpadm() function, in particular when reading data from a genotype matrix directly, because it re-uses f4-statistics.\n\nTo specify the qpadm() models you want to run, you need to make a data frame with columns left, right, and target, where each model is in a different row.\n\nmodels = tibble(\n           left = list(pops[1:2], pops[3]),\n           right = list(pops[4:6], pops[1:2]),\n           target = c(pops[7], pops[7]))\nresults = qpadm_multi('/my/geno/prefix', models)\n## ℹ Running models...\n## \nThe output is a list where each item is the result from one model. The following command would combine the weights for all models into a new data frame:\n\nresults %>% map('weights') %>% bind_rows(.id = 'model')\n## # A tibble: 3 × 6\n##   model target      left                 weight       se        z\n##   <chr> <chr>       <chr>                 <dbl>    <dbl>    <dbl>\n## 1 1     Denisova.DG Altai_Neanderthal.DG   7.92 2.44e+ 0  3.25e 0\n## 2 1     Denisova.DG Vindija.DG            -6.92 2.44e+ 0 -2.84e 0\n## 3 2     Denisova.DG Chimp.REF              1    1.78e-13  5.63e12\n\n\nqpGraph\nSingle f3-and f4-statistics can tell us how three or four populations are related to each other. qpGraph generalizes this concept to any number of populations. It takes estimated f3-statistics and the topology of an admixture graph, finds the edges weights that minimize the difference between fitted and estimated f3-statistics, and summarizes that difference in a likelihood score. A good model should fit all f3-statistics, and have a score close to zero.\n\nqpg_results = qpgraph(f2_blocks, example_graph)\nqpg_results$score\n## [1] 19219.98\nHere, example_graph is a specific graph included in this R package, but you can provide any other graph in one of three formats.\n\nAn igraph object.\nA two column matrix or data frame, where each row is an edge, the first column is the source of the edge, and the second column is the target. Additional columns labelled lower and upper can be used to constrain certain edges (NA = no constraint).\nThe location of a qpGraph graph file, which will be read and parsed.\nThe leaf nodes of this graph have to match the f2-statistic population labels, and the graph has to be a valid admixture graph: a directed acyclic graph where each node has no more than two parents. If nodes with more than two children are present (polytomies or multifurcations) they will be split in a random order, and the new drift edges will be constrained to 0.\n\n\n\nThe output of qpgraph() is a list with several items:\n\nedges: A data frame with estimated edge weights. This includes normal edges as well as admixture edges.\nscore: The likelihood score. Smaller scores indicate a better fit.\nf2: Estimated f2-statistics and standard errors for all population pairs.\nf3: Estimated and fitted f3-statistics for all population pairs with the outgroup. This includes residuals and z-scores.\nopt: A data frame with details from the weight optimization, with one row per set of starting values.\nppinv: The inverse of the f3-statistics covariance matrix.\nOptionally, fitted and estimated f4-statistics are returned as f4 and the worst residual z-score as worst_residual if return_fstats is set to TRUE. When f2_blocks_test is provided, an out-of-sample score is computed and returned as score_test.\n\nThe fitted graph can be plotted like this:\n\nplot_graph(qpg_results$edges)\n\n\nor as an interactive plot if you want to know the names of the inner nodes:\n\nplotly_graph(qpg_results$edges)\n",
        "metadata": {
            "source": "admixtools",
            "page": 3
        }
        
    },
    {
        "content": "For performing Treemix analysis with PLINK files (.bed, .bim, .fam), follow the steps below.All sample scripts such as plink2treemix.py and vcf2treemix.sh are in the current directory, i.e./plink2treemix.py and./vcf2treemix.sh. Simply modify the input and output file paths as necessary:\n\n1. Convert PLINK files to VCF format:\n   ```plink --bfile ./data/1000GP_pruned --recode vcf --out ./output/094/1000GP_pruned```\n   This command converts the PLINK files (`.bed`, `.bim`, `.fam`) into a VCF file (`1000GP_pruned.vcf`).\n\n2. Compress the VCF file using bgzip:\n   ```bgzip ./output/094/1000GP_pruned.vcf```\n   Compresses the VCF file to `.vcf.gz` format using `bgzip`.\n\n3. Remove variants with missing data using vcftools:\n   ```vcftools --gzvcf ./output/094/1000GP_pruned.vcf.gz --max-missing 1 --recode --stdout | gzip > ./output/094/1000GP_pruned_no_missing.vcf.gz```\n   Filters out variants with missing genotypes and generates a new compressed VCF file (`1000GP_pruned_no_missing.vcf.gz`).\n\n4. Perform LD pruning using PLINK:\n   ```plink --vcf ./output/094/1000GP_pruned_no_missing.vcf.gz --indep-pairwise 50 5 0.2 --out ./output/094/1000GP_pruned_no_missing_LDpruned```\n   Performs LD pruning to remove variants in high linkage disequilibrium based on the parameters (window size 50 SNPs, step size 5, r^2 threshold 0.2).\n\n5. Create a new VCF with LD-pruned SNPs:\n   ```plink --vcf ./output/094/1000GP_pruned_no_missing.vcf.gz --extract ./output/094/1000GP_pruned_no_missing_LDpruned.prune.in --recode vcf --out ./output/094/1000GP_pruned_LDpruned```\n   Generates a new VCF file (`1000GP_pruned_LDpruned.vcf`) containing only the pruned SNPs.\n\n6. Compress the LD-pruned VCF file using bgzip:\n   ```bgzip ./output/094/1000GP_pruned_LDpruned.vcf```\n   Compresses the LD-pruned VCF file to `.vcf.gz`.\n\n7. Generate a cluster file for Treemix:\n   ```bcftools query -l ./output/094/1000GP_pruned_LDpruned.vcf.gz | awk 'BEGIN{OFS=\"\\t\"} {split($1,pop,\"_\"); print $1, $1, pop[1]}' > ./output/094/1000GP_pruned_LDpruned.clust```\n   Creates a cluster file for Treemix, where the population is determined by the prefix of the sample ID.\n\n8. Run the `vcf2treemix.sh` script to prepare data for Treemix:\n   ```bash vcf2treemix.sh ./output/094/1000GP_pruned_LDpruned.vcf.gz  ./output/094/1000GP_pruned_LDpruned.clust```\n   Converts the VCF and cluster files into the format required for Treemix analysis.\n\n9. Run Treemix with different migration models:\n   ```\n   for i in {0..5}\n   do\n   treemix -i ./output/094/1000GP_pruned_LDpruned.treemix.frq.gz -m $i -o ./output/094/1000GP_no_missing_result -root GBR -bootstrap -k 500 -noss > treemix_${i}_log &\n   done\n   ```\n   Runs Treemix with migration models ranging from 0 to 5, and saves the results in the specified output directory. The loop also generates log files for each model (e.g., `treemix_0_log`, `treemix_1_log`, etc.).",
        "metadata": {
            "source": "Treemix",
            "page": 4
        }
        
    },
    {
        "content": "You can enter R in a single line or use Rscript to enter the R environment, admixtool and other packages have been installed, no additional installation is required. ### D Statistic (D-test) Explanation\n\n**D Statistic**, also known as the BABA-ABBA test, detects gene flow or admixture between populations by comparing allele patterns across four groups.\n\n#### Key Concepts\n- **PopA & PopB**: Main populations being compared.\n- **PopC**: Reference population (outgroup).\n- **PopD**: Test population for potential gene flow.\n\n#### BABA-ABBA Patterns\n- **BABA**: PopA has a specific allele; PopB does not.\n- **ABBA**: PopB has a specific allele; PopA does not.\n\nComparing these patterns reveals gene flow direction and strength.\n\n#### Analysis Steps\n1. **Data Preparation**: Use PLINK to preprocess genotype data.\n2. **Define Populations**: Select PopA, PopB, PopC, and PopD.\n3. **Calculate D Statistic**: Use the `f4` function from the `admixtools` package.\n4. **Interpret Results**:\n   - **D ≠ 0**: Indicates gene flow.\n   - **Positive/Negative**: Direction of gene flow.\n\n#### Code Overview\n```r\nlibrary(admixtools)\noutput_dir <- \"./output/090\"\nprefix <- \"./data/1000GP_pruned\"\n\nfam_data <- read.table(paste0(prefix, \".fam\"), header = FALSE)\ncolnames(fam_data) <- c(\"Population\", \"SampleID\", \"FatherID\", \"MotherID\", \"Sex\", \"Phenotype\")\ntable(fam_data$Population)\n\npopA <- 'CEU'\npopB <- 'GBR'\npopC <- 'YRI'\npopD <- 'ESN'\n\nD_results <- f4(prefix, popA, popB, popC, popD, f4mode = FALSE)\nwrite.table(D_results, file.path(output_dir, \"D_test_results.txt\"), sep = \"\\t\", row.names = FALSE, col.names = TRUE)\nprint(D_results)\n```\n\n- **Loading Package**: `admixtools` for admixture analysis.\n- **Setting Paths**: Define input and output directories.\n- **Data Handling**: Read and label population data.\n- **Define Populations**: Specify the four populations.\n- **Compute D Statistic**: Perform the BABA-ABBA test.\n- **Output Results**: Save and display the results.\n\n#### Result Interpretation\n- **D Value**: Significant deviation from zero indicates gene flow.\n- **Sign**: Positive or negative values show the direction of gene flow.\n\nThe D statistic helps understand historical gene flow and population relationships.",
        "metadata": {
            "source": "Admixtools(D)",
            "page": 5
        }
        
    },
    {
        "content": "2. run-sort-bam.sh:\nData-type-independent, generic bam sorting module\nInput : any unsorted bam file (.bam)\nOutput : a bam file sorted by coordinate (.sorted.bam) and its index (.sorted.bam.bai).\nUsage\nRun the following in the container.\nrun-sort-bam.sh <input_bam> <output_prefix>\n# input_bam : any bam file to be sorted\n# output_prefix : prefix of the output bam file.\n\nSet parameters according to the example: Suppose the input file is: ./output/GM12878_bwa_1.bam and ./output/GM12878_bwa_2.bam, the target is./output/GM12878_bwa_sorted.bam and ./output/GM12878_bwa_sorted, Generate the following sample script:\nbash ./scripts/run-sort-bam.sh ./output/GM12878_bwa_1.bam ./output/GM12878_bwa_sorted \n\nbash ./scripts/run-sort-bam.sh ./output/GM12878_bwa_2.bam ./output/GM12878_bwa_sorted\n\nYou can install the tool, but do not do any additional operations.You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-sort-bam.sh",
            "page": 6
        }
        
    },
    {
        "content": "3. run-bam2pairs.sh:\nBam to pairs conversion module for Hi-C data, based on samtools, bgzip and pairix.\nInput : any paired-end bam file\nOutput : a chromosome-block-sorted and bgzipped pairs pairs file that contains all the mapped read pairs in the bam file, along with its index (.bsorted.pairs.gz and .bsorted.pairs.gz.px2)\nUsage\nRun the following in the container.\nrun-bam2pairs.sh <input_bam> <output_prefix>\n# input_bam : input bam file.\n# output_prefix : prefix of the output pairs file.\nSet parameters according to the example:\nSuppose the input file is: ./output/GM12878_bwa_sorted_1.bam and ./output/GM12878_bwa_sorted_2.bam, the target is./output/GM12878_bwa_sorted_pairs  and./output/GM12878_bwa_sorted_pairs , Generate the following sample script:\nbash ./scripts/run-bam2pairs.sh ./output/GM12878_bwa_sorted_1.bam ./output/GM12878_bwa_sorted_pairs   \nbash ./scripts/run-bam2pairs.sh ./output/GM12878_bwa_sorted_2.bam ./output/GM12878_bwa_sorted_pairs \n\nYou can install the tool, but do not do any additional operations.You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-bam2pairs.sh",
            "page": 7
        }
        
    },
    {
        "content": "1. run-bwa-mem.sh:\nAlignment module for Hi-C data, based on bwa-mem.\nInput : a pair of Hi-C fastq files\nOutput : a bam file (Lossless, not sorted by coordinate)\nUsage\nRun the following in the container.\nrun-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <outdir> <output_prefix> <nThreads>\n# fastq1, fastq2 : input fastq files, either gzipped or not\n# bwaIndex : tarball for bwa index, .tgz.\n# outdir : output directory\n# output_prefix : prefix of the output bam file.\n# nThreads : number of threads\n\nThe script is invoked in the./script/ path as bash./script/run-bwa-mem.sh. Set parameters according to the example:I want to output under./output/003/.Reference file path for: ./data/hg38.bwaindex.tgz,no need to extract the reference file, and the first pair are ./data/4DNFI15H1RVG.fastq.gz and ./data/4DNFIZHUKESO.fastq.gz, the second pair are for: ./data/4DNFIKVDGNJN.fastq.gz and ./data/4DNFIEQ58J6G.fastq.gz,  the target is the first pair result is bwa_1.bam,the second pair result is bwa.2.bam. the generated script is:\n\nbash ./scripts/run-bwa-mem.sh ./data/4DNFI15H1RVG.fastq.gz ./data/4DNFIZHUKESO.fastq.gz  ./data/hg38.bwaindex.tgz ./output/003/\u0020 bwa_1 64\n\nbash ./scripts/run-bwa-mem.sh ./data/4DNFIKVDGNJN.fastq.gz ./data/4DNFIEQ58J6G.fastq.gz  ./data/hg38.bwaindex.tgz ./output/003/\u0020bwa_2 64\n\nWhen the command is generated, the file name and the output path must require a space, otherwise the file name will be regarded as the folder name. bwa_1,bwa_2 indicate the generated file name, You should adjust according to the description of the desired destination file name. and 64 indicate the number of threads. The number of threads is usually set to 16 or 32. You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-bwa-mem.sh by hic workflow",
            "page": 8
        }
    },
    {
        "content": "2. run-pairsam-parse-sort.sh\nRuns pairsam parse and sort on a bwa-produced bam file and produces a sorted pairsam file\nInput: a bam file\nOutput: a pairsam file\nUsage\nRun the following in the container\nrun-pairsam-parse-sort.sh <input_bam> <chromsizes> <outdir> <outprefix> <nthread> <compress_program>\n# input_bam : an input bam file.\n# chromsizes : a chromsize file\n# outdir : output directory\n# outprefix : prefix of output files\n# nthread : number of threads to use\n\nSet parameters according to the example:\nSuppose the input file is: ./output/aligned/a1.bam  and ./output/aligned/abc.bam, the target is ./output/003/pairsam_fix1.sam.pairs.gz and ./output/003/pairsam_fix2.sam.pairs.gz . ./hg38.chrom.sizes  is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-pairsam-parse-sort.sh ./output/aligned/a1.bam ./hg38.chrom.sizes ./output/003/  pairsam_fix1 64 lz4c \nbash ./scripts/run-pairsam-parse-sort.sh ./output/aligned/abc.bam ./hg38.chrom.sizes ./output/003/  pairsam_fix2 64 lz4c\n\nYou can install the tool, but do not do any additional operations.and 64 indicate the number of threads.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 9
        }
        
    },
    {
        "content": "3. run-pairsam-merge.sh\nMerges a list of pairsam files\nInput: a list of pairsam files\nOutput: a merged pairsam file\nUsage\nRun the following in the container\nrun-pairsam-merge.sh <outprefix> <nthreads> <input_pairsam1> [<input_pairsam2> [<input_pairsam3> [...]]]\n# outprefix : prefix of output files\n# nthreads : number of threads to use   \n# input_pairsam : an input pairsam file.\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/pairsam_fix1.sam.pairs.gz  and ./output/003/pairsam_fix2.sam.pairs.gz, the target is ./output/003/out.merged.sam.pairs.gz . Generate the following sample script:\n\nbash ./scripts/run-pairsam-merge.sh ./output/003/out 64 './output/003/pairsam_fix1.sam.pairs.gz ./output/003/pairsam_fix2.sam.pairs.gz' \n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 10
        }
        
    },
    {
        "content": "4. run-pairsam-markasdup.sh\nTakes a pairsam file in and creates a pairsam file with duplicate reads marked\n* Input: a pairsam file\n* Output: a duplicate-marked pairsam file\nUsage\nRun the following in the container\nrun-pairsam-markasdup.sh <input_pairsam>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n\nSet parameters according to the example:\nSuppose the input file is:  ./output/003/out.merged.sam.pairs.gz,the target is ./output/003/out1.  Generate the following sample script:\n\nbash ./scripts/run-pairsam-markasdup.sh  ./output/003/out.merged.sam.pairs.gz  ./output/003/out1  \n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",          
        "metadata": 
        {            
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 11
        }
        
    },
    {
        "content": "5. run-pairsam-filter.sh\nTakes in a pairsam file and creates a lossless, annotated bam file and a filtered pairs file.\nInput: a pairsam file\nOutput: an annotated bam file and a filtered pairs file\nUsage\nRun the following in the container\nrun-pairsam-filter.sh <input_pairsam> <outprefix> <chromsizes>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n# chromsizes : a chromsize file\n\nSet parameters according to the example:\nSuppose the input file is: ../sample_data/outlala.merged.sam.pairs.gz, the target is  ./out/out12345.dedup.pairs.gz and ./out/out12345.lossless.bam ,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-pairsam-filter.sh  ./sample_data/outlala.merged.sam.pairs.gz ./out/out12345  /home/agent/POPGENAGENT/data/hg38.chrom.sizes\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 12
        }
        
    },
    {
        "content": "6. run-merge-pairs.sh\nAlignment module for Hi-C data, based on merge-pairs.\n\nInput : a set of pairs files, with their associated indices\nOutput : a merged pairs file and its index\nUsage\nRun the following in the container.\nrun-merge-pairs.sh <output_prefix> <pairs1> <pairs2> [<pairs3> [...]]  \n# output_prefix : prefix of the output pairs file.\n# pairs1, pairs2, ... : input pairs files\n\nSet parameters according to the example:\nSuppose the input file is: ./out/out12345.dedup.pairs.gz, the target is ./out/out321.pairs.gz. Generate the following sample script:\n\nbash ./scripts/run-merge-pairs.sh ./out/out321 ./out/out12345.dedup.pairs.gz\n\nYou can install the tool, but do not do any additional operations. Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 13
        }
        
    },
    {
        "content": "7. run-cooler.sh\nRuns cooler to create an unnormalized matrix .cool file, taking in a (4dn-style) pairs file\nInput : a pairs file (.gz, along with .px2), chrom.size file\nOutput : a contact matrix file (.cool)\nUsage\nRun the following in the container.\nrun-cooler.sh <input_pairs> `<chromsize>` `<binsize>` `<ncores>` <output_prefix> <max_split>\nrun-cooler.sh <input_pairs> <chromsize> <binsize> <ncores> <output_prefix> <max_split>\n# input_pairs : a pairs file\n# chromsize : a chromsize file\n# binsize : binsize in bp\n# ncores : number of cores to use\n# output_prefix : prefix of the output cool file\n# max_split : max_split argument for cooler (e.g. 2 which is default for cooler) \n\nSet parameters according to the example:\nSuppose the input file is:/home/agent/POPGENAGENT/sample_data/outlala.ff.pairs.gz , the target is  ./out/out3 ,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-cooler.sh      /home/agent/POPGENAGENT/sample_data/outlala.ff.pairs.gz      /home/agent/POPGENAGENT/data/hg38.chrom.sizes      1000      32      ./out/out3      2\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 32 indicate the number of threads.",            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 14
        }
        
    },
    {
        "content": "8. run-cooler-balance.sh\nRuns cooler to create a normalized matrix file, taking in an unnormalized .cool file\nInput: a cool file (.cool)\nOutput : a cool file (.cool)\nUsage\nRun the following in the container.\nrun-cooler-balance.sh <input_cool> <max_iter> <output_prefix> <chunksize>\n# input_cool : a cool file (without normalization vector)\n# max_iter : maximum number of iterations\n# output_prefix : prefix of the output cool file\n# chunksize : chunksize argument for cooler (e.g. 10000000 which is default for cooler)\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/cool_prefix.cool , the target is ./output/003/cool_prefix_normalized.cool,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-cooler-balance.sh ./output/003/cool_prefix.cool 1000 ./output/003/cool_prefix_normalized 10000000\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 15
        }
        
    },
    {
        "content": "9. run-cool2multirescool.sh\nRuns cooler coarsegrain to create multi-res cool file from a .cool file.\nInput : a cool file (.cool)\nOutput : a multires.cool file (.multires.cool)\nUsage\nRun the following in the container.\nrun-cool2multirescool.sh -i <input_cool> [-p <ncores>] [-o <output_prefix>] [-c <chunksize>] [-j] [-u custom_res] [-B]\n# input_cool : a (single-res) cool file with the highest resolution you want in the multi-res cool file\n# -p ncores: number of cores to use (default: 1)\n# -o output_prefix: prefix of the output multires.cool file (default: out)\n# -c chunksize : chunksize argument of cooler (e.g. default: 10000000)\n# -j : juicer resolutions (default: use HiGlass resolutions)\n# -u custom_res : custom resolutions separated by commas (e.g. 100000,200000,500000). The minimum of this set must match min_res (-r).\n# -B : no balancing/normalizations\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/cool_prefix_normalized.cool , the target is ./output/003/multirescool_prefix.mcool. Generate the following sample script:\n\nbash ./scripts/run-cool2multirescool.sh -i ./output/003/cool_prefix_normalized.cool -p 8 -o ./output/003/multirescool_prefix -c 10000000 -j -u 100000,200000,500000\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
            "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 16
        }
        
    },
    {
        "content":  "## Overview\n\nBWA (Burrows-Wheeler Aligner) is a widely used sequence alignment tool designed for mapping low-divergent reads to large reference genomes (e.g., human genomes). BWA includes three main algorithms:\n\n1. **BWA-backtrack**: Optimized for short reads up to 100 bp.\n2. **BWA-SW**: Optimized for longer sequences (70 bp to 1 Mbps), supports long reads and split alignment.\n3. **BWA-MEM**: The newest algorithm in the BWA suite, recommended for high-quality reads. It is both fast and accurate, with support for read lengths from 70 bp to 1 Mbps. BWA-MEM also performs well for reads between 70 and 100 bp.\n\nAmong these, **BWA-MEM** is widely used due to its balance of speed and accuracy, making it suitable for various downstream applications, such as whole-genome variant calling. It supports long reads, split alignment, and chimeric reads, and outputs standard SAM files that are compatible with tools like samtools and GATK.\n\nBefore running BWA-MEM, you must build an FM-index of the reference genome using the `bwa index` command. Once the index is created, you can align reads to the reference genome with the `bwa mem` sub-command.\n\n---\n\n## Installation\n\n1. **Download the BWA package**: Obtain the source code from the official repository.\n2. **Compile and install**: Typically, you will use a C compiler (e.g., GCC). The official documentation provides detailed build instructions.\n3. **Finalize installation**: Make sure your environment variables or system PATH are set correctly so you can call `bwa` directly.\n\n---\n\n## Quick Start\n\n1. **Indexing the reference genome**:\n\n```bash\nbwa index reference.fa\n```\n\nThis command builds the required FM-index for the reference genome `reference.fa`.\n\n2. **Running the alignment**:\n\n```bash\nbwa mem reference.fa reads.fq > alignment.sam\n```\n\nAligns the reads in `reads.fq` to the reference `reference.fa` and writes the alignment in SAM format to `alignment.sam`.\n\n---\n\n## Examples of Popular Commands\n\nBelow are five common BWA-MEM usage examples:\n\n1. **Indexing a reference genome with a custom prefix**:\n\n```bash\nbwa index -p ref_index reference.fa\n```\n\nGenerates an index for `reference.fa` with the prefix `ref_index`.\n\n2. **Single-end read alignment**:\n\n```bash\nbwa mem ref_index reads.fq > aligned_reads.sam\n```\n\nAligns single-end reads in `reads.fq` to the reference indexed by `ref_index` and outputs the result to `aligned_reads.sam`.\n\n3. **Paired-end read alignment**:\n\n```bash\nbwa mem ref_index reads_1.fq reads_2.fq > aligned_pair.sam\n```\n\nAligns paired-end reads in `reads_1.fq` and `reads_2.fq` and saves them to `aligned_pair.sam`.\n\n4. **Generating a sorted BAM file directly**:\n\n```bash\nbwa mem ref_index reads.fq | samtools sort -o sorted_reads.bam\n```\n\nPipes the alignment output from BWA to samtools and sorts it before writing to `sorted_reads.bam`.\n\n5. **Using extra options for speed or marking secondary alignments**:\n\n```bash\nbwa mem -t 4 -M ref_index reads.fq > aligned_reads.sam\n```\n\n- `-t 4`: Use 4 threads for faster alignment.\n- `-M`: Mark shorter split alignments as secondary, which helps compatibility with certain downstream tools (e.g., Picard).\n\n---\n\n## Learning Objectives (Variant Calling Workflow)\n\n- Exploring the variant calling workflow\n- Choosing suitable BWA parameters for your dataset\n- Understanding alignment clean-up steps\n\n---\n\n## Variant Calling Workflow\n\nA typical variant calling workflow includes:\n\n1. Quality control (QC)\n2. Alignment (e.g., using BWA)\n3. Alignment clean-up (e.g., marking duplicates, sorting)\n4. Variant calling\n5. Variant filtering and annotation\n\nAfter obtaining raw sequencing data, you typically use tools like FastQC to check read quality. Next, you align the reads to a reference genome and clean up the alignments before applying variant-calling tools (e.g., GATK or samtools/bcftools) and then filter/annotate the resulting variants.\n\n---\n\n## Environment and Directory Setup\n\nIn a cluster environment (e.g., Harvard’s O2), the process might be as follows:\n\n1. **Request an interactive session** (example command):\n\n```bash\nsrun --pty -p interactive -t 0-6:00 --mem 8G -c 2 --reservation=HBC bash\n```\n\n2. **Create project directories**:\n\n```bash\nmkdir ~/var-calling\ncd ~/var-calling\n\nmkdir -p raw_data reference_data scripts logs meta results/bwa\n```\n\n3. **Copy required data**:\n\n```bash\ncp /n/groups/hbctraining/ngs-data-analysis-longcourse/var-calling/raw_fastq/*fq raw_data/\ncp /n/groups/hbctraining/ngs-data-analysis-longcourse/var-calling/reference_data/chr20.fa reference_data/\n```\n\nIn this tutorial, we use a subset of the Genome in a Bottle (GIAB) NA12878 data (human genome reads restricted to chromosome 20), consisting of ~4 million paired-end reads.\n\n---\n\n## QC and Alignment\n\n1. **Skipping QC**: Typically you would use FastQC, but for demonstration we skip that step.\n2. **Aligner choice**: BWA is often preferred in variant calling for its high accuracy. Minimal misalignment can help avoid false positives in variant detection.\n3. **BWA modes**:\n   - BWA-backtrack: Up to 100-bp reads\n   - BWA-SW: Longer reads (70 bp ~ 1 Mbps) with split alignment\n   - BWA-MEM: Newest, recommended for most use cases, supports long reads and high accuracy\n\nIn most variant-calling workflows, **BWA-MEM** is used.\n\n---\n\n## Using BWA-MEM for Alignment\n\n### 1. Creating a BWA-MEM Index\n\n```bash\ncd ~/var-calling/reference_data\nmodule load gcc/6.2.0 bwa/0.7.8\n\nbwa index -p chr20 chr20.fa\n```\n\n- `-p chr20`: Uses `chr20` as the prefix for all index files.\n- `chr20.fa`: Reference genome file (only chromosome 20 here).\n\n### 2. Aligning Reads\n\n```bash\ncd ~/var-calling\n\nbwa mem -M -t 2 \\\n  reference_data/chr20 \\\n  raw_data/na12878_1.fq raw_data/na12878_2.fq \\\n  2> logs/bwa.err \\\n  > results/bwa/na12878.sam\n```\n\n- `-M`: Marks shorter split hits as secondary, helpful for some downstream tools.\n- `-t 2`: Uses 2 threads.\n- `2> logs/bwa.err`: Redirects standard error to a log file.\n- `> results/bwa/na12878.sam`: Writes output to `na12878.sam`.\n\n---\n\n## Alignment Clean-up\n\nFor variant calling, marking duplicates is crucial to avoid PCR artifact errors.\n\n### 1. Installing/Loading Picard\n\n```bash\nmodule spider picard\nmodule load picard/2.8.0\n```\n\nUsage:\n\n```bash\njava -jar $PICARD/picard-2.8.0.jar [ToolName] [options]\n```\n\n### 2. Sorting by Coordinate (SortSam)\n\nPicard’s `SortSam` tool sorts SAM/BAM files by coordinate. Key options:\n- `INPUT`: Input file (SAM/BAM)\n- `OUTPUT`: Output file (SAM/BAM)\n- `SORT_ORDER`: Sort order (e.g., coordinate, queryname)\n- `VALIDATION_STRINGENCY`: Level of validation (set to `SILENT` to avoid errors from BWA’s unmapped flags)\n\nExample:\n\n```bash\ncd results/bwa\n\njava -Xmx8G -jar $PICARD/picard-2.8.0.jar SortSam \\\n  INPUT=na12878.sam \\\n  OUTPUT=na12878_sorted.sam \\\n  SORT_ORDER=coordinate \\\n  VALIDATION_STRINGENCY=SILENT\n```\n\n### 3. Marking Duplicates (MarkDuplicates)\n\nPicard’s `MarkDuplicates` identifies and tags duplicate reads (PCR or optical) in BAM/SAM files. Key options:\n- `INPUT`: Sorted input file\n- `OUTPUT`: Output file\n- `METRICS_FILE`: File to write duplication metrics\n- `ASSUME_SORTED`: Set to true if the input is coordinate-sorted\n- `VALIDATION_STRINGENCY`: Similar to above\n\nExample:\n\n```bash\njava -Xmx8G -jar $PICARD/picard-2.8.0.jar MarkDuplicates \\\n  INPUT=na12878_sorted.sam \\\n  OUTPUT=na12878_sorted_marked.bam \\\n  METRICS_FILE=metrics.txt \\\n  ASSUME_SORTED=true \\\n  VALIDATION_STRINGENCY=SILENT\n```\n\n`-Xmx8G` ensures Java uses no more than 8 GB of memory (adjust if needed for larger data).\n\n### 4. Creating an Index for the BAM File\n\nUse samtools to index the marked BAM file for visualization or downstream steps:\n\n```bash\nmodule load gcc/6.2.0 samtools/1.9\n\nsamtools index na12878_sorted_marked.bam\n```\n\n---\n\n## Summary\n\nFollowing these steps, you have:\n1. Used **BWA-MEM** to align reads to a reference genome\n2. Sorted the alignment results\n3. Marked duplicates\n4. Created an index for the final BAM file\n\nThe resulting file can now be used in subsequent variant-calling pipelines (e.g., GATK or bcftools), followed by variant filtering and annotation. These materials are adapted from open-access teaching materials by the Harvard Chan Bioinformatics Core (HBC).",          
         "metadata": {
            "source": "BWA-MEM",
            "page": 17
        }
        
    },
    {
        "content":  "# samtools – Utilities for the Sequence Alignment/Map (SAM) format\n\n## NAME\n**samtools** – A suite of programs for manipulating alignments in the SAM (Sequence Alignment/Map), BAM, and CRAM formats.\n\n## SYNOPSIS (Selected Examples)\n\n```bash\nsamtools addreplacerg -r 'ID:fish' -r 'LB:1334' -r 'SM:alpha' -o output.bam input.bam\nsamtools ampliconclip -b bed.file input.bam\nsamtools ampliconstats primers.bed in.bam\nsamtools bedcov aln.sorted.bam\nsamtools calmd in.sorted.bam ref.fasta\nsamtools cat out.bam in1.bam in2.bam in3.bam\nsamtools collate -o aln.name_collated.bam aln.sorted.bam\nsamtools consensus -o out.fasta in.bam\nsamtools coverage aln.sorted.bam\nsamtools cram-size -v -o out.size in.cram\nsamtools depad input.bam\nsamtools depth aln.sorted.bam\nsamtools dict -a GRCh38 -s \"Homo sapiens\" ref.fasta\nsamtools faidx ref.fasta\nsamtools fasta input.bam > output.fasta\nsamtools fastq input.bam > output.fastq\nsamtools fixmate in.namesorted.sam out.bam\nsamtools flags PAIRED,UNMAP,MUNMAP\nsamtools flagstat aln.sorted.bam\nsamtools fqidx ref.fastq\nsamtools head in.bam\nsamtools idxstats aln.sorted.bam\nsamtools import input.fastq > output.bam\nsamtools index aln.sorted.bam\nsamtools markdup in.algnsorted.bam out.bam\nsamtools merge out.bam in1.bam in2.bam in3.bam\nsamtools mpileup -f ref.fasta -r chr3:1,000-2,000 in1.bam in2.bam\nsamtools phase input.bam\nsamtools quickcheck in1.bam in2.cram\nsamtools reference -o ref.fa in.cram\nsamtools reheader in.header.sam in.bam > out.bam\nsamtools reset -o /tmp/reset.bam processed.bam\nsamtools samples input.bam\nsamtools sort -T /tmp/aln.sorted -o aln.sorted.bam aln.bam\nsamtools split merged.bam\nsamtools stats aln.sorted.bam\nsamtools targetcut input.bam\nsamtools tview aln.sorted.bam ref.fasta\nsamtools view -bt ref_list.txt -o aln.bam aln.sam.gz\n```\n\n## BRIEF DESCRIPTION\nSamtools converts between SAM/BAM/CRAM formats, sorts and merges alignments, indexes them for fast random access, and retrieves reads within specific genomic regions. It can read/write compressed data, works well in pipelines, and supports remote file access.\n\n## SELECTED COMMANDS\n\n### samtools view\n```bash\nsamtools view [options] in.bam [region...]\n```\n- Converts input to SAM/BAM/CRAM.\n- If indexed and region(s) are specified, outputs only alignments overlapping those regions.\n\n### samtools tview\n```bash\nsamtools tview in.sorted.bam [ref.fasta]\n```\n- Text-based viewer for alignments. Press `?` for help.\n\n### samtools quickcheck\n```bash\nsamtools quickcheck in.bam in.cram\n```\n- Quickly checks if files are intact (e.g., valid header, EOF block in BAM).\n\n### samtools head\n```bash\nsamtools head in.bam\n```\n- Prints file headers and optionally the first few alignments.\n\n### samtools index\n```bash\nsamtools index aln.bam\n```\n- Indexes a coordinate-sorted BAM or CRAM file for fast region queries.\n\n### samtools sort\n```bash\nsamtools sort -o aln.sorted.bam aln.bam\n```\n- Sorts alignments by coordinate (default) or by name (`-n`).\n\n### samtools collate\n```bash\nsamtools collate -o aln.name_collated.bam aln.sorted.bam\n```\n- Groups reads by name, without full sorting.\n\n### samtools idxstats\n```bash\nsamtools idxstats aln.sorted.bam\n```\n- Reports reference sequence length, mapped reads, and unmapped reads.\n\n### samtools flagstat\n```bash\nsamtools flagstat aln.sorted.bam\n```\n- Gives overall alignment statistics (number of mapped/unmapped reads, duplicates, etc.).\n\n### samtools mpileup\n```bash\nsamtools mpileup -f ref.fasta in1.bam in2.bam\n```\n- Generates textual pileup of reads.\n\n### samtools consensus\n```bash\nsamtools consensus -o out.fasta in.bam\n```\n- Creates consensus sequence from alignment data.\n\n### samtools coverage\n```bash\nsamtools coverage aln.sorted.bam\n```\n- Produces coverage stats per chromosome or region.\n\n### samtools merge\n```bash\nsamtools merge out.bam in1.bam in2.bam\n```\n- Merges multiple sorted BAM/CRAM files into one.\n\n### samtools split\n```bash\nsamtools split merged.bam\n```\n- Splits one file into separate files by read group.\n\n### samtools cat\n```bash\nsamtools cat out.bam in1.bam in2.bam\n```\n- Concatenates BAM/CRAM files of the same format.\n\n### samtools fastq/fasta\n```bash\nsamtools fastq in.bam > out.fastq\nsamtools fasta in.bam > out.fasta\n```\n- Converts BAM/CRAM to FASTQ or FASTA.\n\n### samtools faidx\n```bash\nsamtools faidx ref.fasta\n```\n- Indexes a FASTA file or extracts subsequences if regions are given.\n\n### samtools fqidx\n```bash\nsamtools fqidx ref.fastq\n```\n- Similar to faidx but for FASTQ (small files only).\n\n### samtools calmd\n```bash\nsamtools calmd aln.bam ref.fasta\n```\n- Adds or updates the MD tag.\n\n### samtools fixmate\n```bash\nsamtools fixmate in.nameSrt.bam out.bam\n```\n- Fills in mate coordinates, ISIZE, and flags from a name-sorted file.\n\n### samtools markdup\n```bash\nsamtools markdup in.algnsorted.bam out.bam\n```\n- Marks duplicate reads after fixmate.\n\n### samtools reheader\n```bash\nsamtools reheader in.header.sam in.bam > out.bam\n```\n- Replaces the header without re-converting the entire file.\n\n### samtools stats\n```bash\nsamtools stats aln.sorted.bam\n```\n- Collects alignment statistics, output can be plotted with plot-bamstats.\n\n---\n\n## AUTHOR & LINKS\n- Original author: Heng Li (Sanger Institute).\n- Project page: [http://www.htslib.org/](http://www.htslib.org/)\n- Code repositories:\n  - [Samtools GitHub](https://github.com/samtools/samtools)\n  - [HTSlib GitHub](https://github.com/samtools/htslib)\n  - [Bcftools GitHub](http://samtools.github.io/bcftools)\n",
        "metadata": {
            "source": "samtools",
            "page": 18
    }
    },
    {
        "content": "## Synopsis\nThis tutorial demonstrates how to explore, process, and manipulate SAM/BAM files using **samtools**. We will walk through installing samtools, creating a demo directory, converting between SAM and BAM formats, sorting, indexing, and selectively viewing alignments.\n\n---\n\n## Installing samtools\n```bash\ncd ~\nmkdir src          # optional if you do not already have a src directory\ncd ~/src\n\ngit clone https://github.com/samtools/htslib\ngit clone https://github.com/samtools/samtools\n\ncd samtools\nmake\ncp samtools ~/bin  # move the samtools binary to your PATH\n```\n\n---\n\n## Setup\n1. **Create** a new directory from your home directory:\n\n   ```bash\n   cd ~\n   mkdir samtools-demo\n   cd samtools-demo\n   ```\n2. **Download** the sample SAM file and decompress:\n\n   ```bash\n   curl https://s3.amazonaws.com/samtools-tutorial/sample.sam.gz > sample.sam.gz\n   gzip -d sample.sam.gz\n   ```\n\n---\n\n## samtools help\nTyping `samtools` alone displays the available subcommands. Examples:\n```bash\nsamtools view\nsamtools sort\nsamtools depth\n```\n\n---\n\n## Converting SAM to BAM (`samtools view`)\n1. **Convert** the SAM file to BAM:\n   ```bash\n   samtools view -S -b sample.sam > sample.bam\n   ```\n   Here:\n   - `-S` specifies the input is SAM.\n   - `-b` specifies to output BAM.\n   - The `>` redirect writes to `sample.bam`.\n2. **Check** the BAM contents (by converting back to SAM and showing only the first few lines):\n   ```bash\n   samtools view sample.bam | head\n   ```\n\n---\n\n## Sorting alignments (`samtools sort`)\n1. **Sort** the BAM so alignments are in order by genomic position:\n   ```bash\n   samtools sort sample.bam -o sample.sorted.bam\n   ```\n2. **Inspect** a few lines:\n   ```bash\n   samtools view sample.sorted.bam | head\n   ```\n   Now the coordinates should appear in ascending order.\n\n---\n\n## Indexing a sorted BAM (`samtools index`)\n1. **Index** the sorted BAM:\n   ```bash\n   samtools index sample.sorted.bam\n   ```\n2. **Check** for the newly created index file (`.bai` extension):\n   ```bash\n   ls\n   ```\n3. **Use** the index to extract alignments from chromosome 1, positions `33,000,000–34,000,000`:\n   ```bash\n   samtools view sample.sorted.bam 1:33000000-34000000\n   ```\n   And count how many alignments are in that region:\n   ```bash\n   samtools view sample.sorted.bam 1:33000000-34000000 | wc -l\n   ```\n\n---\n\n## More on `samtools view`\n### Viewing a subset of alignments\n\n- **Print** the first five lines:\n  ```bash\n  samtools view sample.sorted.bam | head -n 5\n  ```\n- **Make FLAG more readable** with `-X`:\n  ```bash\n  samtools view -X sample.sorted.bam | head -n 5\n  ```\n  (Try `samtools view -?` for help on all options.)\n\n### Counting alignments\n```bash\nsamtools view sample.sorted.bam | wc -l\n```\n\n### Inspecting the header\n```bash\nsamtools view -H sample.sorted.bam\n```\n\n### Filtering by FLAG\n- **Proper pairs** only (`-f 0x2`):\n  ```bash\n  samtools view -f 0x2 sample.sorted.bam | wc -l\n  ```\n- **NOT properly paired** (`-F 0x2`):\n  ```bash\n  samtools view -F 0x2 sample.sorted.bam | wc -l\n  ```\n  `-F` indicates **exclude** flags. The counts of `-f 0x2` plus `-F 0x2` should sum to the total number of alignments.\n\n---\n\n## Summary\n1. **Install** samtools.\n2. **Convert** SAM→BAM.\n3. **Sort** by genomic position.\n4. **Index** to enable region queries.\n5. **View** subsets of data based on coordinates or specific flags.\n6. **Inspect** headers and other metadata.\n\nThis workflow covers basic operations required for many downstream applications (e.g., variant calling or visualization in IGV). For more details, consult the [samtools documentation](https://www.htslib.org/).",
        "metadata": {
            "source": "samtools",
            "page": 19
        }
    },
    {
        "content": "## FASTQ to BAM/CRAM\nModern sequencing instruments typically produce *unaligned* data in FASTQ format. We can store unaligned data in BAM or CRAM to include valuable metadata (e.g., headers, auxiliary tags), but the focus here is on producing a **sorted, aligned** BAM or CRAM file.\n\n---\n\n## Two Main Approaches\n1. **Alignment / mapping** to a known reference.\n2. **De-novo assembly** (not covered in detail here).\n\n---\n\n## Example Workflow for Alignment\n\nThis pipeline takes paired FASTQ reads, aligns them to a reference using Minimap2, and produces a sorted, duplicate-marked BAM or CRAM.\n\n### Steps Overview\n1. **Map / align**\n2. **Fix mate-pair issues** (e.g., with `samtools fixmate`)\n3. **Mark duplicates (part 1)** – add tags needed for later duplication marking\n4. **Sort** to positional order\n5. **Mark duplicates (part 2)** – final marking of duplicates\n6. **Convert** to final file format (BAM or CRAM)\n\n### Step 1: Mapping\n```bash\nminimap2 -t 8 -a -x sr C.Elegans.fa \\\n  SRR065390_1.fastq SRR065390_2.fastq -o CE.sam\n```\n- `-t 8`: uses 8 threads.\n- `-a`: output in SAM.\n- `-x sr`: sets Minimap2 presets for paired-end short reads.\n- Output is name-collated (read pairs are together), which is needed for the next step.\n\n### Step 2: Fixing Mate-Pair Issues\n```bash\nsamtools fixmate -O bam,level=1 CE.sam fixmate.bam\n```\n- `samtools fixmate` checks and corrects mate information (FLAG, RNEXT, PNEXT, TLEN).\n- `-O bam,level=1` outputs BAM with minimal compression for speed.\n- Add `-m` if you want to populate mate CIGAR (`MC`) and mate score (`ms`) tags:\n\n  ```bash\n  samtools fixmate -O bam,level=1 -m CE.sam fixmate.bam\n  ```\n\n### Step 3 & 4: Sorting to Positional Order\n```bash\nsamtools sort -l 1 -@8 -o pos.srt.bam -T /tmp/example_prefix fixmate.bam\n```\n- Sort reads by genomic coordinate.\n- `-@8`: uses 8 threads.\n- `-l 1`: minimal compression (optional).\n- `-T /tmp/example_prefix`: prefix for temporary sorting files.\n\n### Step 5: Marking Duplicates\n```bash\nsamtools markdup -O bam,level=1 pos.srt.bam markdup.bam\n```\n- Uses the MC/ms tags from the `-m` fixmate step.\n\n### Step 6: Converting to Final Format\n```bash\nsamtools view -@8 markdup.bam -o final.bam\n```\nor, to produce CRAM:\n```bash\nsamtools view -T C.Elegans.fa -@8 markdup.bam -o final.cram\n```\n\n---\n\n## Pipelining for Efficiency\nInstead of writing each intermediate file to disk, we can **pipe** the commands:\n```bash\nminimap2 -t 8 -a -x sr C.Elegans.fa SRR065390_[12].fastq | \\\nsamtools fixmate -u -m - - | \\\nsamtools sort -u -@2 -T /tmp/example_prefix - | \\\nsamtools markdup -@8 --reference C.Elegans.fa - final.cram\n```\n- `-u` or `-O bam,level=0` keeps data uncompressed in the pipeline for speed.\n- `set -o pipefail` helps detect errors in earlier commands.\n\n---\n\n## Converting Back to FASTQ\nIf the unmapped reads were retained, you can revert your final BAM/CRAM to FASTQ:\n```bash\nsamtools sort -n -@8 final.cram | \\\nsamtools fastq - -1 dat_1.fq -2 dat_2.fq > /dev/null\n```\n- Sorting by read name (`-n`) is required before producing paired FASTQ.\n- The original exact FASTQ order is not preserved but is generally unimportant for reanalysis.\n\n---\n\n## De-novo Assembly\n- Assemblers produce a consensus FASTA or FASTQ rather than individual alignments.\n- If you need per-read alignments to the newly assembled consensus, just build an index on the assembly and follow the same **Mapping** steps as above.\n- For CRAM, ensure you have access to the same consensus reference or embed it:\n  ```bash\n  samtools view -O CRAM,embed_ref in.sam -o out.cram\n  ```\n\n---\n\n## Key Takeaways\n- **Name-collated** BAM is essential for certain steps (e.g., `fixmate`, partial duplication marking).\n- **Coordinate-sorted** BAM is needed for final duplicate marking and most downstream analyses.\n- **Pipelines** avoid intermediate files, are faster, and use uncompressed or minimal-compression data in memory.\n- **CRAM** can reduce storage size but requires consistent references.\n- Always verify each step with basic stats (`samtools flagstat`, etc.) to ensure no unexpected data loss.\n",
        "metadata": {
            "source": "samtools",
            "page": 20
        }
    },
    {
        "content": "# WGS/WES Mapping to Variant Calls\n\n## Overview\nTypical DNA sequence analysis involves three main phases: **Mapping**, **Improvement**, and **Variant Calling**. Below is a common workflow illustrating these steps for Whole Genome Sequencing (WGS) or Whole Exome Sequencing (WES).\n\n---\n\n## 1. Mapping\n\n1. **Reference Preparation**:\n   ```bash\n   bwa index <ref.fa>\n   ```\n   - Prepares a Burrows–Wheeler Transform (BWT) index, required for BWA.\n\n2. **Aligning Reads with BWA-MEM**:\n   ```bash\n   bwa mem -R '@RG\\tID:foo\\tSM:bar\\tLB:library1' <ref.fa> <read1.fq> <read2.fq> > lane.sam\n   ```\n   - The `-R` flag attaches read-group metadata (e.g., sample name, library). This data is crucial for downstream tools.\n   - Output is in SAM format.\n\n3. **Fixmate**:\n   ```bash\n   samtools fixmate -O bam <lane.sam> <lane_fixmate.bam>\n   ```\n   - Cleans up pairing flags, ensuring consistent FLAG and mate info.\n\n4. **Sort**:\n   ```bash\n   samtools sort -O bam -o <lane_sorted.bam> -T </tmp/lane_temp> <lane_fixmate.bam>\n   ```\n   - Converts name-collated BAM to coordinate-sorted BAM for downstream steps.\n\n---\n\n## 2. Improvement\n\n### 2.1 Indel Realignment (GATK)\n\nMisalignment around INDELs can cause false variant calls. The GATK realignment step includes two commands:\n\n```bash\n# Step 1: Identify intervals needing realignment\njava -Xmx2g -jar GenomeAnalysisTK.jar -T RealignerTargetCreator \\\n  -R <ref.fa> -I <lane.bam> -o <lane.intervals> --known <Mills1000G.b38.vcf>\n\n# Step 2: Realign around those intervals\njava -Xmx4g -jar GenomeAnalysisTK.jar -T IndelRealigner -R <ref.fa> \\\n  -I <lane.bam> -targetIntervals <lane.intervals> \\\n  --known <Mills1000G.b38.vcf> -o <lane_realigned.bam>\n```\n\n### 2.2 Base Quality Score Recalibration (GATK BQSR)\n\nReduces machine- or run-specific biases by adjusting base quality scores.\n\n```bash\n# Analyze covariates\njava -Xmx4g -jar GenomeAnalysisTK.jar -T BaseRecalibrator \\\n  -R <ref.fa> -knownSites <dbsnp_142.b38.vcf> \\\n  -I <lane.bam> -o <lane_recal.table>\n\n# Apply recalibration\njava -Xmx2g -jar GenomeAnalysisTK.jar -T PrintReads -R <ref.fa> \\\n  -I <lane.bam> --BQSR <lane_recal.table> -o <lane_recal.bam>\n```\n\n### 2.3 Marking Duplicates & Merging Libraries\n\n1. **Mark Duplicates**:\n   ```bash\n   java -Xmx2g -jar MarkDuplicates.jar VALIDATION_STRINGENCY=LENIENT \\\n     INPUT=<lane_1.bam> INPUT=<lane_2.bam> \\\n     OUTPUT=<library.bam>\n   ```\n   - Marks PCR/optical duplicates.\n\n2. **Merge** libraries into a single sample-level BAM:\n   ```bash\n   samtools merge <sample.bam> <library1.bam> <library2.bam> <library3.bam>\n   samtools index <sample.bam>\n   ```\n\n3. **(Optional) Realign Once More**:\n   ```bash\n   java -Xmx2g -jar GenomeAnalysisTK.jar -T RealignerTargetCreator ...\n   java -Xmx4g -jar GenomeAnalysisTK.jar -T IndelRealigner ...\n   samtools index <sample_realigned.bam>\n   ```\n\n---\n\n## 3. Variant Calling\n\n### 3.1 Generating BCF/VCF with bcftools\n\n`mpileup` is used to gather per-base data, and `bcftools call` calls variants:\n\n```bash\n# One-step approach using a pipe\nbcftools mpileup -Ou -f <ref.fa> <sample1.bam> <sample2.bam> <sample3.bam> | \\\n  bcftools call -vmO z -o <study.vcf.gz>\n```\n\nAlternatively, two-step:\n```bash\n# Step 1: produce BCF\nbcftools mpileup -Ob -o <study.bcf> -f <ref.fa> <sample1.bam> <sample2.bam> <sample3.bam>\n\n# Step 2: call variants\nbcftools call -vmO z -o <study.vcf.gz> <study.bcf>\n```\n\n### 3.2 VCF Indexing & Statistics\n\n```bash\ntabix -p vcf <study.vcf.gz>\n\nbcftools stats -F <ref.fa> -s - <study.vcf.gz> > <study.vcf.gz.stats>\nmkdir plots\nplot-vcfstats -p plots/ <study.vcf.gz.stats>\n```\n\n### 3.3 Filtering\n\n```bash\nbcftools filter -O z -o <study_filtered.vcf.gz> -s LOWQUAL \\\n  -i'%QUAL>10' <study.vcf.gz>\n```\n\nFiltering strategies depend heavily on the study’s goals, data quality, and coverage.\n\n---\n\n## Key Points\n1. **Mapping**: Use BWA-MEM (index reference, align reads, fixmate, sort).\n2. **Improvement**: Realign around indels, recalibrate base qualities, mark duplicates, and merge.\n3. **Variant Calling**: Generate pileup and call variants (bcftools), index VCF, gather stats, and filter.\n4. **Read Group Info** (`@RG`): Ensure correct `ID`, `SM`, and `LB` for GATK/bcftools compatibility.\n5. **QC & Filtering**: Visualize with IGV, review alignment stats, apply appropriate variant filters.\n",
        "metadata": {
            "source": "samtools",
            "page": 21
        }
    },
    {
        "content": "## Quick Guide to DiffBind\n\n**DiffBind** is an R/Bioconductor package for analyzing differential binding (peaks) in ChIP-seq or ATAC-seq data. Below is a concise workflow:\n\n---\n\n### 1. Installation & Setup\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DiffBind\")\n\nlibrary(DiffBind)\n```\n\n---\n\n### 2. Prepare Input Files\n\n- **SampleSheet.csv**: Each row describes one sample, including columns:\n  - `SampleID`, `Tissue`, `Factor`, `Condition`, `Replicate`\n  - `bamReads`, `bamControl` (paths to BAM files)\n  - `Peaks` (path to called peak file)\n  - `PeakCaller` (e.g., MACS2)\n\nExample:\n```csv\nSampleID,Tissue,Factor,Condition,Replicate,bamReads,bamControl,Peaks,PeakCaller\nS1,MCF7,ER,Responsive,1,S1.bam,S1_input.bam,S1_peaks.bed,macs2\nS2,MCF7,ER,Responsive,2,S2.bam,S2_input.bam,S2_peaks.bed,macs2\n...\n```\n\n---\n\n### 3. Read & Merge Peaks\n```r\n# Read metadata from CSV and merge overlapping peaks\ndbObj <- dba(sampleSheet = \"SampleSheet.csv\")\nplot(dbObj)  # Heatmap of sample correlations\n```\n\n---\n\n### 4. Count Reads in Peaks\n```r\n# Counts reads from BAM files in merged peak regions\n# bUseSummarizeOverlaps=TRUE uses a more standardized counting method\ndbObj <- dba.count(dbObj, bUseSummarizeOverlaps=TRUE)\n\n# Optional: library size normalization\ndbObj <- dba.normalize(dbObj)\n```\n\n---\n\n### 5. Differential Analysis\n\n1. **Set contrast** (which groups to compare):\n```r\n# Example: Condition = \"Resistant\" vs. \"Responsive\", with \"Responsive\" as baseline\n# or specify a design formula including multiple factors\n\ndbObj <- dba.contrast(dbObj, reorderMeta=list(Condition=\"Responsive\"))\n```\n\n2. **Run differential binding** (using DESeq2 by default):\n```r\ndbObj <- dba.analyze(dbObj)\n\n# Show results (number of significantly changed sites)\ndba.show(dbObj, bContrasts=TRUE)\n```\n\n3. **Extract differentially bound peaks**:\n```r\ndbSites <- dba.report(dbObj)\n\n# Convert to data frame, filter by FDR, save as BED\nout <- as.data.frame(dbSites)\nsigPeaks <- out[out$FDR < 0.05, c(\"seqnames\",\"start\",\"end\",\"strand\",\"Fold\")]\nwrite.table(sigPeaks, \"diffbind_sig_peaks.bed\", sep=\"\\t\", row.names=FALSE, col.names=FALSE)\n```\n\n---\n\n### 6. Visualization\n\n- **Heatmap**:\n  ```r\n  dba.plotHeatmap(dbObj)\n  ```\n\n- **PCA**:\n  ```r\n  dba.plotPCA(dbObj, label=DBA_CONDITION)\n  ```\n\n- **Venn Diagram** (overlaps):\n  ```r\n  dba.plotVenn(dbObj, contrast=1)\n  ```\n\n- **Volcano Plot**:\n  ```r\n  dba.plotVolcano(dbObj)\n  ```\n\n---\n\n### Key Points\n1. **SampleSheet** + **peak files** + **BAM files** required.\n2. Merged peaks → read counting → normalization → differential analysis.\n3. Results are retrieved via `dba.report()`. Visualization includes heatmaps, PCA, venn diagrams.\n4. By default, **DESeq2** is used, but **edgeR** can also be specified.\n\nFor more detail, see the [DiffBind vignette](https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf).",
        "metadata": {
            "source": "Diffbind",
            "page": 22
        }
    },
    {
        "content": "Instructions for using the STAR tool: Below is a more concise overview of STAR usage (sections 1.2 through 14.25), preserving all key instructions.\n\n---\n\n## 1.2 Basic Work Flow\n\n**STAR** involves two main steps:\n\n1. **Generating genome indexes**  \n   - Input: reference genome FASTA (one or more) and (optional but recommended) annotation GTF.\n   - Output: STAR-specific index files stored in a user-defined directory.\n\n2. **Mapping reads to the genome**  \n   - Input: the STAR index folder plus RNA-seq reads (FASTA/FASTQ).  \n   - Output: alignment files (SAM/BAM), splice junctions, unmapped reads, logs, etc.\n\n---\n\n## 2. Generating Genome Indexes\n\n### 2.1 Basic Options\n\nExample:\n```\nSTAR \\\n  --runThreadN <threads> \\\n  --runMode genomeGenerate \\\n  --genomeDir /path/to/genomeDir \\\n  --genomeFastaFiles /path/to/genome.fa [more FASTAs...] \\\n  --sjdbGTFfile /path/to/annotations.gtf \\\n  --sjdbOverhang <ReadLength - 1>\n```\n- **--runThreadN**: number of threads.\n- **--runMode genomeGenerate**: tells STAR to build the genome index.\n- **--genomeDir**: output folder for index files.\n- **--genomeFastaFiles**: one or more uncompressed FASTA files.\n- **--sjdbGTFfile**: optional but strongly recommended GTF.\n- **--sjdbOverhang**: normally set to read length minus 1.\n\n### 2.2 Advanced Notes\n\n- **Include major chromosomes and unlocalized scaffolds**; skip patches/haplotypes unless needed.\n- **Annotations must match genome FASTA** (naming consistency: e.g., both from Ensembl).\n- **Small genomes**: scale down `--genomeSAindexNbases` (e.g., `min(14, log2(GenomeLength)/2 - 1)`).\n- **Large references**: if >5000 scaffolds, reduce `--genomeChrBinNbits`.\n\n---\n\n## 3. Running Mapping Jobs\n\n### 3.1 Basic Options\n\nExample:\n```\nSTAR \\\n  --runThreadN <threads> \\\n  --genomeDir /path/to/genomeDir \\\n  --readFilesIn /path/to/read1 [ /path/to/read2 ]\n```\n- **--genomeDir**: folder containing the STAR indexes.\n- **--readFilesIn**: reads to map (paired-end requires read1 and read2).  \n- If reads are gzipped: `--readFilesCommand zcat` (or similar) to decompress on the fly.\n\n### 3.2 Other Key Options\n\n- **Using annotations at mapping**: `--sjdbGTFfile <ann.gtf>` can be specified again here, even if not used when generating the index.\n- **ENCODE-style defaults**: e.g., `--outFilterType BySJout`, `--alignSJoverhangMin 8`, etc.\n- **Shared memory**: controlled by `--genomeLoad` (e.g. `LoadAndKeep`). May need adjusting system kernel parameters.\n\n---\n\n## 4. Output Files\n\n- **Log.out**: main detailed log.  \n- **Log.progress.out**: real-time stats.  \n- **Log.final.out**: final mapping summary (unique/multi-mapping rates, splices, etc.).\n- **Aligned.out.sam** or **Aligned.out.bam**: primary alignment file.\n- **Aligned.sortedByCoord.out.bam**: coordinate-sorted BAM if `--outSAMtype BAM SortedByCoordinate`.\n- **SJ.out.tab**: high-confidence splice junctions.\n\n---\n\n## 5. Chimeric/Fusion Alignments\n\n- Enable detection with `--chimSegmentMin > 0`.\n- Can output to the main BAM (`--chimOutType WithinBAM`) or a separate file (`Chimeric.out.sam` or `Chimeric.out.junction`).\n- **STAR-Fusion** is recommended for robust fusion detection.\n\n---\n\n## 6. Output in Transcript Coordinates\n\n- **--quantMode TranscriptomeSAM**: outputs `Aligned.toTranscriptome.out.bam` (used by tools like RSEM/eXpress).  \n- By default, indels and soft-clips are disallowed in transcriptome alignments.\n\n---\n\n## 7. Counting Reads per Gene\n\n- **--quantMode GeneCounts**: counts reads overlapping genes.  \n- Output in `ReadsPerGene.out.tab` with four columns (unstranded, forward-strand, reverse-strand, etc.).\n\n---\n\n## 8. 2-pass Mapping\n\nImproves novel junction discovery:\n1. **Multi-sample**: run pass 1 for all samples, merge/collect junctions, then pass 2 using these junctions.\n2. **Per-sample**: use `--twopassMode Basic`, so STAR auto-discovers junctions in pass 1 and re-maps in pass 2.\n\n---\n\n## 9. Merging Overlapping Paired-end Reads\n\n- **--peOverlapNbasesMin**, **--peOverlapMMp**: merges mates if they overlap significantly, improving accuracy and enabling chimeric detection in the overlapped region.\n\n---\n\n## 10. Personal Variants\n\n- **--varVCFfile**: specify a VCF (currently SNVs only) to mark overlap with genotype alleles in alignments.\n\n---\n\n## 11. WASP Filtering (Allele-Specific)\n\n- **--waspOutputMode SAMtag**: adds `vW` to each alignment indicating whether it passes WASP filters.\n\n---\n\n## 12. Multimapping Chimeras\n\n- **chimMultimapNmax > 0**: detects chimeric alignments that also multimap.\n- Reported in `Chimeric.out.junction`.\n\n---\n\n## 13. STARsolo (Single-Cell RNA-seq)\n\n- **--soloType Droplet**: for 10X/Drop-seq.  \n- **--soloCBwhitelist**: file with valid barcodes.  \n- Must specify cell barcode/UMI positions (e.g. `--soloCBstart`, `--soloUMIstart`).  \n- Outputs barcodes.tsv, genes.tsv, matrix.mtx, etc. in a CellRanger-compatible format.\n\n---\n\n## 14. Description of All Options\n\nSTAR has extensive parameters grouped by function:\n1. **Parameter Files**: load config from `--parametersFiles`.\n2. **System**: shell settings.\n3. **Run Parameters**: `--runMode`, `--runThreadN`, etc.\n4. **Genome Parameters**: `--genomeDir`, `--genomeLoad`, etc.\n5. **Genome Indexing**: `--genomeChrBinNbits`, etc.\n6. **Splice Junctions Database**: `--sjdbGTFfile`, `--sjdbOverhang`.\n7. **Variation**: `--varVCFfile`.\n8. **Input Files**: `--readFilesIn`, etc.\n9. **Read Parameters**: e.g., clipping, strand.\n10. **Limits**: memory, buffers.\n11. **General Output**: `--outFileNamePrefix`, etc.\n12. **SAM/BAM Output**: `--outSAMtype`, `--outSAMattributes`.\n13. **BAM Processing**: e.g. `--bamRemoveDuplicatesType`.\n14. **Output Wiggle**: bedGraph/wiggle tracks.\n15. **Filtering**: `--outFilterType`, `--outFilterMultimapNmax`.\n16. **Splice-Junction Filtering**: `--outSJfilter*`.\n17. **Scoring**: gap penalties.\n18. **Alignments/Seeding**: `--alignIntronMin`, etc.\n19. **Paired-End**: overlap merges.\n20. **Windows/Anchors**: binning parameters.\n21. **Chimeric**: `--chimOutType`, etc.\n22. **Quantification**: `--quantMode`, `--quantTranscriptomeBan`.\n23. **2-pass**: `--twopassMode`.\n24. **WASP**: `--waspOutputMode`.\n25. **STARsolo**: single-cell.\n\nDefaults generally suffice, but advanced users can customize alignment scoring, filtering, shared-memory usage, chimeric detection, etc.  \n\n---\n\n### Summary\n\n1. **Generate genome indexes**: runMode = genomeGenerate with FASTA + (optional) GTF.\n2. **Map reads**: supply genomeDir, read files, possibly specifying on-the-fly annotations.\n3. **Outputs**: logs, alignments (SAM/BAM), splice junctions (SJ.out.tab), chimeric reads, etc.\n4. **Chimeric/fusions**: `--chimSegmentMin` > 0.\n5. **Transcriptome**: `--quantMode TranscriptomeSAM` for transcriptome-based BAM.\n6. **Gene counts**: `--quantMode GeneCounts`.\n7. **2-pass mapping**: improved novel splice detection.\n8. **Overlapping PE**: merges if mates overlap.\n9. **Variants/WASP**: optional to tag personal SNPs or filter allele-specific reads.\n10. **STARsolo**: droplet-based single-cell analysis.\n\nThis streamlined reference should help you efficiently configure and run STAR for standard RNA-seq, single-cell, fusion detection, or other specialized tasks.\n",
        "metadata": {
            "source": "STAR",
            "page": 23
        }
    },
    {
       "content": "Instructions for using the STAR-fusion:## STAR-Fusion Usage Guide\n\nThis document consolidates notes on running **STAR-Fusion** for fusion transcript detection, either starting directly from FASTQ files or using an existing STAR alignment (Kickstart mode). It also covers important parameters, output format, and integration with **FusionInspector**.\n\n---\n\n### 1. Running STAR-Fusion\n\nSTAR-Fusion can be run in **two primary ways**:\n\n1. **Starting from FASTQ files (typical)**\n   \n   **Paired-end FASTQ**:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     --left_fq reads_1.fq \\\n     --right_fq reads_2.fq \\\n     --output_dir star_fusion_outdir\n   ```\n   **Single-end FASTQ**:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     --left_fq reads_1.fq \\\n     --output_dir star_fusion_outdir\n   ```\n   Here, `CTAT_resource_lib` is the path to the STAR-Fusion-compatible reference. STAR-Fusion internally runs the STAR aligner with recommended fusion-related parameters.\n\n2. **Using an existing STAR alignment (Kickstart mode)**\n   \n   If you have already aligned reads with STAR and have a `Chimeric.out.junction` file, you can feed that directly to STAR-Fusion:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     -J Chimeric.out.junction \\\n     --output_dir star_fusion_outdir\n   ```\n   This only works if you ran STAR with certain parameters (listed below) that enable chimeric read detection and generate the properly formatted `Chimeric.out.junction` file.\n\n---\n\n### 2. Running STAR for Fusion Detection\n\nIf you prefer to run STAR separately, be sure to include **key parameters** required by STAR-Fusion:\n\n```bash\nSTAR --genomeDir /path/to/star_index \\\n     --readFilesIn reads_1.fq reads_2.fq \\\n     --outReadsUnmapped None \\\n     --twopassMode Basic \\\n     --readFilesCommand \"gunzip -c\" \\  # or zcat if gzipped\n     --outSAMstrandField intronMotif \\\n     --outSAMunmapped Within \\\n     --chimSegmentMin 12 \\  # *** essential for chimeric read detection ***\n     --chimJunctionOverhangMin 8 \\\n     --chimOutJunctionFormat 1 \\  # *** required for STAR-Fusion ***\n     --alignSJDBoverhangMin 10 \\\n     --alignMatesGapMax 100000 \\\n     --alignIntronMax 100000 \\\n     --alignSJstitchMismatchNmax 5 -1 5 5 \\\n     --outSAMattrRGline ID:GRPundef \\\n     --chimMultimapScoreRange 3 \\\n     --chimScoreJunctionNonGTAG -4 \\\n     --chimMultimapNmax 20 \\\n     --chimNonchimScoreDropMin 10 \\\n     --peOverlapNbasesMin 12 \\\n     --peOverlapMMp 0.1 \\\n     --alignInsertionFlush Right \\\n     --alignSplicedMateMapLminOverLmate 0 \\\n     --alignSplicedMateMapLmin 30\n```\n\nThis creates `Chimeric.out.junction`, which STAR-Fusion can use via `-J Chimeric.out.junction`. Ensure `--chimOutJunctionFormat 1` is set so that STAR-Fusion can estimate FFPM values.\n\n---\n\n### 3. STAR-Fusion Outputs\n\nAfter successful completion, STAR-Fusion writes results to the `star_fusion_outdir` (or chosen directory). The main outputs are:\n\n1. **star-fusion.fusion_predictions.tsv**: A tab-delimited file listing each detected fusion event with supporting data. An abridged version (`.abridged.tsv`) omits explicit read identities.\n2. **Columns of interest**:\n   - **FusionName**: e.g. `GENE1--GENE2`.\n   - **JunctionReadCount**: number of reads spanning the junction (split reads).\n   - **SpanningFragCount**: number of read pairs where each mate maps to different genes.\n   - **LargeAnchorSupport**: `YES_LDAS` if the split reads have ≥25 bases of unique alignment on both sides of the breakpoint.\n   - **FFPM**: Fusion Fragments Per Million total reads (normalizes for library depth).\n   - **SpliceType**: shows if breakpoints occur at known exon junctions.\n   - **LeftGene/RightGene** and corresponding breakpoints.\n   - **LeftBreakEntropy/RightBreakEntropy**: Shannon entropy near the breakpoint (range 0–2, with low entropy often less reliable).\n   - **annots**: additional annotations, e.g. known fusions in cancer datasets, or whether the event is intrachromosomal or interchromosomal.\n\n---\n\n### 4. FusionInspector (In Silico Validation)\n\n**FusionInspector** is included in STAR-Fusion for validating or refining predicted fusions:\n\n- **--FusionInspector inspect**: re-map only the fusion-evidence reads to custom \"fusion contigs,\" enabling quick inspection.\n- **--FusionInspector validate**: re-map **all** reads to both the reference genome and the constructed fusion contigs, re-scoring fusions.\n- **--denovo_reconstruct**: attempts to assemble fusion transcripts de novo using Trinity.\n- **--examine_coding_effect**: determines coding in-frame/frameshift status and domain structure.\n\nExample:\n```bash\nSTAR-Fusion \\\n  --left_fq reads_1.fq.gz \\\n  --right_fq reads_2.fq.gz \\\n  --genome_lib_dir /path/to/CTAT_genome_lib \\\n  --FusionInspector validate \\\n  --denovo_reconstruct \\\n  --examine_coding_effect\n```\n\nThis produces a `FusionInspector-validate/` subdirectory with refined predictions (`finspector.fusion_predictions.final`) and optional de novo–assembled contigs (`finspector.gmap_trinity_GG.fusions.fasta`).\n\n---\n\n### 5. Additional Details\n\n- **Memory requirements**: Running STAR-Fusion from FASTQ typically needs ~40GB of RAM (STAR alignment). The post-processing steps themselves need far less.\n- **CTAT resource library**: Must contain the genome plus specialized annotation files for known fusions, repeats, etc.\n- **Filtering**: Low-support fusions can be noise; adjusting parameters like `--min_FFPM` can remove artifactual events.\n- **Visualization**: Output files (contig BAM, bed files) can be loaded into IGV or used via an HTML-based viewer (e.g., igv-reports) for analyzing supporting read evidence.\n\n---\n\n### 6. Summary of Recommended Steps\n\n1. **Build or download** a CTAT resource library (contains genome + fusion metadata).\n2. **Run STAR-Fusion**\n   - From **FASTQ**: specify `--left_fq [--right_fq]`.\n   - Using **existing STAR alignment**: ensure recommended chimeric parameters, then feed `Chimeric.out.junction` via `-J`.\n3. **Review outputs** in `star_fusion_outdir`, focusing on:\n   - `star-fusion.fusion_predictions.tsv`/`.abridged.tsv`\n   - Optional `FusionInspector-validate/` for more in-depth data.\n4. **Use additional flags** like `--examine_coding_effect` or `--denovo_reconstruct` to deepen analysis.\n\nWith these steps and parameters, you can accurately detect and annotate fusion transcripts. FusionInspector can further validate or assemble the fusions for additional confidence, and outputs are easily viewed in IGV or other genomic viewers.\n",
        "metadata": {
            "source": "STAR-Fusion ",
            "page": 24
        }
    },
    {
        "content": "GO/KEGG enrichment analysis First, install the required packages.\n\n#source(\"https://bioconductor.org/biocLite.R\")\n#biocLite(\"DOSE\")\n#biocLite(\"topGO\")\n#biocLite(\"clusterProfiler\")\n#biocLite(\"pathview\")\n\nLoad the packages:\n\nlibrary(DOSE)\nlibrary(org.Hs.eg.db)\nlibrary(topGO)\nlibrary(clusterProfiler)\nlibrary(pathview)\n\nImport the data. The raw data is a gene list with one column (with a header):\n\ndata <- read.table(\"gene.list\", header=TRUE)\ndata$GeneName <- as.character(data$GeneName)\n\nConvert gene names using org.Hs.eg.db:\n\ntransID = bitr(data$GeneName,\n  fromType = \"SYMBOL\",\n  toType = c(\"ENSEMBL\", \"ENTREZID\"),\n  OrgDb = \"org.Hs.eg.db\"\n)\n\nCreate folders to store the results:\n\ndir.create(\"GO\")\ndir.create(\"KEGG\")\n\nGO_CC annotation:\n\nCC <- enrichGO(transID$ENTREZID,\n  \"org.Hs.eg.db\",\n  keyType = \"ENTREZID\",\n  ont = \"CC\",\n  pvalueCutoff = 0.05,\n  pAdjustMethod = \"BH\",\n  qvalueCutoff = 0.1\n)\nCC <- setReadable(CC, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_CC_Dotplot.svg\", bg = \"transparent\")\ndotplot(CC, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_CC\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_CC_Barplot.svg\", bg = \"transparent\")\nbarplot(CC, showCategory = 12, title = \"GO_CC\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_CC_Network.svg\", bg = \"transparent\")\nplotGOgraph(CC)\ndev.off()\n\nwrite.table(as.data.frame(CC@result), file = \"./GO/GO_CC.xls\", sep = \"\\t\", row.names = F)\n\nGO_MF annotation:\n\nMF <- enrichGO(transID$ENTREZID, \"org.Hs.eg.db\", keyType = \"ENTREZID\", ont = \"MF\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nMF <- setReadable(MF, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_MF_Dotplot.svg\", bg = \"transparent\")\ndotplot(MF, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_MF\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_MF_Barplot.svg\", bg = \"transparent\")\nbarplot(MF, showCategory = 12, title = \"GO_MF\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_MF_Network.svg\", bg = \"transparent\")\nplotGOgraph(MF)\ndev.off()\n\nwrite.table(as.data.frame(MF@result), file = \"./GO/GO_MF.xls\", sep = \"\\t\", row.names = F)\n\nGO_BP annotation:\n\nBP <- enrichGO(transID$ENTREZID, \"org.Hs.eg.db\", keyType = \"ENTREZID\", ont = \"BP\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nBP <- setReadable(BP, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_BP_Dotplot.svg\", bg = \"transparent\")\ndotplot(BP, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_BP\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_BP_Barplot.svg\", bg = \"transparent\")\nbarplot(BP, showCategory = 12, title = \"GO_BP\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_BP_Network.svg\", bg = \"transparent\")\nplotGOgraph(BP)\ndev.off()\n\nwrite.table(as.data.frame(BP@result), file = \"./GO/GO_BP.xls\", sep = \"\\t\", row.names = F)\n\nKEGG annotation:\n\nkegg <- enrichKEGG(transID$ENTREZID, organism = \"hsa\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nkegg <- setReadable(kegg, OrgDb = org.Hs.eg.db, keytype = \"ENTREZID\")\n\nsvg(file = \"./KEGG/KEGG_Dotplot.svg\", bg = \"transparent\")\ndotplot(kegg, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"KEGG\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./KEGG/KEGG_Barplot.svg\", bg = \"transparent\")\nbarplot(kegg, showCategory = 12, title = \"KEGG\", font.size = 8)\ndev.off()\n\nwrite.table(as.data.frame(kegg@result), file = \"./KEGG/kegg.xls\", sep = \"\\t\", row.names = F)\n\nCreate a directory for KEGG pathway maps:\n\ndir.create(\"./KEGG/MAP\")\nkegg_df = as.data.frame(kegg)\n\nfor(i in kegg_df$ID){\n  pathview(gene.data = transID$ENTREZID,\n           pathway.id = i,\n           species = \"hsa\",\n           kegg.native = TRUE,\n           kegg.dir = \"./KEGG/MAP\"\n  )\n}\n\nprint(\"TASK DONE\")",
         "metadata": {
             "source": "GO/KEGG enrichment analysis ",
             "page": 25
         }
     },
     {
        "content": "CHIP-seq Functional enrichment analysis\nOnce we have obtained the annotated nearest genes, we can perform functional enrichment analysis to identify predominant biological themes among these genes by incorporating biological knowledge provided by biological ontologies. For instance, Gene Ontology (GO) (Ashburner et al. 2000) annotates genes to biological processes, molecular functions, and cellular components in a directed acyclic graph structure; Kyoto Encyclopedia of Genes and Genomes (KEGG) (Kanehisa et al. 2004) annotates genes to pathways; Disease Ontology (DO) (Schriml et al. 2011) annotates genes with human disease association; and Reactome (Croft et al. 2013) annotates genes to pathways and reactions.\n\nChIPseeker also provides a function, seq2gene, for linking genomic regions to genes in a many-to-many mapping. It considers the host gene (exon/intron), promoter region, and flanking gene from intergenic regions that may be under cis-regulation. This function is designed to link both coding and non-coding genomic regions to coding genes and facilitate functional analysis.\n\nEnrichment analysis is a widely used approach to identify biological themes. Several Bioconductor packages have been developed to investigate whether the number of selected genes associated with a particular biological term is larger than expected. These include DOSE (Yu et al. 2015) for Disease Ontology, ReactomePA for Reactome pathway analysis, and clusterProfiler (Yu et al. 2012) for Gene Ontology and KEGG enrichment analysis.\n\nlibrary(ReactomePA)\n\npathway1 <- enrichPathway(as.data.frame(peakAnno)$geneId)\nhead(pathway1, 2)\n##                          ID        Description GeneRatio   BgRatio RichFactor\n## R-HSA-9830369 R-HSA-9830369 Kidney development    19/499  46/11146  0.4130435\n## R-HSA-9758941 R-HSA-9758941       Gastrulation    27/499 125/11146  0.2160000\n##               FoldEnrichment    zScore       pvalue     p.adjust       qvalue\n## R-HSA-9830369       9.226017 12.102741 2.252177e-14 2.360281e-11 2.351747e-11\n## R-HSA-9758941       4.824721  9.309385 5.773075e-12 3.025091e-09 3.014153e-09\n##                                                                                                                                                   geneID\n## R-HSA-9830369                                          2625/5076/7490/652/6495/2303/3975/6928/10736/5455/7849/3237/6092/2122/255743/2296/3400/28514/2138\n## R-HSA-9758941 5453/5692/5076/5080/7546/3169/652/5015/2303/5717/3975/2627/5714/344022/7849/5077/2637/7022/8320/7545/6657/4487/51176/2296/28514/2626/64321\n##               Count\n## R-HSA-9830369    19\n## R-HSA-9758941    27\ngene <- seq2gene(peak, tssRegion = c(-1000, 1000), flankDistance = 3000, TxDb = txdb)\npathway2 <- enrichPathway(gene)\nhead(pathway2, 2)\n##                          ID        Description GeneRatio   BgRatio RichFactor\n## R-HSA-9830369 R-HSA-9830369 Kidney development    17/408  46/11146  0.3695652\n## R-HSA-9758941 R-HSA-9758941       Gastrulation    24/408 125/11146  0.1920000\n##               FoldEnrichment    zScore       pvalue     p.adjust       qvalue\n## R-HSA-9830369      10.096014 12.049721 1.806589e-13 1.765037e-10 1.709604e-10\n## R-HSA-9758941       5.245176  9.303549 1.834131e-11 8.959732e-09 8.678338e-09\n##                                                                                                                                    geneID\n## R-HSA-9830369                                      2625/5076/3227/652/6495/2303/3975/3237/6092/2122/255743/7490/6928/7849/5455/2296/28514\n## R-HSA-9758941 5453/5692/5076/7546/3169/652/2303/5717/3975/2627/5714/344022/2637/8320/7545/7020/2626/5080/5015/7849/51176/2296/64321/28514\n##               Count\n## R-HSA-9830369    17\n## R-HSA-9758941    24\ndotplot(pathway2)",
         "metadata": {
             "source": "CHIP-seq Functional enrichment ",
             "page": 26
         }
     },
     {
         "content": "# DESeq2 Differential Expression Analysis Tutorial\n\nDESeq2 is an R package designed for normalization, visualization, and differential expression analysis of high-dimensional count data. It uses empirical Bayes techniques to estimate log₂ fold changes and dispersion priors, and then calculates their posterior values. Originally released in 2014 by Professor Michael Love from the University of North Carolina, DESeq2 continues to be updated and is one of the most widely used tools for differential expression analysis.\n\n---\n\n## 1. Installation\n\n### In R\n\nRun the following commands in the R console or RStudio (for R version 4.2 and above):\n\n```r\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DESeq2\")\n```\n\n### In a conda Environment\n\nUse the following command:\n\n```bash\nconda install -c bioconda bioconductor-deseq2\n```\n\n---\n\n## 2. Workflow Overview\n\nThis tutorial covers the following steps:\n\n1. **Data Import and Preprocessing**: \n   - Load the count matrix (e.g., `gene_count_matrix.csv`).\n   - Perform data cleaning, such as fixing gene names with extra information and removing invalid rows.\n   - Filter out genes with low expression.\n\n2. **Differential Expression Analysis**: \n   - Create group and sample information.\n   - Build a `DESeqDataSet` object from the count matrix.\n   - Perform normalization and statistical testing.\n   - Extract and sort results by p-value and log₂ fold change, and filter significant genes.\n\n3. **Visualization**: \n   - Display expression for a single gene.\n   - Generate a heatmap of differentially expressed genes.\n   - Create a volcano plot to visualize overall differential expression.\n\n---\n\n## 3. Data Import and Preprocessing\n\nFirst, load the required R packages and read the expression count matrix. Assume the file `gene_count_matrix.csv` contains gene IDs as row names.\n\n```r\n# Load necessary libraries\nlibrary(DESeq2)\nlibrary(pheatmap)  # For heatmaps\nlibrary(ggplot2)   # For plotting\n\n# Read the count matrix (set your working directory accordingly)\ncountData <- as.matrix(read.csv(\"gene_count_matrix.csv\", row.names = \"gene_id\"))\n```\n\n**Note:**\n\n- The gene names might include extra information, and there could be invalid rows. Use data cleaning functions in R as needed.\n- Since many genes might show no expression (or zero counts), you can filter out these low-expression genes:\n\n```r\ncountData <- countData[rowMeans(countData) > 1, ]  # Remove genes with very low expression\n```\n\n---\n\n## 4. Differential Expression Analysis\n\n### 4.1 Preparing Group Information\n\nAssume you have three control samples and three osmotic-treated samples. Create a factor vector for the condition and a sample information data frame (`colData`) with row names matching the columns in `countData`.\n\n```r\ncondition <- factor(c(rep(\"control\", 3), rep(\"osmotic\", 3)))\ncolData <- data.frame(row.names = colnames(countData), condition)\n```\n\n### 4.2 Creating the DESeqDataSet Object\n\nUse the `DESeqDataSetFromMatrix` function to create the object:\n\n```r\ndds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ condition)\nhead(dds)  # View the structure of the DESeqDataSet\n```\n\n### 4.3 Normalization and Statistical Testing\n\nNormalize the data and perform differential expression testing:\n\n```r\ndds1 <- DESeq(dds, fitType = 'mean', minReplicatesForReplace = 7, parallel = FALSE)\nres <- results(dds1)\nsummary(res)\n```\n\nThe output will display the number of upregulated and downregulated genes, along with counts for outliers and low-count genes.\n\n### 4.4 Organizing and Filtering Results\n\nConvert the results to a data frame, sort by p-value and log₂ fold change, and filter for significant genes (e.g., p-value < 0.05 and |log₂ fold change| ≥ 1):\n\n```r\nres1 <- data.frame(res, stringsAsFactors = FALSE, check.names = FALSE)\nres1 <- res1[order(res1$pvalue, res1$log2FoldChange, decreasing = c(FALSE, TRUE)), ]\n\n# Filter for upregulated and downregulated genes\nres1_up <- res1[which(res1$log2FoldChange >= 1 & res1$pvalue < 0.05), ]\nres1_down <- res1[which(res1$log2FoldChange <= -1 & res1$pvalue < 0.05), ]\nres1_total <- rbind(res1_up, res1_down)\n```\n\n---\n\n## 5. Visualization\n\n### 5.1 Displaying a Single Gene's Expression\n\nYou can quickly visualize the expression of a specific gene using DESeq2's `plotCounts` function:\n\n```r\nplotCounts(dds1, gene = \"AT4G38770\", intgroup = \"condition\")\n```\n\nAlternatively, customize the plot using ggplot2:\n\n```r\nd <- data.frame(t(subset(countData, rownames(countData) == \"AT4G38770\")))\nggplot(d, aes(x = condition, y = AT4G38770, color = condition)) +\n  geom_point(position = position_jitter(w = 0.2, h = 0)) +\n  geom_text_repel(aes(label = rownames(d))) +\n  theme_bw() +\n  ggtitle(\"AT4G38770\") +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n\n### 5.2 Creating a Heatmap\n\nUse the pheatmap package to cluster and display the expression of differentially expressed genes:\n\n```r\n# Extract the differential genes from the count matrix\n df <- countData[intersect(rownames(countData), rownames(res1_total)), ]\n df2 <- as.matrix(df)\n pheatmap(df2,\n          show_rownames = FALSE,\n          show_colnames = TRUE,\n          cluster_cols = FALSE,\n          cluster_rows = TRUE,\n          height = 10,\n          scale = \"row\",\n          fontsize = 10,\n          angle_col = 45, \n          color = colorRampPalette(c(\"#8854d0\", \"#ffffff\", \"#fa8231\"))(100),\n          clustering_method = 'single'\n )\n```\n\n### 5.3 Drawing a Volcano Plot\n\nUse ggplot2 to create a volcano plot that displays the log₂ fold changes against -log₁₀ (adjusted p-values):\n\n```r\ngenes <- res1\n# Assign colors based on upregulation, downregulation, or no significant change\n genes$color <- ifelse(genes$padj < 0.05 & abs(genes$log2FoldChange) >= 1,\n                        ifelse(genes$log2FoldChange > 1, 'red', 'blue'),\n                        'gray')\n color <- c(red = \"red\", gray = \"gray\", blue = \"blue\")\n\n p <- ggplot(genes, aes(log2FoldChange, -log10(padj), col = color)) +\n   geom_point() +\n   theme_bw() +\n   scale_color_manual(values = color) +\n   labs(x = \"log2 (fold change)\", y = \"-log10 (q-value)\") +\n   geom_hline(yintercept = -log10(0.05), lty = 4, col = \"grey\", lwd = 0.6) +\n   geom_vline(xintercept = c(-1, 1), lty = 4, col = \"grey\", lwd = 0.6) +\n   theme(legend.position = \"none\",\n         panel.grid = element_blank(),\n         axis.title = element_text(size = 16),\n         axis.text = element_text(size = 14)) +\n   geom_text_repel(\n     data = subset(genes, padj < 1e-100 & abs(genes$log2FoldChange) >= 10),\n     aes(label = rownames(genes)),\n     size = 5,\n     box.padding = unit(0.35, \"lines\"),\n     point.padding = unit(0.3, \"lines\")\n   )\n p\n```\n\n---\n\n## 6. A Complete DESeq2 Example in RStudio\n\nBelow is a complete example code that can be executed in RStudio:\n\n```r\n# Clear the workspace and set the working directory\nrm(list = ls())\nsetwd('/XX/XX/XX')\nif (!dir.exists('./01_DEGs')) {\n  dir.create('./01_DEGs')\n}\nsetwd('./01_DEGs/')\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(DESeq2)\n\n# Import the processed expression matrix (row names are gene symbols, columns are sample IDs)\ndat <- read.csv('../../00_rawdata/GSE203346/dat.GSE203346_count.csv', check.names = FALSE, row.names = 1)\n\n# Import the group information\ncolData <- read.csv('../../00_rawdata/GSE203346/group.GSE203346.csv')\ncolData$group <- factor(colData$group, levels = c(\"control\", \"disease\"))\nprint(table(colData$group))\n# Ensure the samples in the expression matrix match the group information\n dat <- dat[, colData$sample]\n\n# Create the DESeqDataSet object\ndds <- DESeqDataSetFromMatrix(countData = dat, colData = colData, design = ~ group)\n# Filter out genes with low counts (genes with counts ≤ 1 in all samples)\ndds <- dds[rownames(counts(dds)) > 1, ]\n# Estimate size factors to normalize for sequencing depth\ndds <- estimateSizeFactors(dds)\n\n# Perform differential expression analysis\ndds <- DESeq(dds)\nres <- results(dds, contrast = c(\"group\", \"disease\", \"control\"))\nDEG <- as.data.frame(res)\n\n# Add labels for differential expression (criteria: |log2FoldChange| ≥ 0.5 & p-value < 0.05)\nDEG$change <- as.factor(\n  ifelse(DEG$pvalue < 0.05 & abs(DEG$log2FoldChange) >= 0.5,\n         ifelse(DEG$log2FoldChange > 0.5, 'Up', 'Down'), 'Not')\n)\nprint(table(DEG$change))\nDEG_write <- cbind(symbol = rownames(DEG), DEG)\n\n# Filter for significantly differentially expressed genes\nsig_diff <- subset(DEG, DEG$pvalue < 0.05 & abs(DEG$log2FoldChange) >= 0.5)\nsig_diff_write <- cbind(symbol = rownames(sig_diff), sig_diff)\n\n# Save the results\nwrite.csv(DEG_write, file = 'DEG_all.csv')\nwrite.csv(sig_diff_write, file = 'DEG_sig.csv')\n```\n\n**Explanation:**\n\n- **Data Import:** The expression matrix and group information are imported, ensuring that the sample orders match.\n- **DESeqDataSet Creation:** The count data, sample information, and experimental design (e.g., control vs. disease) are used to create a DESeqDataSet object.\n- **Filtering & Normalization:** Lowly expressed genes are removed, and size factors are estimated to normalize the data.\n- **Differential Expression:** The `DESeq` function performs the analysis, and results are extracted and organized.\n- **Annotation & Saving:** Differential expression labels (Up, Down, Not) are added based on the criteria, and results are saved to CSV files for further analysis or visualization.\n\n---\n\n## 7. Summary\n\nThis tutorial provides a detailed guide for performing RNA-seq differential expression analysis using DESeq2. It covers:\n\n- Data import and preprocessing to prepare count data for analysis.\n- Normalization and statistical testing to identify differentially expressed genes.\n- Visualization techniques including single-gene expression plots, heatmaps, and volcano plots.\n\nBy following these steps, researchers can construct a robust differential expression analysis pipeline and tailor parameters to suit their experimental design, ultimately drawing reliable biological conclusions.",
         "metadata": {
             "source": "DESeq2",
             "page": 27
         }
     },
     {
        "content": "# SortMeRNA Usage Documentation\n\nSortMeRNA is a tool designed for detecting and filtering out rRNA reads from sequencing libraries. It supports both single-end and paired-end data, and automatically recognizes file formats and compression (e.g. plain fasta/fastq or gzipped files).\n\n---\n\n## 1. Installation\n\n### Using conda\n\nInstall SortMeRNA with the following command:\n\n```bash\nconda install sortmerna\n```\n\nVerify the installation by checking the version:\n\n```bash\n$ sortmerna --version\nSortMeRNA version 4.3.2\nBuild Date: Apr  2 2021\n```\n\n---\n\n## 2. Basic Usage\n\nThe only required options are `--ref` and `--reads`. Options can be specified using a single dash (e.g. `-ref` and `-reads`). File extensions are optional because the format and compression are automatically recognized.\n\n### Example Commands\n\n**Single Reference and Single Reads File**:\n\n```bash\nsortmerna --ref REF_PATH --reads READS_PATH\n```\n\n**Multiple References**:\n\n```bash\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH\n```\n\n**Paired-End Reads**:\n\n```bash\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH_1 --reads READS_PATH_2\n```\n\n---\n\n## 3. Example: Identifying Bacterial 16S rRNA\n\nThe following command identifies rRNA in the file `DRR110568_1.fastq` using the bacterial 16S rRNA database. rRNA-matched sequences are saved to `DRR110568.1.16s` while non-matched sequences are saved to `DRR110568.1.non.16s`.\n\n```bash\n./sortmerna \\\n  --ref rRNA_databases/silva-bac-16s-id90.fasta,/index/silva-bac-16s-db: \\\n  --reads DRR110568_1.fastq \\\n  --aligned DRR110568.1.16s \\    # rRNA-matched sequences\n  --sam --num_alignments 1 --fastx \\\n  --other DRR110568.1.non.16s \\   # non-rRNA sequences\n  --log -v\n```\n\n---\n\n## 4. Indexing rRNA Databases\n\nTo avoid re-indexing at every run, pre-build the index for your rRNA databases. Once the index is built, copy the generated `idx/` directory to your working directory.\n\n### Indexing Command Example:\n\n```bash\nsortmerna --index 1 --threads 32 \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-16s-id90.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-16s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-18s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-28s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5s-database-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5.8s-database-id98.fasta \\\n  --workdir ./\n```\n\nAfter indexing, the working directory will include the following folders:\n\n```\nWORKDIR/\n  idx/    (index files)\n  kvdb/   (key-value alignment results)\n  out/    (output files)\n  readb/  (preprocessed reads index)\n```\n\n> **Note:** Ensure the `kvdb/` directory is empty before each run to avoid errors.\n\n---\n\n## 5. Removing rRNA Reads from Paired-End Data\n\nThe following command removes rRNA reads from paired-end sequencing data. The output files will retain the input format (e.g. `.fq.gz`).\n\n```bash\nsortmerna --index 0 --threads 48 \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-16s-id90.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-16s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-18s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-28s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5s-database-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5.8s-database-id98.fasta \\\n  --workdir ./ \\\n  --reads NR1.fq.gz \\\n  --reads NR2.fq.gz \\\n  --aligned \"NR/N_rRNA\" --other \"NR/N_clean\" --paired_in --fastx --out2\n```\n\n### Expected Output\n\nAfter running, the output directory (e.g. `NR/`) should contain files such as:\n\n```\nN_clean_fwd.fq.gz  N_clean_rev.fq.gz\nN_rRNA_fwd.fq.gz   N_rRNA_rev.fq.gz\nN_rRNA.log\n```\n\nThe log file provides statistics on the total reads classified as rRNA versus non-rRNA, along with coverage by each reference database.\n\n---\n\n## 6. Key Parameters and Options\n\n- **--ref**: Specify the reference fasta file. Use multiple times for multiple databases.\n- **--reads**: Input reads file. For paired-end data, provide this option twice.\n- **--aligned**: Output for reads that match the rRNA database. Can include a path and prefix for the output file.\n- **--other**: Output for reads that do not match the rRNA database.\n- **--index**: Controls indexing behavior:\n  - `--index 1`: Only build the index.\n  - `--index 0`: Skip indexing (requires an existing index in the `idx/` directory).\n  - Default (or `--index 2`): Automatically detect or build the index if not found.\n- **--sam**: Output alignments in SAM format.\n- **--fastx**: Output in fasta/fastq format, matching the input format.\n- **--num_alignments INT**: Specifies the number of alignments to output. Use `1` for faster filtering; higher values increase runtime.\n- **--paired_in / --paired_out**: Control output handling for paired-end reads.\n  - `--paired_in`: Outputs non-rRNA reads to the `other` file (although a few rRNA reads might be included).\n  - `--paired_out`: Ensures rRNA reads appear only in the aligned file (with a chance of non-rRNA reads also appearing in the other file).\n\n---\n\n## 7. Advanced Options\n\nSortMeRNA includes additional parameters for fine-tuning alignment sensitivity and speed:\n\n- **--num_seeds INT**: Minimum number of seed matches required (default: 2).\n- **passes INT,INT,INT**: Adjusts seed search passes using decreasing intervals (default: L, L/2, L/3).\n- **--edges INT(%)**: Extends the alignment region on the reference by a set number of nucleotides (default: 4).\n- **--full_search**: Disables early termination upon finding 0-error seed matches, which can increase sensitivity but decrease speed up to four-fold.\n- **--pid**: Appends the process ID to output filenames to avoid overwriting files from concurrent runs.\n\n---\n\n## 8. Summary\n\nSortMeRNA is a powerful tool for rRNA detection and removal from sequencing libraries. It offers:\n\n- Automatic recognition of input file formats and compression.\n- Support for both single-end and paired-end data.\n- Flexible parameter settings for balancing speed and sensitivity.\n- Compatibility with standard rRNA databases such as SILVA and Rfam.\n\nWhile SortMeRNA is highly sensitive, for quick rRNA estimation some users might prefer aligners like STAR or Bowtie2. However, for comprehensive rRNA filtering, SortMeRNA remains an effective solution.\n",
        "metadata": {
            "source": "SortMeRNA",
            "page": 28
        }
    }
    
]
