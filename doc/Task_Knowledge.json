[
    {
        "content": "2. run-sort-bam.sh:\nData-type-independent, generic bam sorting module\nInput : any unsorted bam file (.bam)\nOutput : a bam file sorted by coordinate (.sorted.bam) and its index (.sorted.bam.bai).\nUsage\nRun the following in the container.\nrun-sort-bam.sh <input_bam> <output_prefix>\n# input_bam : any bam file to be sorted\n# output_prefix : prefix of the output bam file.\n\nSet parameters according to the example: Suppose the input file is: ./output/GM12878_bwa_1.bam and ./output/GM12878_bwa_2.bam, the target is./output/GM12878_bwa_sorted.bam and ./output/GM12878_bwa_sorted, Generate the following sample script:\nbash ./scripts/run-sort-bam.sh ./output/GM12878_bwa_1.bam ./output/GM12878_bwa_sorted \n\nbash ./scripts/run-sort-bam.sh ./output/GM12878_bwa_2.bam ./output/GM12878_bwa_sorted\n\nYou can install the tool, but do not do any additional operations.You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-sort-bam.sh",
            "page": 6
        }
    },
    {
        "content": "3. run-bam2pairs.sh:\nBam to pairs conversion module for Hi-C data, based on samtools, bgzip and pairix.\nInput : any paired-end bam file\nOutput : a chromosome-block-sorted and bgzipped pairs pairs file that contains all the mapped read pairs in the bam file, along with its index (.bsorted.pairs.gz and .bsorted.pairs.gz.px2)\nUsage\nRun the following in the container.\nrun-bam2pairs.sh <input_bam> <output_prefix>\n# input_bam : input bam file.\n# output_prefix : prefix of the output pairs file.\nSet parameters according to the example:\nSuppose the input file is: ./output/GM12878_bwa_sorted_1.bam and ./output/GM12878_bwa_sorted_2.bam, the target is./output/GM12878_bwa_sorted_pairs  and./output/GM12878_bwa_sorted_pairs , Generate the following sample script:\nbash ./scripts/run-bam2pairs.sh ./output/GM12878_bwa_sorted_1.bam ./output/GM12878_bwa_sorted_pairs   \nbash ./scripts/run-bam2pairs.sh ./output/GM12878_bwa_sorted_2.bam ./output/GM12878_bwa_sorted_pairs \n\nYou can install the tool, but do not do any additional operations.You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-bam2pairs.sh",
            "page": 7
        }
    },
    {
        "content": "1. run-bwa-mem.sh:\nAlignment module for Hi-C data, based on bwa-mem.\nInput : a pair of Hi-C fastq files\nOutput : a bam file (Lossless, not sorted by coordinate)\nUsage\nRun the following in the container.\nrun-bwa-mem.sh <fastq1> <fastq2> <bwaIndex> <outdir> <output_prefix> <nThreads>\n# fastq1, fastq2 : input fastq files, either gzipped or not\n# bwaIndex : tarball for bwa index, .tgz.\n# outdir : output directory\n# output_prefix : prefix of the output bam file.\n# nThreads : number of threads\n\nThe script is invoked in the./script/ path as bash./script/run-bwa-mem.sh. Set parameters according to the example:I want to output under./output/003/.Reference file path for: ./data/hg38.bwaindex.tgz,no need to extract the reference file, and the first pair are ./data/4DNFI15H1RVG.fastq.gz and ./data/4DNFIZHUKESO.fastq.gz, the second pair are for: ./data/4DNFIKVDGNJN.fastq.gz and ./data/4DNFIEQ58J6G.fastq.gz,  the target is the first pair result is bwa_1.bam,the second pair result is bwa.2.bam. the generated script is:\n\nbash ./scripts/run-bwa-mem.sh ./data/4DNFI15H1RVG.fastq.gz ./data/4DNFIZHUKESO.fastq.gz  ./data/hg38.bwaindex.tgz ./output/003/  bwa_1 64\n\nbash ./scripts/run-bwa-mem.sh ./data/4DNFIKVDGNJN.fastq.gz ./data/4DNFIEQ58J6G.fastq.gz  ./data/hg38.bwaindex.tgz ./output/003/ bwa_2 64\n\nWhen the command is generated, the file name and the output path must require a space, otherwise the file name will be regarded as the folder name. bwa_1,bwa_2 indicate the generated file name, You should adjust according to the description of the desired destination file name. For example, the directive: bash ./scripts/run-bwa-mem.sh ./data/4DNFIKVDGNJN.fastq.gz ./data/4DNFIEQ58J6G.fastq.gz  ./data/hg38.bwaindex. tgz  In./output/003/ bwa_2 64, there is a space between /output/003/ and bwa_2, which must not be omitted. There is also a space between bwa_2 and 64 that cannot be omitted.and 64 indicate the number of threads. The number of threads is usually set to 16 or 32. You can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-bwa-mem.sh by hic workflow",
            "page": 8
        }
    },
    {
        "content": "2. run-pairsam-parse-sort.sh\nRuns pairsam parse and sort on a bwa-produced bam file and produces a sorted pairsam file\nInput: a bam file\nOutput: a pairsam file\nUsage\nRun the following in the container\nrun-pairsam-parse-sort.sh <input_bam> <chromsizes> <outdir> <outprefix> <nthread> <compress_program>\n# input_bam : an input bam file.\n# chromsizes : a chromsize file\n# outdir : output directory\n# outprefix : prefix of output files\n# nthread : number of threads to use\n\nSet parameters according to the example:\nSuppose the input file is: ./output/aligned/a1.bam  and ./output/aligned/abc.bam, the target is ./output/003/pairsam_fix1.sam.pairs.gz and ./output/003/pairsam_fix2.sam.pairs.gz . ./hg38.chrom.sizes  is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-pairsam-parse-sort.sh ./output/aligned/a1.bam ./hg38.chrom.sizes ./output/003/  pairsam_fix1 64 lz4c \nbash ./scripts/run-pairsam-parse-sort.sh ./output/aligned/abc.bam ./hg38.chrom.sizes ./output/003/  pairsam_fix2 64 lz4c\n\nYou can install the tool, but do not do any additional operations.and 64 indicate the number of threads.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 9
        }
    },
    {
        "content": "3. run-pairsam-merge.sh\nMerges a list of pairsam files\nInput: a list of pairsam files\nOutput: a merged pairsam file\nUsage\nRun the following in the container\nrun-pairsam-merge.sh <outprefix> <nthreads> <input_pairsam1> [<input_pairsam2> [<input_pairsam3> [...]]]\n# outprefix : prefix of output files\n# nthreads : number of threads to use   \n# input_pairsam : an input pairsam file.\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/pairsam_fix1.sam.pairs.gz  and ./output/003/pairsam_fix2.sam.pairs.gz, the target is ./output/003/out.merged.sam.pairs.gz . Generate the following sample script:\n\nbash ./scripts/run-pairsam-merge.sh ./output/003/out 64 './output/003/pairsam_fix1.sam.pairs.gz ./output/003/pairsam_fix2.sam.pairs.gz' \n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 10
        }
    },
    {
        "content": "4. run-pairsam-markasdup.sh\nTakes a pairsam file in and creates a pairsam file with duplicate reads marked\n* Input: a pairsam file\n* Output: a duplicate-marked pairsam file\nUsage\nRun the following in the container\nrun-pairsam-markasdup.sh <input_pairsam>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n\nSet parameters according to the example:\nSuppose the input file is:  ./output/003/out.merged.sam.pairs.gz,the target is ./output/003/out1.  Generate the following sample script:\n\nbash ./scripts/run-pairsam-markasdup.sh  ./output/003/out.merged.sam.pairs.gz  ./output/003/out1  \n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 11
        }
    },
    {
        "content": "5. run-pairsam-filter.sh\nTakes in a pairsam file and creates a lossless, annotated bam file and a filtered pairs file.\nInput: a pairsam file\nOutput: an annotated bam file and a filtered pairs file\nUsage\nRun the following in the container\nrun-pairsam-filter.sh <input_pairsam> <outprefix> <chromsizes>\n# input_pairsam : an input pairsam file.\n# outprefix : prefix of output files\n# chromsizes : a chromsize file\n\nSet parameters according to the example:\nSuppose the input file is: ../sample_data/outlala.merged.sam.pairs.gz, the target is  ./out/out12345.dedup.pairs.gz and ./out/out12345.lossless.bam ,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-pairsam-filter.sh  ./sample_data/outlala.merged.sam.pairs.gz ./out/out12345  /home/agent/POPGENAGENT/data/hg38.chrom.sizes\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 12
        }
    },
    {
        "content": "6. run-merge-pairs.sh\nAlignment module for Hi-C data, based on merge-pairs.\n\nInput : a set of pairs files, with their associated indices\nOutput : a merged pairs file and its index\nUsage\nRun the following in the container.\nrun-merge-pairs.sh <output_prefix> <pairs1> <pairs2> [<pairs3> [...]]  \n# output_prefix : prefix of the output pairs file.\n# pairs1, pairs2, ... : input pairs files\n\nSet parameters according to the example:\nSuppose the input file is: ./out/out12345.dedup.pairs.gz, the target is ./out/out321.pairs.gz. Generate the following sample script:\n\nbash ./scripts/run-merge-pairs.sh ./out/out321 ./out/out12345.dedup.pairs.gz\n\nYou can install the tool, but do not do any additional operations. Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 13
        }
    },
    {
        "content": "7. run-cooler.sh\nRuns cooler to create an unnormalized matrix .cool file, taking in a (4dn-style) pairs file\nInput : a pairs file (.gz, along with .px2), chrom.size file\nOutput : a contact matrix file (.cool)\nUsage\nRun the following in the container.\nrun-cooler.sh <input_pairs> `<chromsize>` `<binsize>` `<ncores>` <output_prefix> <max_split>\nrun-cooler.sh <input_pairs> <chromsize> <binsize> <ncores> <output_prefix> <max_split>\n# input_pairs : a pairs file\n# chromsize : a chromsize file\n# binsize : binsize in bp\n# ncores : number of cores to use\n# output_prefix : prefix of the output cool file\n# max_split : max_split argument for cooler (e.g. 2 which is default for cooler) \n\nSet parameters according to the example:\nSuppose the input file is:/home/agent/POPGENAGENT/sample_data/outlala.ff.pairs.gz , the target is  ./out/out3 ,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-cooler.sh      /home/agent/POPGENAGENT/sample_data/outlala.ff.pairs.gz      /home/agent/POPGENAGENT/data/hg38.chrom.sizes      1000      32      ./out/out3      2\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 32 indicate the number of threads.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 14
        }
    },
    {
        "content": "8. run-cooler-balance.sh\nRuns cooler to create a normalized matrix file, taking in an unnormalized .cool file\nInput: a cool file (.cool)\nOutput : a cool file (.cool)\nUsage\nRun the following in the container.\nrun-cooler-balance.sh <input_cool> <max_iter> <output_prefix> <chunksize>\n# input_cool : a cool file (without normalization vector)\n# max_iter : maximum number of iterations\n# output_prefix : prefix of the output cool file\n# chunksize : chunksize argument for cooler (e.g. 10000000 which is default for cooler)\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/cool_prefix.cool , the target is ./output/003/cool_prefix_normalized.cool,/home/agent/POPGENAGENT/data/hg38.chrom.sizes is the a chromsize file. Generate the following sample script:\n\nbash ./scripts/run-cooler-balance.sh ./output/003/cool_prefix.cool 1000 ./output/003/cool_prefix_normalized 10000000\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 15
        }
    },
    {
        "content": "9. run-cool2multirescool.sh\nRuns cooler coarsegrain to create multi-res cool file from a .cool file.\nInput : a cool file (.cool)\nOutput : a multires.cool file (.multires.cool)\nUsage\nRun the following in the container.\nrun-cool2multirescool.sh -i <input_cool> [-p <ncores>] [-o <output_prefix>] [-c <chunksize>] [-j] [-u custom_res] [-B]\n# input_cool : a (single-res) cool file with the highest resolution you want in the multi-res cool file\n# -p ncores: number of cores to use (default: 1)\n# -o output_prefix: prefix of the output multires.cool file (default: out)\n# -c chunksize : chunksize argument of cooler (e.g. default: 10000000)\n# -j : juicer resolutions (default: use HiGlass resolutions)\n# -u custom_res : custom resolutions separated by commas (e.g. 100000,200000,500000). The minimum of this set must match min_res (-r).\n# -B : no balancing/normalizations\n\nSet parameters according to the example:\nSuppose the input file is: ./output/003/cool_prefix_normalized.cool , the target is ./output/003/multirescool_prefix.mcool. Generate the following sample script:\n\nbash ./scripts/run-cool2multirescool.sh -i ./output/003/cool_prefix_normalized.cool -p 8 -o ./output/003/multirescool_prefix -c 10000000 -j -u 100000,200000,500000\n\nYou can install the tool, but do not do any additional operations.Please follow the example to generate rather than copy and paste completely, especially for folder names, file names, etc.and 64 indicate the number of threads.",
        "metadata": {
            "source": "run-pairsam-parse-sort.sh by hic workflow",
            "page": 16
        }
    },
    {
        "content": "## Overview\n\nBWA (Burrows-Wheeler Aligner) is a widely used sequence alignment tool designed for mapping low-divergent reads to large reference genomes (e.g., human genomes). BWA includes three main algorithms:\n\n1. **BWA-backtrack**: Optimized for short reads up to 100 bp.\n2. **BWA-SW**: Optimized for longer sequences (70 bp to 1 Mbps), supports long reads and split alignment.\n3. **BWA-MEM**: The newest algorithm in the BWA suite, recommended for high-quality reads. It is both fast and accurate, with support for read lengths from 70 bp to 1 Mbps. BWA-MEM also performs well for reads between 70 and 100 bp.\n\nAmong these, **BWA-MEM** is widely used due to its balance of speed and accuracy, making it suitable for various downstream applications, such as whole-genome variant calling. It supports long reads, split alignment, and chimeric reads, and outputs standard SAM files that are compatible with tools like samtools and GATK.\n\nBefore running BWA-MEM, you must build an FM-index of the reference genome using the `bwa index` command. Once the index is created, you can align reads to the reference genome with the `bwa mem` sub-command.\n\n---\n\n## Installation\n\n1. **Download the BWA package**: Obtain the source code from the official repository.\n2. **Compile and install**: Typically, you will use a C compiler (e.g., GCC). The official documentation provides detailed build instructions.\n3. **Finalize installation**: Make sure your environment variables or system PATH are set correctly so you can call `bwa` directly.\n\n---\n\n## Quick Start\n\n1. **Indexing the reference genome**:\n\n```bash\nbwa index reference.fa\n```\n\nThis command builds the required FM-index for the reference genome `reference.fa`.\n\n2. **Running the alignment**:\n\n```bash\nbwa mem reference.fa reads.fq > alignment.sam\n```\n\nAligns the reads in `reads.fq` to the reference `reference.fa` and writes the alignment in SAM format to `alignment.sam`.\n\n---\n\n## Examples of Popular Commands\n\nBelow are five common BWA-MEM usage examples:\n\n1. **Indexing a reference genome with a custom prefix**:\n\n```bash\nbwa index -p ref_index reference.fa\n```\n\nGenerates an index for `reference.fa` with the prefix `ref_index`.\n\n2. **Single-end read alignment**:\n\n```bash\nbwa mem ref_index reads.fq > aligned_reads.sam\n```\n\nAligns single-end reads in `reads.fq` to the reference indexed by `ref_index` and outputs the result to `aligned_reads.sam`.\n\n3. **Paired-end read alignment**:\n\n```bash\nbwa mem ref_index reads_1.fq reads_2.fq > aligned_pair.sam\n```\n\nAligns paired-end reads in `reads_1.fq` and `reads_2.fq` and saves them to `aligned_pair.sam`.\n\n4. **Generating a sorted BAM file directly**:\n\n```bash\nbwa mem ref_index reads.fq | samtools sort -o sorted_reads.bam\n```\n\nPipes the alignment output from BWA to samtools and sorts it before writing to `sorted_reads.bam`.\n\n5. **Using extra options for speed or marking secondary alignments**:\n\n```bash\nbwa mem -t 4 -M ref_index reads.fq > aligned_reads.sam\n```\n\n- `-t 4`: Use 4 threads for faster alignment.\n- `-M`: Mark shorter split alignments as secondary, which helps compatibility with certain downstream tools (e.g., Picard).\n\n---\n\n## Learning Objectives (Variant Calling Workflow)\n\n- Exploring the variant calling workflow\n- Choosing suitable BWA parameters for your dataset\n- Understanding alignment clean-up steps\n\n---\n\n## Variant Calling Workflow\n\nA typical variant calling workflow includes:\n\n1. Quality control (QC)\n2. Alignment (e.g., using BWA)\n3. Alignment clean-up (e.g., marking duplicates, sorting)\n4. Variant calling\n5. Variant filtering and annotation\n\nAfter obtaining raw sequencing data, you typically use tools like FastQC to check read quality. Next, you align the reads to a reference genome and clean up the alignments before applying variant-calling tools (e.g., GATK or samtools/bcftools) and then filter/annotate the resulting variants.\n\n---\n\n## Environment and Directory Setup\n\nIn a cluster environment (e.g., Harvard’s O2), the process might be as follows:\n\n1. **Request an interactive session** (example command):\n\n```bash\nsrun --pty -p interactive -t 0-6:00 --mem 8G -c 2 --reservation=HBC bash\n```\n\n2. **Create project directories**:\n\n```bash\nmkdir ~/var-calling\ncd ~/var-calling\n\nmkdir -p raw_data reference_data scripts logs meta results/bwa\n```\n\n3. **Copy required data**:\n\n```bash\ncp /n/groups/hbctraining/ngs-data-analysis-longcourse/var-calling/raw_fastq/*fq raw_data/\ncp /n/groups/hbctraining/ngs-data-analysis-longcourse/var-calling/reference_data/chr20.fa reference_data/\n```\n\nIn this tutorial, we use a subset of the Genome in a Bottle (GIAB) NA12878 data (human genome reads restricted to chromosome 20), consisting of ~4 million paired-end reads.\n\n---\n\n## QC and Alignment\n\n1. **Skipping QC**: Typically you would use FastQC, but for demonstration we skip that step.\n2. **Aligner choice**: BWA is often preferred in variant calling for its high accuracy. Minimal misalignment can help avoid false positives in variant detection.\n3. **BWA modes**:\n   - BWA-backtrack: Up to 100-bp reads\n   - BWA-SW: Longer reads (70 bp ~ 1 Mbps) with split alignment\n   - BWA-MEM: Newest, recommended for most use cases, supports long reads and high accuracy\n\nIn most variant-calling workflows, **BWA-MEM** is used.\n\n---\n\n## Using BWA-MEM for Alignment\n\n### 1. Creating a BWA-MEM Index\n\n```bash\ncd ~/var-calling/reference_data\nmodule load gcc/6.2.0 bwa/0.7.8\n\nbwa index -p chr20 chr20.fa\n```\n\n- `-p chr20`: Uses `chr20` as the prefix for all index files.\n- `chr20.fa`: Reference genome file (only chromosome 20 here).\n\n### 2. Aligning Reads\n\n```bash\ncd ~/var-calling\n\nbwa mem -M -t 2 \\\n  reference_data/chr20 \\\n  raw_data/na12878_1.fq raw_data/na12878_2.fq \\\n  2> logs/bwa.err \\\n  > results/bwa/na12878.sam\n```\n\n- `-M`: Marks shorter split hits as secondary, helpful for some downstream tools.\n- `-t 2`: Uses 2 threads.\n- `2> logs/bwa.err`: Redirects standard error to a log file.\n- `> results/bwa/na12878.sam`: Writes output to `na12878.sam`.\n\n---\n\n## Alignment Clean-up\n\nFor variant calling, marking duplicates is crucial to avoid PCR artifact errors.\n\n### 1. Installing/Loading Picard\n\n```bash\nmodule spider picard\nmodule load picard/2.8.0\n```\n\nUsage:\n\n```bash\njava -jar $PICARD/picard-2.8.0.jar [ToolName] [options]\n```\n\n### 2. Sorting by Coordinate (SortSam)\n\nPicard’s `SortSam` tool sorts SAM/BAM files by coordinate. Key options:\n- `INPUT`: Input file (SAM/BAM)\n- `OUTPUT`: Output file (SAM/BAM)\n- `SORT_ORDER`: Sort order (e.g., coordinate, queryname)\n- `VALIDATION_STRINGENCY`: Level of validation (set to `SILENT` to avoid errors from BWA’s unmapped flags)\n\nExample:\n\n```bash\ncd results/bwa\n\njava -Xmx8G -jar $PICARD/picard-2.8.0.jar SortSam \\\n  INPUT=na12878.sam \\\n  OUTPUT=na12878_sorted.sam \\\n  SORT_ORDER=coordinate \\\n  VALIDATION_STRINGENCY=SILENT\n```\n\n### 3. Marking Duplicates (MarkDuplicates)\n\nPicard’s `MarkDuplicates` identifies and tags duplicate reads (PCR or optical) in BAM/SAM files. Key options:\n- `INPUT`: Sorted input file\n- `OUTPUT`: Output file\n- `METRICS_FILE`: File to write duplication metrics\n- `ASSUME_SORTED`: Set to true if the input is coordinate-sorted\n- `VALIDATION_STRINGENCY`: Similar to above\n\nExample:\n\n```bash\njava -Xmx8G -jar $PICARD/picard-2.8.0.jar MarkDuplicates \\\n  INPUT=na12878_sorted.sam \\\n  OUTPUT=na12878_sorted_marked.bam \\\n  METRICS_FILE=metrics.txt \\\n  ASSUME_SORTED=true \\\n  VALIDATION_STRINGENCY=SILENT\n```\n\n`-Xmx8G` ensures Java uses no more than 8 GB of memory (adjust if needed for larger data).\n\n### 4. Creating an Index for the BAM File\n\nUse samtools to index the marked BAM file for visualization or downstream steps:\n\n```bash\nmodule load gcc/6.2.0 samtools/1.9\n\nsamtools index na12878_sorted_marked.bam\n```\n\n---\n\n## Summary\n\nFollowing these steps, you have:\n1. Used **BWA-MEM** to align reads to a reference genome\n2. Sorted the alignment results\n3. Marked duplicates\n4. Created an index for the final BAM file\n\nThe resulting file can now be used in subsequent variant-calling pipelines (e.g., GATK or bcftools), followed by variant filtering and annotation. These materials are adapted from open-access teaching materials by the Harvard Chan Bioinformatics Core (HBC).",
        "metadata": {
            "source": "BWA-MEM",
            "page": 17
        }
    },
    {
        "content": "# samtools – Utilities for the Sequence Alignment/Map (SAM) format\n\n## NAME\n**samtools** – A suite of programs for manipulating alignments in the SAM (Sequence Alignment/Map), BAM, and CRAM formats.\n\n## SYNOPSIS (Selected Examples)\n\n```bash\nsamtools addreplacerg -r 'ID:fish' -r 'LB:1334' -r 'SM:alpha' -o output.bam input.bam\nsamtools ampliconclip -b bed.file input.bam\nsamtools ampliconstats primers.bed in.bam\nsamtools bedcov aln.sorted.bam\nsamtools calmd in.sorted.bam ref.fasta\nsamtools cat out.bam in1.bam in2.bam in3.bam\nsamtools collate -o aln.name_collated.bam aln.sorted.bam\nsamtools consensus -o out.fasta in.bam\nsamtools coverage aln.sorted.bam\nsamtools cram-size -v -o out.size in.cram\nsamtools depad input.bam\nsamtools depth aln.sorted.bam\nsamtools dict -a GRCh38 -s \"Homo sapiens\" ref.fasta\nsamtools faidx ref.fasta\nsamtools fasta input.bam > output.fasta\nsamtools fastq input.bam > output.fastq\nsamtools fixmate in.namesorted.sam out.bam\nsamtools flags PAIRED,UNMAP,MUNMAP\nsamtools flagstat aln.sorted.bam\nsamtools fqidx ref.fastq\nsamtools head in.bam\nsamtools idxstats aln.sorted.bam\nsamtools import input.fastq > output.bam\nsamtools index aln.sorted.bam\nsamtools markdup in.algnsorted.bam out.bam\nsamtools merge out.bam in1.bam in2.bam in3.bam\nsamtools mpileup -f ref.fasta -r chr3:1,000-2,000 in1.bam in2.bam\nsamtools phase input.bam\nsamtools quickcheck in1.bam in2.cram\nsamtools reference -o ref.fa in.cram\nsamtools reheader in.header.sam in.bam > out.bam\nsamtools reset -o /tmp/reset.bam processed.bam\nsamtools samples input.bam\nsamtools sort -T /tmp/aln.sorted -o aln.sorted.bam aln.bam\nsamtools split merged.bam\nsamtools stats aln.sorted.bam\nsamtools targetcut input.bam\nsamtools tview aln.sorted.bam ref.fasta\nsamtools view -bt ref_list.txt -o aln.bam aln.sam.gz\n```\n\n## BRIEF DESCRIPTION\nSamtools converts between SAM/BAM/CRAM formats, sorts and merges alignments, indexes them for fast random access, and retrieves reads within specific genomic regions. It can read/write compressed data, works well in pipelines, and supports remote file access.\n\n## SELECTED COMMANDS\n\n### samtools view\n```bash\nsamtools view [options] in.bam [region...]\n```\n- Converts input to SAM/BAM/CRAM.\n- If indexed and region(s) are specified, outputs only alignments overlapping those regions.\n\n### samtools tview\n```bash\nsamtools tview in.sorted.bam [ref.fasta]\n```\n- Text-based viewer for alignments. Press `?` for help.\n\n### samtools quickcheck\n```bash\nsamtools quickcheck in.bam in.cram\n```\n- Quickly checks if files are intact (e.g., valid header, EOF block in BAM).\n\n### samtools head\n```bash\nsamtools head in.bam\n```\n- Prints file headers and optionally the first few alignments.\n\n### samtools index\n```bash\nsamtools index aln.bam\n```\n- Indexes a coordinate-sorted BAM or CRAM file for fast region queries.\n\n### samtools sort\n```bash\nsamtools sort -o aln.sorted.bam aln.bam\n```\n- Sorts alignments by coordinate (default) or by name (`-n`).\n\n### samtools collate\n```bash\nsamtools collate -o aln.name_collated.bam aln.sorted.bam\n```\n- Groups reads by name, without full sorting.\n\n### samtools idxstats\n```bash\nsamtools idxstats aln.sorted.bam\n```\n- Reports reference sequence length, mapped reads, and unmapped reads.\n\n### samtools flagstat\n```bash\nsamtools flagstat aln.sorted.bam\n```\n- Gives overall alignment statistics (number of mapped/unmapped reads, duplicates, etc.).\n\n### samtools mpileup\n```bash\nsamtools mpileup -f ref.fasta in1.bam in2.bam\n```\n- Generates textual pileup of reads.\n\n### samtools consensus\n```bash\nsamtools consensus -o out.fasta in.bam\n```\n- Creates consensus sequence from alignment data.\n\n### samtools coverage\n```bash\nsamtools coverage aln.sorted.bam\n```\n- Produces coverage stats per chromosome or region.\n\n### samtools merge\n```bash\nsamtools merge out.bam in1.bam in2.bam\n```\n- Merges multiple sorted BAM/CRAM files into one.\n\n### samtools split\n```bash\nsamtools split merged.bam\n```\n- Splits one file into separate files by read group.\n\n### samtools cat\n```bash\nsamtools cat out.bam in1.bam in2.bam\n```\n- Concatenates BAM/CRAM files of the same format.\n\n### samtools fastq/fasta\n```bash\nsamtools fastq in.bam > out.fastq\nsamtools fasta in.bam > out.fasta\n```\n- Converts BAM/CRAM to FASTQ or FASTA.\n\n### samtools faidx\n```bash\nsamtools faidx ref.fasta\n```\n- Indexes a FASTA file or extracts subsequences if regions are given.\n\n### samtools fqidx\n```bash\nsamtools fqidx ref.fastq\n```\n- Similar to faidx but for FASTQ (small files only).\n\n### samtools calmd\n```bash\nsamtools calmd aln.bam ref.fasta\n```\n- Adds or updates the MD tag.\n\n### samtools fixmate\n```bash\nsamtools fixmate in.nameSrt.bam out.bam\n```\n- Fills in mate coordinates, ISIZE, and flags from a name-sorted file.\n\n### samtools markdup\n```bash\nsamtools markdup in.algnsorted.bam out.bam\n```\n- Marks duplicate reads after fixmate.\n\n### samtools reheader\n```bash\nsamtools reheader in.header.sam in.bam > out.bam\n```\n- Replaces the header without re-converting the entire file.\n\n### samtools stats\n```bash\nsamtools stats aln.sorted.bam\n```\n- Collects alignment statistics, output can be plotted with plot-bamstats.\n\n---\n\n## AUTHOR & LINKS\n- Original author: Heng Li (Sanger Institute).\n- Project page: [http://www.htslib.org/](http://www.htslib.org/)\n- Code repositories:\n  - [Samtools GitHub](https://github.com/samtools/samtools)\n  - [HTSlib GitHub](https://github.com/samtools/htslib)\n  - [Bcftools GitHub](http://samtools.github.io/bcftools)\n",
        "metadata": {
            "source": "samtools",
            "page": 18
        }
    },
    {
        "content": "## Synopsis\nThis tutorial demonstrates how to explore, process, and manipulate SAM/BAM files using **samtools**. We will walk through installing samtools, creating a demo directory, converting between SAM and BAM formats, sorting, indexing, and selectively viewing alignments.\n\n---\n\n## Installing samtools\n```bash\ncd ~\nmkdir src          # optional if you do not already have a src directory\ncd ~/src\n\ngit clone https://github.com/samtools/htslib\ngit clone https://github.com/samtools/samtools\n\ncd samtools\nmake\ncp samtools ~/bin  # move the samtools binary to your PATH\n```\n\n---\n\n## Setup\n1. **Create** a new directory from your home directory:\n\n   ```bash\n   cd ~\n   mkdir samtools-demo\n   cd samtools-demo\n   ```\n2. **Download** the sample SAM file and decompress:\n\n   ```bash\n   curl https://s3.amazonaws.com/samtools-tutorial/sample.sam.gz > sample.sam.gz\n   gzip -d sample.sam.gz\n   ```\n\n---\n\n## samtools help\nTyping `samtools` alone displays the available subcommands. Examples:\n```bash\nsamtools view\nsamtools sort\nsamtools depth\n```\n\n---\n\n## Converting SAM to BAM (`samtools view`)\n1. **Convert** the SAM file to BAM:\n   ```bash\n   samtools view -S -b sample.sam > sample.bam\n   ```\n   Here:\n   - `-S` specifies the input is SAM.\n   - `-b` specifies to output BAM.\n   - The `>` redirect writes to `sample.bam`.\n2. **Check** the BAM contents (by converting back to SAM and showing only the first few lines):\n   ```bash\n   samtools view sample.bam | head\n   ```\n\n---\n\n## Sorting alignments (`samtools sort`)\n1. **Sort** the BAM so alignments are in order by genomic position:\n   ```bash\n   samtools sort sample.bam -o sample.sorted.bam\n   ```\n2. **Inspect** a few lines:\n   ```bash\n   samtools view sample.sorted.bam | head\n   ```\n   Now the coordinates should appear in ascending order.\n\n---\n\n## Indexing a sorted BAM (`samtools index`)\n1. **Index** the sorted BAM:\n   ```bash\n   samtools index sample.sorted.bam\n   ```\n2. **Check** for the newly created index file (`.bai` extension):\n   ```bash\n   ls\n   ```\n3. **Use** the index to extract alignments from chromosome 1, positions `33,000,000–34,000,000`:\n   ```bash\n   samtools view sample.sorted.bam 1:33000000-34000000\n   ```\n   And count how many alignments are in that region:\n   ```bash\n   samtools view sample.sorted.bam 1:33000000-34000000 | wc -l\n   ```\n\n---\n\n## More on `samtools view`\n### Viewing a subset of alignments\n\n- **Print** the first five lines:\n  ```bash\n  samtools view sample.sorted.bam | head -n 5\n  ```\n- **Make FLAG more readable** with `-X`:\n  ```bash\n  samtools view -X sample.sorted.bam | head -n 5\n  ```\n  (Try `samtools view -?` for help on all options.)\n\n### Counting alignments\n```bash\nsamtools view sample.sorted.bam | wc -l\n```\n\n### Inspecting the header\n```bash\nsamtools view -H sample.sorted.bam\n```\n\n### Filtering by FLAG\n- **Proper pairs** only (`-f 0x2`):\n  ```bash\n  samtools view -f 0x2 sample.sorted.bam | wc -l\n  ```\n- **NOT properly paired** (`-F 0x2`):\n  ```bash\n  samtools view -F 0x2 sample.sorted.bam | wc -l\n  ```\n  `-F` indicates **exclude** flags. The counts of `-f 0x2` plus `-F 0x2` should sum to the total number of alignments.\n\n---\n\n## Summary\n1. **Install** samtools.\n2. **Convert** SAM→BAM.\n3. **Sort** by genomic position.\n4. **Index** to enable region queries.\n5. **View** subsets of data based on coordinates or specific flags.\n6. **Inspect** headers and other metadata.\n\nThis workflow covers basic operations required for many downstream applications (e.g., variant calling or visualization in IGV). For more details, consult the [samtools documentation](https://www.htslib.org/).",
        "metadata": {
            "source": "samtools",
            "page": 19
        }
    },
    {
        "content": "## FASTQ to BAM/CRAM\nModern sequencing instruments typically produce *unaligned* data in FASTQ format. We can store unaligned data in BAM or CRAM to include valuable metadata (e.g., headers, auxiliary tags), but the focus here is on producing a **sorted, aligned** BAM or CRAM file.\n\n---\n\n## Two Main Approaches\n1. **Alignment / mapping** to a known reference.\n2. **De-novo assembly** (not covered in detail here).\n\n---\n\n## Example Workflow for Alignment\n\nThis pipeline takes paired FASTQ reads, aligns them to a reference using Minimap2, and produces a sorted, duplicate-marked BAM or CRAM.\n\n### Steps Overview\n1. **Map / align**\n2. **Fix mate-pair issues** (e.g., with `samtools fixmate`)\n3. **Mark duplicates (part 1)** – add tags needed for later duplication marking\n4. **Sort** to positional order\n5. **Mark duplicates (part 2)** – final marking of duplicates\n6. **Convert** to final file format (BAM or CRAM)\n\n### Step 1: Mapping\n```bash\nminimap2 -t 8 -a -x sr C.Elegans.fa \\\n  SRR065390_1.fastq SRR065390_2.fastq -o CE.sam\n```\n- `-t 8`: uses 8 threads.\n- `-a`: output in SAM.\n- `-x sr`: sets Minimap2 presets for paired-end short reads.\n- Output is name-collated (read pairs are together), which is needed for the next step.\n\n### Step 2: Fixing Mate-Pair Issues\n```bash\nsamtools fixmate -O bam,level=1 CE.sam fixmate.bam\n```\n- `samtools fixmate` checks and corrects mate information (FLAG, RNEXT, PNEXT, TLEN).\n- `-O bam,level=1` outputs BAM with minimal compression for speed.\n- Add `-m` if you want to populate mate CIGAR (`MC`) and mate score (`ms`) tags:\n\n  ```bash\n  samtools fixmate -O bam,level=1 -m CE.sam fixmate.bam\n  ```\n\n### Step 3 & 4: Sorting to Positional Order\n```bash\nsamtools sort -l 1 -@8 -o pos.srt.bam -T /tmp/example_prefix fixmate.bam\n```\n- Sort reads by genomic coordinate.\n- `-@8`: uses 8 threads.\n- `-l 1`: minimal compression (optional).\n- `-T /tmp/example_prefix`: prefix for temporary sorting files.\n\n### Step 5: Marking Duplicates\n```bash\nsamtools markdup -O bam,level=1 pos.srt.bam markdup.bam\n```\n- Uses the MC/ms tags from the `-m` fixmate step.\n\n### Step 6: Converting to Final Format\n```bash\nsamtools view -@8 markdup.bam -o final.bam\n```\nor, to produce CRAM:\n```bash\nsamtools view -T C.Elegans.fa -@8 markdup.bam -o final.cram\n```\n\n---\n\n## Pipelining for Efficiency\nInstead of writing each intermediate file to disk, we can **pipe** the commands:\n```bash\nminimap2 -t 8 -a -x sr C.Elegans.fa SRR065390_[12].fastq | \\\nsamtools fixmate -u -m - - | \\\nsamtools sort -u -@2 -T /tmp/example_prefix - | \\\nsamtools markdup -@8 --reference C.Elegans.fa - final.cram\n```\n- `-u` or `-O bam,level=0` keeps data uncompressed in the pipeline for speed.\n- `set -o pipefail` helps detect errors in earlier commands.\n\n---\n\n## Converting Back to FASTQ\nIf the unmapped reads were retained, you can revert your final BAM/CRAM to FASTQ:\n```bash\nsamtools sort -n -@8 final.cram | \\\nsamtools fastq - -1 dat_1.fq -2 dat_2.fq > /dev/null\n```\n- Sorting by read name (`-n`) is required before producing paired FASTQ.\n- The original exact FASTQ order is not preserved but is generally unimportant for reanalysis.\n\n---\n\n## De-novo Assembly\n- Assemblers produce a consensus FASTA or FASTQ rather than individual alignments.\n- If you need per-read alignments to the newly assembled consensus, just build an index on the assembly and follow the same **Mapping** steps as above.\n- For CRAM, ensure you have access to the same consensus reference or embed it:\n  ```bash\n  samtools view -O CRAM,embed_ref in.sam -o out.cram\n  ```\n\n---\n\n## Key Takeaways\n- **Name-collated** BAM is essential for certain steps (e.g., `fixmate`, partial duplication marking).\n- **Coordinate-sorted** BAM is needed for final duplicate marking and most downstream analyses.\n- **Pipelines** avoid intermediate files, are faster, and use uncompressed or minimal-compression data in memory.\n- **CRAM** can reduce storage size but requires consistent references.\n- Always verify each step with basic stats (`samtools flagstat`, etc.) to ensure no unexpected data loss.\n",
        "metadata": {
            "source": "samtools",
            "page": 20
        }
    },
    {
        "content": "# WGS/WES Mapping to Variant Calls\n\n## Overview\nTypical DNA sequence analysis involves three main phases: **Mapping**, **Improvement**, and **Variant Calling**. Below is a common workflow illustrating these steps for Whole Genome Sequencing (WGS) or Whole Exome Sequencing (WES).\n\n---\n\n## 1. Mapping\n\n1. **Reference Preparation**:\n   ```bash\n   bwa index <ref.fa>\n   ```\n   - Prepares a Burrows–Wheeler Transform (BWT) index, required for BWA.\n\n2. **Aligning Reads with BWA-MEM**:\n   ```bash\n   bwa mem -R '@RG\\tID:foo\\tSM:bar\\tLB:library1' <ref.fa> <read1.fq> <read2.fq> > lane.sam\n   ```\n   - The `-R` flag attaches read-group metadata (e.g., sample name, library). This data is crucial for downstream tools.\n   - Output is in SAM format.\n\n3. **Fixmate**:\n   ```bash\n   samtools fixmate -O bam <lane.sam> <lane_fixmate.bam>\n   ```\n   - Cleans up pairing flags, ensuring consistent FLAG and mate info.\n\n4. **Sort**:\n   ```bash\n   samtools sort -O bam -o <lane_sorted.bam> -T </tmp/lane_temp> <lane_fixmate.bam>\n   ```\n   - Converts name-collated BAM to coordinate-sorted BAM for downstream steps.\n\n---\n\n## 2. Improvement\n\n### 2.1 Indel Realignment (GATK)\n\nMisalignment around INDELs can cause false variant calls. The GATK realignment step includes two commands:\n\n```bash\n# Step 1: Identify intervals needing realignment\njava -Xmx2g -jar GenomeAnalysisTK.jar -T RealignerTargetCreator \\\n  -R <ref.fa> -I <lane.bam> -o <lane.intervals> --known <Mills1000G.b38.vcf>\n\n# Step 2: Realign around those intervals\njava -Xmx4g -jar GenomeAnalysisTK.jar -T IndelRealigner -R <ref.fa> \\\n  -I <lane.bam> -targetIntervals <lane.intervals> \\\n  --known <Mills1000G.b38.vcf> -o <lane_realigned.bam>\n```\n\n### 2.2 Base Quality Score Recalibration (GATK BQSR)\n\nReduces machine- or run-specific biases by adjusting base quality scores.\n\n```bash\n# Analyze covariates\njava -Xmx4g -jar GenomeAnalysisTK.jar -T BaseRecalibrator \\\n  -R <ref.fa> -knownSites <dbsnp_142.b38.vcf> \\\n  -I <lane.bam> -o <lane_recal.table>\n\n# Apply recalibration\njava -Xmx2g -jar GenomeAnalysisTK.jar -T PrintReads -R <ref.fa> \\\n  -I <lane.bam> --BQSR <lane_recal.table> -o <lane_recal.bam>\n```\n\n### 2.3 Marking Duplicates & Merging Libraries\n\n1. **Mark Duplicates**:\n   ```bash\n   java -Xmx2g -jar MarkDuplicates.jar VALIDATION_STRINGENCY=LENIENT \\\n     INPUT=<lane_1.bam> INPUT=<lane_2.bam> \\\n     OUTPUT=<library.bam>\n   ```\n   - Marks PCR/optical duplicates.\n\n2. **Merge** libraries into a single sample-level BAM:\n   ```bash\n   samtools merge <sample.bam> <library1.bam> <library2.bam> <library3.bam>\n   samtools index <sample.bam>\n   ```\n\n3. **(Optional) Realign Once More**:\n   ```bash\n   java -Xmx2g -jar GenomeAnalysisTK.jar -T RealignerTargetCreator ...\n   java -Xmx4g -jar GenomeAnalysisTK.jar -T IndelRealigner ...\n   samtools index <sample_realigned.bam>\n   ```\n\n---\n\n## 3. Variant Calling\n\n### 3.1 Generating BCF/VCF with bcftools\n\n`mpileup` is used to gather per-base data, and `bcftools call` calls variants:\n\n```bash\n# One-step approach using a pipe\nbcftools mpileup -Ou -f <ref.fa> <sample1.bam> <sample2.bam> <sample3.bam> | \\\n  bcftools call -vmO z -o <study.vcf.gz>\n```\n\nAlternatively, two-step:\n```bash\n# Step 1: produce BCF\nbcftools mpileup -Ob -o <study.bcf> -f <ref.fa> <sample1.bam> <sample2.bam> <sample3.bam>\n\n# Step 2: call variants\nbcftools call -vmO z -o <study.vcf.gz> <study.bcf>\n```\n\n### 3.2 VCF Indexing & Statistics\n\n```bash\ntabix -p vcf <study.vcf.gz>\n\nbcftools stats -F <ref.fa> -s - <study.vcf.gz> > <study.vcf.gz.stats>\nmkdir plots\nplot-vcfstats -p plots/ <study.vcf.gz.stats>\n```\n\n### 3.3 Filtering\n\n```bash\nbcftools filter -O z -o <study_filtered.vcf.gz> -s LOWQUAL \\\n  -i'%QUAL>10' <study.vcf.gz>\n```\n\nFiltering strategies depend heavily on the study’s goals, data quality, and coverage.\n\n---\n\n## Key Points\n1. **Mapping**: Use BWA-MEM (index reference, align reads, fixmate, sort).\n2. **Improvement**: Realign around indels, recalibrate base qualities, mark duplicates, and merge.\n3. **Variant Calling**: Generate pileup and call variants (bcftools), index VCF, gather stats, and filter.\n4. **Read Group Info** (`@RG`): Ensure correct `ID`, `SM`, and `LB` for GATK/bcftools compatibility.\n5. **QC & Filtering**: Visualize with IGV, review alignment stats, apply appropriate variant filters.\n",
        "metadata": {
            "source": "samtools",
            "page": 21
        }
    },
    {
        "content": "## Quick Guide to DiffBind\n\n**DiffBind** is an R/Bioconductor package for analyzing differential binding (peaks) in ChIP-seq or ATAC-seq data. Below is a concise workflow:\n\n---\n\n### 1. Installation & Setup\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DiffBind\")\n\nlibrary(DiffBind)\n```\n\n---\n\n### 2. Prepare Input Files\n\n- **SampleSheet.csv**: Each row describes one sample, including columns:\n  - `SampleID`, `Tissue`, `Factor`, `Condition`, `Replicate`\n  - `bamReads`, `bamControl` (paths to BAM files)\n  - `Peaks` (path to called peak file)\n  - `PeakCaller` (e.g., MACS2)\n\nExample:\n```csv\nSampleID,Tissue,Factor,Condition,Replicate,bamReads,bamControl,Peaks,PeakCaller\nS1,MCF7,ER,Responsive,1,S1.bam,S1_input.bam,S1_peaks.bed,macs2\nS2,MCF7,ER,Responsive,2,S2.bam,S2_input.bam,S2_peaks.bed,macs2\n...\n```\n\n---\n\n### 3. Read & Merge Peaks\n```r\n# Read metadata from CSV and merge overlapping peaks\ndbObj <- dba(sampleSheet = \"SampleSheet.csv\")\nplot(dbObj)  # Heatmap of sample correlations\n```\n\n---\n\n### 4. Count Reads in Peaks\n```r\n# Counts reads from BAM files in merged peak regions\n# bUseSummarizeOverlaps=TRUE uses a more standardized counting method\ndbObj <- dba.count(dbObj, bUseSummarizeOverlaps=TRUE)\n\n# Optional: library size normalization\ndbObj <- dba.normalize(dbObj)\n```\n\n---\n\n### 5. Differential Analysis\n\n1. **Set contrast** (which groups to compare):\n```r\n# Example: Condition = \"Resistant\" vs. \"Responsive\", with \"Responsive\" as baseline\n# or specify a design formula including multiple factors\n\ndbObj <- dba.contrast(dbObj, reorderMeta=list(Condition=\"Responsive\"))\n```\n\n2. **Run differential binding** (using DESeq2 by default):\n```r\ndbObj <- dba.analyze(dbObj)\n\n# Show results (number of significantly changed sites)\ndba.show(dbObj, bContrasts=TRUE)\n```\n\n3. **Extract differentially bound peaks**:\n```r\ndbSites <- dba.report(dbObj)\n\n# Convert to data frame, filter by FDR, save as BED\nout <- as.data.frame(dbSites)\nsigPeaks <- out[out$FDR < 0.05, c(\"seqnames\",\"start\",\"end\",\"strand\",\"Fold\")]\nwrite.table(sigPeaks, \"diffbind_sig_peaks.bed\", sep=\"\\t\", row.names=FALSE, col.names=FALSE)\n```\n\n---\n\n### 6. Visualization\n\n- **Heatmap**:\n  ```r\n  dba.plotHeatmap(dbObj)\n  ```\n\n- **PCA**:\n  ```r\n  dba.plotPCA(dbObj, label=DBA_CONDITION)\n  ```\n\n- **Venn Diagram** (overlaps):\n  ```r\n  dba.plotVenn(dbObj, contrast=1)\n  ```\n\n- **Volcano Plot**:\n  ```r\n  dba.plotVolcano(dbObj)\n  ```\n\n---\n\n### Key Points\n1. **SampleSheet** + **peak files** + **BAM files** required.\n2. Merged peaks → read counting → normalization → differential analysis.\n3. Results are retrieved via `dba.report()`. Visualization includes heatmaps, PCA, venn diagrams.\n4. By default, **DESeq2** is used, but **edgeR** can also be specified.\n\nFor more detail, see the [DiffBind vignette](https://bioconductor.org/packages/release/bioc/vignettes/DiffBind/inst/doc/DiffBind.pdf).",
        "metadata": {
            "source": "Diffbind",
            "page": 22
        }
    },
    {
        "content": "Instructions for using the STAR tool: Below is a more concise overview of STAR usage (sections 1.2 through 14.25), preserving all key instructions.\n\n---\n\n## 1.2 Basic Work Flow\n\n**STAR** involves two main steps:\n\n1. **Generating genome indexes**  \n   - Input: reference genome FASTA (one or more) and (optional but recommended) annotation GTF.\n   - Output: STAR-specific index files stored in a user-defined directory.\n\n2. **Mapping reads to the genome**  \n   - Input: the STAR index folder plus RNA-seq reads (FASTA/FASTQ).  \n   - Output: alignment files (SAM/BAM), splice junctions, unmapped reads, logs, etc.\n\n---\n\n## 2. Generating Genome Indexes\n\n### 2.1 Basic Options\n\nExample:\n```\nSTAR \\\n  --runThreadN <threads> \\\n  --runMode genomeGenerate \\\n  --genomeDir /path/to/genomeDir \\\n  --genomeFastaFiles /path/to/genome.fa [more FASTAs...] \\\n  --sjdbGTFfile /path/to/annotations.gtf \\\n  --sjdbOverhang <ReadLength - 1>\n```\n- **--runThreadN**: number of threads.\n- **--runMode genomeGenerate**: tells STAR to build the genome index.\n- **--genomeDir**: output folder for index files.\n- **--genomeFastaFiles**: one or more uncompressed FASTA files.\n- **--sjdbGTFfile**: optional but strongly recommended GTF.\n- **--sjdbOverhang**: normally set to read length minus 1.\n\n### 2.2 Advanced Notes\n\n- **Include major chromosomes and unlocalized scaffolds**; skip patches/haplotypes unless needed.\n- **Annotations must match genome FASTA** (naming consistency: e.g., both from Ensembl).\n- **Small genomes**: scale down `--genomeSAindexNbases` (e.g., `min(14, log2(GenomeLength)/2 - 1)`).\n- **Large references**: if >5000 scaffolds, reduce `--genomeChrBinNbits`.\n\n---\n\n## 3. Running Mapping Jobs\n\n### 3.1 Basic Options\n\nExample:\n```\nSTAR \\\n  --runThreadN <threads> \\\n  --genomeDir /path/to/genomeDir \\\n  --readFilesIn /path/to/read1 [ /path/to/read2 ]\n```\n- **--genomeDir**: folder containing the STAR indexes.\n- **--readFilesIn**: reads to map (paired-end requires read1 and read2).  \n- If reads are gzipped: `--readFilesCommand zcat` (or similar) to decompress on the fly.\n\n### 3.2 Other Key Options\n\n- **Using annotations at mapping**: `--sjdbGTFfile <ann.gtf>` can be specified again here, even if not used when generating the index.\n- **ENCODE-style defaults**: e.g., `--outFilterType BySJout`, `--alignSJoverhangMin 8`, etc.\n- **Shared memory**: controlled by `--genomeLoad` (e.g. `LoadAndKeep`). May need adjusting system kernel parameters.\n\n---\n\n## 4. Output Files\n\n- **Log.out**: main detailed log.  \n- **Log.progress.out**: real-time stats.  \n- **Log.final.out**: final mapping summary (unique/multi-mapping rates, splices, etc.).\n- **Aligned.out.sam** or **Aligned.out.bam**: primary alignment file.\n- **Aligned.sortedByCoord.out.bam**: coordinate-sorted BAM if `--outSAMtype BAM SortedByCoordinate`.\n- **SJ.out.tab**: high-confidence splice junctions.\n\n---\n\n## 5. Chimeric/Fusion Alignments\n\n- Enable detection with `--chimSegmentMin > 0`.\n- Can output to the main BAM (`--chimOutType WithinBAM`) or a separate file (`Chimeric.out.sam` or `Chimeric.out.junction`).\n- **STAR-Fusion** is recommended for robust fusion detection.\n\n---\n\n## 6. Output in Transcript Coordinates\n\n- **--quantMode TranscriptomeSAM**: outputs `Aligned.toTranscriptome.out.bam` (used by tools like RSEM/eXpress).  \n- By default, indels and soft-clips are disallowed in transcriptome alignments.\n\n---\n\n## 7. Counting Reads per Gene\n\n- **--quantMode GeneCounts**: counts reads overlapping genes.  \n- Output in `ReadsPerGene.out.tab` with four columns (unstranded, forward-strand, reverse-strand, etc.).\n\n---\n\n## 8. 2-pass Mapping\n\nImproves novel junction discovery:\n1. **Multi-sample**: run pass 1 for all samples, merge/collect junctions, then pass 2 using these junctions.\n2. **Per-sample**: use `--twopassMode Basic`, so STAR auto-discovers junctions in pass 1 and re-maps in pass 2.\n\n---\n\n## 9. Merging Overlapping Paired-end Reads\n\n- **--peOverlapNbasesMin**, **--peOverlapMMp**: merges mates if they overlap significantly, improving accuracy and enabling chimeric detection in the overlapped region.\n\n---\n\n## 10. Personal Variants\n\n- **--varVCFfile**: specify a VCF (currently SNVs only) to mark overlap with genotype alleles in alignments.\n\n---\n\n## 11. WASP Filtering (Allele-Specific)\n\n- **--waspOutputMode SAMtag**: adds `vW` to each alignment indicating whether it passes WASP filters.\n\n---\n\n## 12. Multimapping Chimeras\n\n- **chimMultimapNmax > 0**: detects chimeric alignments that also multimap.\n- Reported in `Chimeric.out.junction`.\n\n---\n\n## 13. STARsolo (Single-Cell RNA-seq)\n\n- **--soloType Droplet**: for 10X/Drop-seq.  \n- **--soloCBwhitelist**: file with valid barcodes.  \n- Must specify cell barcode/UMI positions (e.g. `--soloCBstart`, `--soloUMIstart`).  \n- Outputs barcodes.tsv, genes.tsv, matrix.mtx, etc. in a CellRanger-compatible format.\n\n---\n\n## 14. Description of All Options\n\nSTAR has extensive parameters grouped by function:\n1. **Parameter Files**: load config from `--parametersFiles`.\n2. **System**: shell settings.\n3. **Run Parameters**: `--runMode`, `--runThreadN`, etc.\n4. **Genome Parameters**: `--genomeDir`, `--genomeLoad`, etc.\n5. **Genome Indexing**: `--genomeChrBinNbits`, etc.\n6. **Splice Junctions Database**: `--sjdbGTFfile`, `--sjdbOverhang`.\n7. **Variation**: `--varVCFfile`.\n8. **Input Files**: `--readFilesIn`, etc.\n9. **Read Parameters**: e.g., clipping, strand.\n10. **Limits**: memory, buffers.\n11. **General Output**: `--outFileNamePrefix`, etc.\n12. **SAM/BAM Output**: `--outSAMtype`, `--outSAMattributes`.\n13. **BAM Processing**: e.g. `--bamRemoveDuplicatesType`.\n14. **Output Wiggle**: bedGraph/wiggle tracks.\n15. **Filtering**: `--outFilterType`, `--outFilterMultimapNmax`.\n16. **Splice-Junction Filtering**: `--outSJfilter*`.\n17. **Scoring**: gap penalties.\n18. **Alignments/Seeding**: `--alignIntronMin`, etc.\n19. **Paired-End**: overlap merges.\n20. **Windows/Anchors**: binning parameters.\n21. **Chimeric**: `--chimOutType`, etc.\n22. **Quantification**: `--quantMode`, `--quantTranscriptomeBan`.\n23. **2-pass**: `--twopassMode`.\n24. **WASP**: `--waspOutputMode`.\n25. **STARsolo**: single-cell.\n\nDefaults generally suffice, but advanced users can customize alignment scoring, filtering, shared-memory usage, chimeric detection, etc.  \n\n---\n\n### Summary\n\n1. **Generate genome indexes**: runMode = genomeGenerate with FASTA + (optional) GTF.\n2. **Map reads**: supply genomeDir, read files, possibly specifying on-the-fly annotations.\n3. **Outputs**: logs, alignments (SAM/BAM), splice junctions (SJ.out.tab), chimeric reads, etc.\n4. **Chimeric/fusions**: `--chimSegmentMin` > 0.\n5. **Transcriptome**: `--quantMode TranscriptomeSAM` for transcriptome-based BAM.\n6. **Gene counts**: `--quantMode GeneCounts`.\n7. **2-pass mapping**: improved novel splice detection.\n8. **Overlapping PE**: merges if mates overlap.\n9. **Variants/WASP**: optional to tag personal SNPs or filter allele-specific reads.\n10. **STARsolo**: droplet-based single-cell analysis.\n\nThis streamlined reference should help you efficiently configure and run STAR for standard RNA-seq, single-cell, fusion detection, or other specialized tasks.\n",
        "metadata": {
            "source": "STAR",
            "page": 23
        }
    },
    {
        "content": "Instructions for using the STAR-fusion:## STAR-Fusion Usage Guide\n\nThis document consolidates notes on running **STAR-Fusion** for fusion transcript detection, either starting directly from FASTQ files or using an existing STAR alignment (Kickstart mode). It also covers important parameters, output format, and integration with **FusionInspector**.\n\n---\n\n### 1. Running STAR-Fusion\n\nSTAR-Fusion can be run in **two primary ways**:\n\n1. **Starting from FASTQ files (typical)**\n   \n   **Paired-end FASTQ**:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     --left_fq reads_1.fq \\\n     --right_fq reads_2.fq \\\n     --output_dir star_fusion_outdir\n   ```\n   **Single-end FASTQ**:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     --left_fq reads_1.fq \\\n     --output_dir star_fusion_outdir\n   ```\n   Here, `CTAT_resource_lib` is the path to the STAR-Fusion-compatible reference. STAR-Fusion internally runs the STAR aligner with recommended fusion-related parameters.\n\n2. **Using an existing STAR alignment (Kickstart mode)**\n   \n   If you have already aligned reads with STAR and have a `Chimeric.out.junction` file, you can feed that directly to STAR-Fusion:\n   ```bash\n   STAR-Fusion \\\n     --genome_lib_dir /path/to/CTAT_resource_lib \\\n     -J Chimeric.out.junction \\\n     --output_dir star_fusion_outdir\n   ```\n   This only works if you ran STAR with certain parameters (listed below) that enable chimeric read detection and generate the properly formatted `Chimeric.out.junction` file.\n\n---\n\n### 2. Running STAR for Fusion Detection\n\nIf you prefer to run STAR separately, be sure to include **key parameters** required by STAR-Fusion:\n\n```bash\nSTAR --genomeDir /path/to/star_index \\\n     --readFilesIn reads_1.fq reads_2.fq \\\n     --outReadsUnmapped None \\\n     --twopassMode Basic \\\n     --readFilesCommand \"gunzip -c\" \\  # or zcat if gzipped\n     --outSAMstrandField intronMotif \\\n     --outSAMunmapped Within \\\n     --chimSegmentMin 12 \\  # *** essential for chimeric read detection ***\n     --chimJunctionOverhangMin 8 \\\n     --chimOutJunctionFormat 1 \\  # *** required for STAR-Fusion ***\n     --alignSJDBoverhangMin 10 \\\n     --alignMatesGapMax 100000 \\\n     --alignIntronMax 100000 \\\n     --alignSJstitchMismatchNmax 5 -1 5 5 \\\n     --outSAMattrRGline ID:GRPundef \\\n     --chimMultimapScoreRange 3 \\\n     --chimScoreJunctionNonGTAG -4 \\\n     --chimMultimapNmax 20 \\\n     --chimNonchimScoreDropMin 10 \\\n     --peOverlapNbasesMin 12 \\\n     --peOverlapMMp 0.1 \\\n     --alignInsertionFlush Right \\\n     --alignSplicedMateMapLminOverLmate 0 \\\n     --alignSplicedMateMapLmin 30\n```\n\nThis creates `Chimeric.out.junction`, which STAR-Fusion can use via `-J Chimeric.out.junction`. Ensure `--chimOutJunctionFormat 1` is set so that STAR-Fusion can estimate FFPM values.\n\n---\n\n### 3. STAR-Fusion Outputs\n\nAfter successful completion, STAR-Fusion writes results to the `star_fusion_outdir` (or chosen directory). The main outputs are:\n\n1. **star-fusion.fusion_predictions.tsv**: A tab-delimited file listing each detected fusion event with supporting data. An abridged version (`.abridged.tsv`) omits explicit read identities.\n2. **Columns of interest**:\n   - **FusionName**: e.g. `GENE1--GENE2`.\n   - **JunctionReadCount**: number of reads spanning the junction (split reads).\n   - **SpanningFragCount**: number of read pairs where each mate maps to different genes.\n   - **LargeAnchorSupport**: `YES_LDAS` if the split reads have ≥25 bases of unique alignment on both sides of the breakpoint.\n   - **FFPM**: Fusion Fragments Per Million total reads (normalizes for library depth).\n   - **SpliceType**: shows if breakpoints occur at known exon junctions.\n   - **LeftGene/RightGene** and corresponding breakpoints.\n   - **LeftBreakEntropy/RightBreakEntropy**: Shannon entropy near the breakpoint (range 0–2, with low entropy often less reliable).\n   - **annots**: additional annotations, e.g. known fusions in cancer datasets, or whether the event is intrachromosomal or interchromosomal.\n\n---\n\n### 4. FusionInspector (In Silico Validation)\n\n**FusionInspector** is included in STAR-Fusion for validating or refining predicted fusions:\n\n- **--FusionInspector inspect**: re-map only the fusion-evidence reads to custom \"fusion contigs,\" enabling quick inspection.\n- **--FusionInspector validate**: re-map **all** reads to both the reference genome and the constructed fusion contigs, re-scoring fusions.\n- **--denovo_reconstruct**: attempts to assemble fusion transcripts de novo using Trinity.\n- **--examine_coding_effect**: determines coding in-frame/frameshift status and domain structure.\n\nExample:\n```bash\nSTAR-Fusion \\\n  --left_fq reads_1.fq.gz \\\n  --right_fq reads_2.fq.gz \\\n  --genome_lib_dir /path/to/CTAT_genome_lib \\\n  --FusionInspector validate \\\n  --denovo_reconstruct \\\n  --examine_coding_effect\n```\n\nThis produces a `FusionInspector-validate/` subdirectory with refined predictions (`finspector.fusion_predictions.final`) and optional de novo–assembled contigs (`finspector.gmap_trinity_GG.fusions.fasta`).\n\n---\n\n### 5. Additional Details\n\n- **Memory requirements**: Running STAR-Fusion from FASTQ typically needs ~40GB of RAM (STAR alignment). The post-processing steps themselves need far less.\n- **CTAT resource library**: Must contain the genome plus specialized annotation files for known fusions, repeats, etc.\n- **Filtering**: Low-support fusions can be noise; adjusting parameters like `--min_FFPM` can remove artifactual events.\n- **Visualization**: Output files (contig BAM, bed files) can be loaded into IGV or used via an HTML-based viewer (e.g., igv-reports) for analyzing supporting read evidence.\n\n---\n\n### 6. Summary of Recommended Steps\n\n1. **Build or download** a CTAT resource library (contains genome + fusion metadata).\n2. **Run STAR-Fusion**\n   - From **FASTQ**: specify `--left_fq [--right_fq]`.\n   - Using **existing STAR alignment**: ensure recommended chimeric parameters, then feed `Chimeric.out.junction` via `-J`.\n3. **Review outputs** in `star_fusion_outdir`, focusing on:\n   - `star-fusion.fusion_predictions.tsv`/`.abridged.tsv`\n   - Optional `FusionInspector-validate/` for more in-depth data.\n4. **Use additional flags** like `--examine_coding_effect` or `--denovo_reconstruct` to deepen analysis.\n\nWith these steps and parameters, you can accurately detect and annotate fusion transcripts. FusionInspector can further validate or assemble the fusions for additional confidence, and outputs are easily viewed in IGV or other genomic viewers.\n",
        "metadata": {
            "source": "STAR-Fusion ",
            "page": 24
        }
    },
    {
        "content": "GO/KEGG enrichment analysis First, install the required packages.\n\n#source(\"https://bioconductor.org/biocLite.R\")\n#biocLite(\"DOSE\")\n#biocLite(\"topGO\")\n#biocLite(\"clusterProfiler\")\n#biocLite(\"pathview\")\n\nLoad the packages:\n\nlibrary(DOSE)\nlibrary(org.Hs.eg.db)\nlibrary(topGO)\nlibrary(clusterProfiler)\nlibrary(pathview)\n\nImport the data. The raw data is a gene list with one column (with a header):\n\ndata <- read.table(\"gene.list\", header=TRUE)\ndata$GeneName <- as.character(data$GeneName)\n\nConvert gene names using org.Hs.eg.db:\n\ntransID = bitr(data$GeneName,\n  fromType = \"SYMBOL\",\n  toType = c(\"ENSEMBL\", \"ENTREZID\"),\n  OrgDb = \"org.Hs.eg.db\"\n)\n\nCreate folders to store the results:\n\ndir.create(\"GO\")\ndir.create(\"KEGG\")\n\nGO_CC annotation:\n\nCC <- enrichGO(transID$ENTREZID,\n  \"org.Hs.eg.db\",\n  keyType = \"ENTREZID\",\n  ont = \"CC\",\n  pvalueCutoff = 0.05,\n  pAdjustMethod = \"BH\",\n  qvalueCutoff = 0.1\n)\nCC <- setReadable(CC, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_CC_Dotplot.svg\", bg = \"transparent\")\ndotplot(CC, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_CC\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_CC_Barplot.svg\", bg = \"transparent\")\nbarplot(CC, showCategory = 12, title = \"GO_CC\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_CC_Network.svg\", bg = \"transparent\")\nplotGOgraph(CC)\ndev.off()\n\nwrite.table(as.data.frame(CC@result), file = \"./GO/GO_CC.xls\", sep = \"\\t\", row.names = F)\n\nGO_MF annotation:\n\nMF <- enrichGO(transID$ENTREZID, \"org.Hs.eg.db\", keyType = \"ENTREZID\", ont = \"MF\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nMF <- setReadable(MF, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_MF_Dotplot.svg\", bg = \"transparent\")\ndotplot(MF, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_MF\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_MF_Barplot.svg\", bg = \"transparent\")\nbarplot(MF, showCategory = 12, title = \"GO_MF\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_MF_Network.svg\", bg = \"transparent\")\nplotGOgraph(MF)\ndev.off()\n\nwrite.table(as.data.frame(MF@result), file = \"./GO/GO_MF.xls\", sep = \"\\t\", row.names = F)\n\nGO_BP annotation:\n\nBP <- enrichGO(transID$ENTREZID, \"org.Hs.eg.db\", keyType = \"ENTREZID\", ont = \"BP\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nBP <- setReadable(BP, OrgDb = org.Hs.eg.db)\n\nsvg(file = \"./GO/GO_BP_Dotplot.svg\", bg = \"transparent\")\ndotplot(BP, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"GO_BP\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./GO/GO_BP_Barplot.svg\", bg = \"transparent\")\nbarplot(BP, showCategory = 12, title = \"GO_BP\", font.size = 8)\ndev.off()\n\nsvg(file = \"./GO/GO_BP_Network.svg\", bg = \"transparent\")\nplotGOgraph(BP)\ndev.off()\n\nwrite.table(as.data.frame(BP@result), file = \"./GO/GO_BP.xls\", sep = \"\\t\", row.names = F)\n\nKEGG annotation:\n\nkegg <- enrichKEGG(transID$ENTREZID, organism = \"hsa\", pvalueCutoff = 0.05, pAdjustMethod = \"BH\", qvalueCutoff = 0.1)\nkegg <- setReadable(kegg, OrgDb = org.Hs.eg.db, keytype = \"ENTREZID\")\n\nsvg(file = \"./KEGG/KEGG_Dotplot.svg\", bg = \"transparent\")\ndotplot(kegg, showCategory = 12, colorBy = \"pvalue\", font.size = 8, title = \"KEGG\") # + theme(axis.text.y = element_text(angle = 45))\ndev.off()\n\nsvg(file = \"./KEGG/KEGG_Barplot.svg\", bg = \"transparent\")\nbarplot(kegg, showCategory = 12, title = \"KEGG\", font.size = 8)\ndev.off()\n\nwrite.table(as.data.frame(kegg@result), file = \"./KEGG/kegg.xls\", sep = \"\\t\", row.names = F)\n\nCreate a directory for KEGG pathway maps:\n\ndir.create(\"./KEGG/MAP\")\nkegg_df = as.data.frame(kegg)\n\nfor(i in kegg_df$ID){\n  pathview(gene.data = transID$ENTREZID,\n           pathway.id = i,\n           species = \"hsa\",\n           kegg.native = TRUE,\n           kegg.dir = \"./KEGG/MAP\"\n  )\n}\n\nprint(\"TASK DONE\")",
        "metadata": {
            "source": "GO/KEGG enrichment analysis ",
            "page": 25
        }
    },
    {
        "content": "CHIP-seq Functional enrichment analysis\nOnce we have obtained the annotated nearest genes, we can perform functional enrichment analysis to identify predominant biological themes among these genes by incorporating biological knowledge provided by biological ontologies. For instance, Gene Ontology (GO) (Ashburner et al. 2000) annotates genes to biological processes, molecular functions, and cellular components in a directed acyclic graph structure; Kyoto Encyclopedia of Genes and Genomes (KEGG) (Kanehisa et al. 2004) annotates genes to pathways; Disease Ontology (DO) (Schriml et al. 2011) annotates genes with human disease association; and Reactome (Croft et al. 2013) annotates genes to pathways and reactions.\n\nChIPseeker also provides a function, seq2gene, for linking genomic regions to genes in a many-to-many mapping. It considers the host gene (exon/intron), promoter region, and flanking gene from intergenic regions that may be under cis-regulation. This function is designed to link both coding and non-coding genomic regions to coding genes and facilitate functional analysis.\n\nEnrichment analysis is a widely used approach to identify biological themes. Several Bioconductor packages have been developed to investigate whether the number of selected genes associated with a particular biological term is larger than expected. These include DOSE (Yu et al. 2015) for Disease Ontology, ReactomePA for Reactome pathway analysis, and clusterProfiler (Yu et al. 2012) for Gene Ontology and KEGG enrichment analysis.\n\nlibrary(ReactomePA)\n\npathway1 <- enrichPathway(as.data.frame(peakAnno)$geneId)\nhead(pathway1, 2)\n##                          ID        Description GeneRatio   BgRatio RichFactor\n## R-HSA-9830369 R-HSA-9830369 Kidney development    19/499  46/11146  0.4130435\n## R-HSA-9758941 R-HSA-9758941       Gastrulation    27/499 125/11146  0.2160000\n##               FoldEnrichment    zScore       pvalue     p.adjust       qvalue\n## R-HSA-9830369       9.226017 12.102741 2.252177e-14 2.360281e-11 2.351747e-11\n## R-HSA-9758941       4.824721  9.309385 5.773075e-12 3.025091e-09 3.014153e-09\n##                                                                                                                                                   geneID\n## R-HSA-9830369                                          2625/5076/7490/652/6495/2303/3975/6928/10736/5455/7849/3237/6092/2122/255743/2296/3400/28514/2138\n## R-HSA-9758941 5453/5692/5076/5080/7546/3169/652/5015/2303/5717/3975/2627/5714/344022/7849/5077/2637/7022/8320/7545/6657/4487/51176/2296/28514/2626/64321\n##               Count\n## R-HSA-9830369    19\n## R-HSA-9758941    27\ngene <- seq2gene(peak, tssRegion = c(-1000, 1000), flankDistance = 3000, TxDb = txdb)\npathway2 <- enrichPathway(gene)\nhead(pathway2, 2)\n##                          ID        Description GeneRatio   BgRatio RichFactor\n## R-HSA-9830369 R-HSA-9830369 Kidney development    17/408  46/11146  0.3695652\n## R-HSA-9758941 R-HSA-9758941       Gastrulation    24/408 125/11146  0.1920000\n##               FoldEnrichment    zScore       pvalue     p.adjust       qvalue\n## R-HSA-9830369      10.096014 12.049721 1.806589e-13 1.765037e-10 1.709604e-10\n## R-HSA-9758941       5.245176  9.303549 1.834131e-11 8.959732e-09 8.678338e-09\n##                                                                                                                                    geneID\n## R-HSA-9830369                                      2625/5076/3227/652/6495/2303/3975/3237/6092/2122/255743/7490/6928/7849/5455/2296/28514\n## R-HSA-9758941 5453/5692/5076/7546/3169/652/2303/5717/3975/2627/5714/344022/2637/8320/7545/7020/2626/5080/5015/7849/51176/2296/64321/28514\n##               Count\n## R-HSA-9830369    17\n## R-HSA-9758941    24\ndotplot(pathway2)",
        "metadata": {
            "source": "CHIP-seq Functional enrichment ",
            "page": 26
        }
    },
    {
        "content": "# DESeq2 Differential Expression Analysis Tutorial\n\nDESeq2 is an R package designed for normalization, visualization, and differential expression analysis of high-dimensional count data. It uses empirical Bayes techniques to estimate log₂ fold changes and dispersion priors, and then calculates their posterior values. Originally released in 2014 by Professor Michael Love from the University of North Carolina, DESeq2 continues to be updated and is one of the most widely used tools for differential expression analysis.\n\n---\n\n## 1. Installation\n\n### In R\n\nRun the following commands in the R console or RStudio (for R version 4.2 and above):\n\n```r\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DESeq2\")\n```\n\n### In a conda Environment\n\nUse the following command:\n\n```bash\nconda install -c bioconda bioconductor-deseq2\n```\n\n---\n\n## 2. Workflow Overview\n\nThis tutorial covers the following steps:\n\n1. **Data Import and Preprocessing**: \n   - Load the count matrix (e.g., `gene_count_matrix.csv`).\n   - Perform data cleaning, such as fixing gene names with extra information and removing invalid rows.\n   - Filter out genes with low expression.\n\n2. **Differential Expression Analysis**: \n   - Create group and sample information.\n   - Build a `DESeqDataSet` object from the count matrix.\n   - Perform normalization and statistical testing.\n   - Extract and sort results by p-value and log₂ fold change, and filter significant genes.\n\n3. **Visualization**: \n   - Display expression for a single gene.\n   - Generate a heatmap of differentially expressed genes.\n   - Create a volcano plot to visualize overall differential expression.\n\n---\n\n## 3. Data Import and Preprocessing\n\nFirst, load the required R packages and read the expression count matrix. Assume the file `gene_count_matrix.csv` contains gene IDs as row names.\n\n```r\n# Load necessary libraries\nlibrary(DESeq2)\nlibrary(pheatmap)  # For heatmaps\nlibrary(ggplot2)   # For plotting\n\n# Read the count matrix (set your working directory accordingly)\ncountData <- as.matrix(read.csv(\"gene_count_matrix.csv\", row.names = \"gene_id\"))\n```\n\n**Note:**\n\n- The gene names might include extra information, and there could be invalid rows. Use data cleaning functions in R as needed.\n- Since many genes might show no expression (or zero counts), you can filter out these low-expression genes:\n\n```r\ncountData <- countData[rowMeans(countData) > 1, ]  # Remove genes with very low expression\n```\n\n---\n\n## 4. Differential Expression Analysis\n\n### 4.1 Preparing Group Information\n\nAssume you have three control samples and three osmotic-treated samples. Create a factor vector for the condition and a sample information data frame (`colData`) with row names matching the columns in `countData`.\n\n```r\ncondition <- factor(c(rep(\"control\", 3), rep(\"osmotic\", 3)))\ncolData <- data.frame(row.names = colnames(countData), condition)\n```\n\n### 4.2 Creating the DESeqDataSet Object\n\nUse the `DESeqDataSetFromMatrix` function to create the object:\n\n```r\ndds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ condition)\nhead(dds)  # View the structure of the DESeqDataSet\n```\n\n### 4.3 Normalization and Statistical Testing\n\nNormalize the data and perform differential expression testing:\n\n```r\ndds1 <- DESeq(dds, fitType = 'mean', minReplicatesForReplace = 7, parallel = FALSE)\nres <- results(dds1)\nsummary(res)\n```\n\nThe output will display the number of upregulated and downregulated genes, along with counts for outliers and low-count genes.\n\n### 4.4 Organizing and Filtering Results\n\nConvert the results to a data frame, sort by p-value and log₂ fold change, and filter for significant genes (e.g., p-value < 0.05 and |log₂ fold change| ≥ 1):\n\n```r\nres1 <- data.frame(res, stringsAsFactors = FALSE, check.names = FALSE)\nres1 <- res1[order(res1$pvalue, res1$log2FoldChange, decreasing = c(FALSE, TRUE)), ]\n\n# Filter for upregulated and downregulated genes\nres1_up <- res1[which(res1$log2FoldChange >= 1 & res1$pvalue < 0.05), ]\nres1_down <- res1[which(res1$log2FoldChange <= -1 & res1$pvalue < 0.05), ]\nres1_total <- rbind(res1_up, res1_down)\n```\n\n---\n\n## 5. Visualization\n\n### 5.1 Displaying a Single Gene's Expression\n\nYou can quickly visualize the expression of a specific gene using DESeq2's `plotCounts` function:\n\n```r\nplotCounts(dds1, gene = \"AT4G38770\", intgroup = \"condition\")\n```\n\nAlternatively, customize the plot using ggplot2:\n\n```r\nd <- data.frame(t(subset(countData, rownames(countData) == \"AT4G38770\")))\nggplot(d, aes(x = condition, y = AT4G38770, color = condition)) +\n  geom_point(position = position_jitter(w = 0.2, h = 0)) +\n  geom_text_repel(aes(label = rownames(d))) +\n  theme_bw() +\n  ggtitle(\"AT4G38770\") +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n\n### 5.2 Creating a Heatmap\n\nUse the pheatmap package to cluster and display the expression of differentially expressed genes:\n\n```r\n# Extract the differential genes from the count matrix\n df <- countData[intersect(rownames(countData), rownames(res1_total)), ]\n df2 <- as.matrix(df)\n pheatmap(df2,\n          show_rownames = FALSE,\n          show_colnames = TRUE,\n          cluster_cols = FALSE,\n          cluster_rows = TRUE,\n          height = 10,\n          scale = \"row\",\n          fontsize = 10,\n          angle_col = 45, \n          color = colorRampPalette(c(\"#8854d0\", \"#ffffff\", \"#fa8231\"))(100),\n          clustering_method = 'single'\n )\n```\n\n### 5.3 Drawing a Volcano Plot\n\nUse ggplot2 to create a volcano plot that displays the log₂ fold changes against -log₁₀ (adjusted p-values):\n\n```r\ngenes <- res1\n# Assign colors based on upregulation, downregulation, or no significant change\n genes$color <- ifelse(genes$padj < 0.05 & abs(genes$log2FoldChange) >= 1,\n                        ifelse(genes$log2FoldChange > 1, 'red', 'blue'),\n                        'gray')\n color <- c(red = \"red\", gray = \"gray\", blue = \"blue\")\n\n p <- ggplot(genes, aes(log2FoldChange, -log10(padj), col = color)) +\n   geom_point() +\n   theme_bw() +\n   scale_color_manual(values = color) +\n   labs(x = \"log2 (fold change)\", y = \"-log10 (q-value)\") +\n   geom_hline(yintercept = -log10(0.05), lty = 4, col = \"grey\", lwd = 0.6) +\n   geom_vline(xintercept = c(-1, 1), lty = 4, col = \"grey\", lwd = 0.6) +\n   theme(legend.position = \"none\",\n         panel.grid = element_blank(),\n         axis.title = element_text(size = 16),\n         axis.text = element_text(size = 14)) +\n   geom_text_repel(\n     data = subset(genes, padj < 1e-100 & abs(genes$log2FoldChange) >= 10),\n     aes(label = rownames(genes)),\n     size = 5,\n     box.padding = unit(0.35, \"lines\"),\n     point.padding = unit(0.3, \"lines\")\n   )\n p\n```\n\n---\n\n## 6. A Complete DESeq2 Example in RStudio\n\nBelow is a complete example code that can be executed in RStudio:\n\n```r\n# Clear the workspace and set the working directory\nrm(list = ls())\nsetwd('/XX/XX/XX')\nif (!dir.exists('./01_DEGs')) {\n  dir.create('./01_DEGs')\n}\nsetwd('./01_DEGs/')\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(DESeq2)\n\n# Import the processed expression matrix (row names are gene symbols, columns are sample IDs)\ndat <- read.csv('../../00_rawdata/GSE203346/dat.GSE203346_count.csv', check.names = FALSE, row.names = 1)\n\n# Import the group information\ncolData <- read.csv('../../00_rawdata/GSE203346/group.GSE203346.csv')\ncolData$group <- factor(colData$group, levels = c(\"control\", \"disease\"))\nprint(table(colData$group))\n# Ensure the samples in the expression matrix match the group information\n dat <- dat[, colData$sample]\n\n# Create the DESeqDataSet object\ndds <- DESeqDataSetFromMatrix(countData = dat, colData = colData, design = ~ group)\n# Filter out genes with low counts (genes with counts ≤ 1 in all samples)\ndds <- dds[rownames(counts(dds)) > 1, ]\n# Estimate size factors to normalize for sequencing depth\ndds <- estimateSizeFactors(dds)\n\n# Perform differential expression analysis\ndds <- DESeq(dds)\nres <- results(dds, contrast = c(\"group\", \"disease\", \"control\"))\nDEG <- as.data.frame(res)\n\n# Add labels for differential expression (criteria: |log2FoldChange| ≥ 0.5 & p-value < 0.05)\nDEG$change <- as.factor(\n  ifelse(DEG$pvalue < 0.05 & abs(DEG$log2FoldChange) >= 0.5,\n         ifelse(DEG$log2FoldChange > 0.5, 'Up', 'Down'), 'Not')\n)\nprint(table(DEG$change))\nDEG_write <- cbind(symbol = rownames(DEG), DEG)\n\n# Filter for significantly differentially expressed genes\nsig_diff <- subset(DEG, DEG$pvalue < 0.05 & abs(DEG$log2FoldChange) >= 0.5)\nsig_diff_write <- cbind(symbol = rownames(sig_diff), sig_diff)\n\n# Save the results\nwrite.csv(DEG_write, file = 'DEG_all.csv')\nwrite.csv(sig_diff_write, file = 'DEG_sig.csv')\n```\n\n**Explanation:**\n\n- **Data Import:** The expression matrix and group information are imported, ensuring that the sample orders match.\n- **DESeqDataSet Creation:** The count data, sample information, and experimental design (e.g., control vs. disease) are used to create a DESeqDataSet object.\n- **Filtering & Normalization:** Lowly expressed genes are removed, and size factors are estimated to normalize the data.\n- **Differential Expression:** The `DESeq` function performs the analysis, and results are extracted and organized.\n- **Annotation & Saving:** Differential expression labels (Up, Down, Not) are added based on the criteria, and results are saved to CSV files for further analysis or visualization.\n\n---\n\n## 7. Summary\n\nThis tutorial provides a detailed guide for performing RNA-seq differential expression analysis using DESeq2. It covers:\n\n- Data import and preprocessing to prepare count data for analysis.\n- Normalization and statistical testing to identify differentially expressed genes.\n- Visualization techniques including single-gene expression plots, heatmaps, and volcano plots.\n\nBy following these steps, researchers can construct a robust differential expression analysis pipeline and tailor parameters to suit their experimental design, ultimately drawing reliable biological conclusions.",
        "metadata": {
            "source": "DESeq2",
            "page": 27
        }
    },
    {
        "content": "# SortMeRNA Usage Documentation\n\nSortMeRNA is a tool designed for detecting and filtering out rRNA reads from sequencing libraries. It supports both single-end and paired-end data, and automatically recognizes file formats and compression (e.g. plain fasta/fastq or gzipped files).\n\n---\n\n## 1. Installation\n\n### Using conda\n\nInstall SortMeRNA with the following command:\n\n```bash\nconda install sortmerna\n```\n\nVerify the installation by checking the version:\n\n```bash\n$ sortmerna --version\nSortMeRNA version 4.3.2\nBuild Date: Apr  2 2021\n```\n\n---\n\n## 2. Basic Usage\n\nThe only required options are `--ref` and `--reads`. Options can be specified using a single dash (e.g. `-ref` and `-reads`). File extensions are optional because the format and compression are automatically recognized.\n\n### Example Commands\n\n**Single Reference and Single Reads File**:\n\n```bash\nsortmerna --ref REF_PATH --reads READS_PATH\n```\n\n**Multiple References**:\n\n```bash\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH\n```\n\n**Paired-End Reads**:\n\n```bash\nsortmerna --ref REF_PATH_1 --ref REF_PATH_2 --ref REF_PATH_3 --reads READS_PATH_1 --reads READS_PATH_2\n```\n\n---\n\n## 3. Example: Identifying Bacterial 16S rRNA\n\nThe following command identifies rRNA in the file `DRR110568_1.fastq` using the bacterial 16S rRNA database. rRNA-matched sequences are saved to `DRR110568.1.16s` while non-matched sequences are saved to `DRR110568.1.non.16s`.\n\n```bash\n./sortmerna \\\n  --ref rRNA_databases/silva-bac-16s-id90.fasta,/index/silva-bac-16s-db: \\\n  --reads DRR110568_1.fastq \\\n  --aligned DRR110568.1.16s \\    # rRNA-matched sequences\n  --sam --num_alignments 1 --fastx \\\n  --other DRR110568.1.non.16s \\   # non-rRNA sequences\n  --log -v\n```\n\n---\n\n## 4. Indexing rRNA Databases\n\nTo avoid re-indexing at every run, pre-build the index for your rRNA databases. Once the index is built, copy the generated `idx/` directory to your working directory.\n\n### Indexing Command Example:\n\n```bash\nsortmerna --index 1 --threads 32 \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-16s-id90.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-16s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-18s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-28s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5s-database-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5.8s-database-id98.fasta \\\n  --workdir ./\n```\n\nAfter indexing, the working directory will include the following folders:\n\n```\nWORKDIR/\n  idx/    (index files)\n  kvdb/   (key-value alignment results)\n  out/    (output files)\n  readb/  (preprocessed reads index)\n```\n\n> **Note:** Ensure the `kvdb/` directory is empty before each run to avoid errors.\n\n---\n\n## 5. Removing rRNA Reads from Paired-End Data\n\nThe following command removes rRNA reads from paired-end sequencing data. The output files will retain the input format (e.g. `.fq.gz`).\n\n```bash\nsortmerna --index 0 --threads 48 \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-16s-id90.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-bac-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-16s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-arc-23s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-18s-id95.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/silva-euk-28s-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5s-database-id98.fasta \\\n  --ref ~/sortmerna4.3/rRNA_databases/rfam-5.8s-database-id98.fasta \\\n  --workdir ./ \\\n  --reads NR1.fq.gz \\\n  --reads NR2.fq.gz \\\n  --aligned \"NR/N_rRNA\" --other \"NR/N_clean\" --paired_in --fastx --out2\n```\n\n### Expected Output\n\nAfter running, the output directory (e.g. `NR/`) should contain files such as:\n\n```\nN_clean_fwd.fq.gz  N_clean_rev.fq.gz\nN_rRNA_fwd.fq.gz   N_rRNA_rev.fq.gz\nN_rRNA.log\n```\n\nThe log file provides statistics on the total reads classified as rRNA versus non-rRNA, along with coverage by each reference database.\n\n---\n\n## 6. Key Parameters and Options\n\n- **--ref**: Specify the reference fasta file. Use multiple times for multiple databases.\n- **--reads**: Input reads file. For paired-end data, provide this option twice.\n- **--aligned**: Output for reads that match the rRNA database. Can include a path and prefix for the output file.\n- **--other**: Output for reads that do not match the rRNA database.\n- **--index**: Controls indexing behavior:\n  - `--index 1`: Only build the index.\n  - `--index 0`: Skip indexing (requires an existing index in the `idx/` directory).\n  - Default (or `--index 2`): Automatically detect or build the index if not found.\n- **--sam**: Output alignments in SAM format.\n- **--fastx**: Output in fasta/fastq format, matching the input format.\n- **--num_alignments INT**: Specifies the number of alignments to output. Use `1` for faster filtering; higher values increase runtime.\n- **--paired_in / --paired_out**: Control output handling for paired-end reads.\n  - `--paired_in`: Outputs non-rRNA reads to the `other` file (although a few rRNA reads might be included).\n  - `--paired_out`: Ensures rRNA reads appear only in the aligned file (with a chance of non-rRNA reads also appearing in the other file).\n\n---\n\n## 7. Advanced Options\n\nSortMeRNA includes additional parameters for fine-tuning alignment sensitivity and speed:\n\n- **--num_seeds INT**: Minimum number of seed matches required (default: 2).\n- **passes INT,INT,INT**: Adjusts seed search passes using decreasing intervals (default: L, L/2, L/3).\n- **--edges INT(%)**: Extends the alignment region on the reference by a set number of nucleotides (default: 4).\n- **--full_search**: Disables early termination upon finding 0-error seed matches, which can increase sensitivity but decrease speed up to four-fold.\n- **--pid**: Appends the process ID to output filenames to avoid overwriting files from concurrent runs.\n\n---\n\n## 8. Summary\n\nSortMeRNA is a powerful tool for rRNA detection and removal from sequencing libraries. It offers:\n\n- Automatic recognition of input file formats and compression.\n- Support for both single-end and paired-end data.\n- Flexible parameter settings for balancing speed and sensitivity.\n- Compatibility with standard rRNA databases such as SILVA and Rfam.\n\nWhile SortMeRNA is highly sensitive, for quick rRNA estimation some users might prefer aligners like STAR or Bowtie2. However, for comprehensive rRNA filtering, SortMeRNA remains an effective solution.\n",
        "metadata": {
            "source": "SortMeRNA",
            "page": 28
        }
    },
    {
        "content": "GeneFEAST run.yml Configuration Instructions must include:Step1: Use shell command to create the run.yml file. Description: You can create the configuration file using shell redirection with `cat << 'EOF' > run.yml`, followed by the structured YAML content, and then closed with `EOF`. This approach allows easy inline editing in terminal or scripts. Step2: Fill in configuration content. Description: The configuration consists of FEA result references, gene-of-interest (GOI) files, optional keyword annotations, and tooltips options. The following fields must be specified: \n\nFEAs:\n    - id: \"mmc2_3h\"\n      goi_file: \"./GeneFEAST/mmc2_goi_3h.csv\"\n      fea_file: \"./GeneFEAST/GO_BP_3h_0.0001_4GF_basic.csv\"\n\n    - id: \"mmc2_24h\"\n      goi_file: \"./GeneFEAST/mmc2_goi_24h.csv\"\n      fea_file: \"./GeneFEAST/GO_BP_24h_0.0001_4GF_basic.csv\"\n\nSEARCH_WORDS:\n- Covid 19\n- sars-cov-2\n\nEA_FILE: ./GeneFEAST/mmc2_3h_24h_extra_annotations.txt\n\nTOOLTIPS: True\n\nDescription of Fields: \n- id: A unique identifier for each FEA dataset, such as \"mmc2_3h\" or \"mmc2_24h\".\n- goi_file: Path to the CSV file containing the gene-of-interest list.\n- fea_file: Path to the functional enrichment analysis result CSV file.\n- SEARCH_WORDS: List of keywords used to highlight or search annotations (e.g., disease-related terms).\n- EA_FILE: Path to an additional annotations file linking gene-level metadata across datasets.\n- TOOLTIPS: Boolean to enable or disable additional visual tooltips in the final report.\n\nTools Used: Bash command line with heredoc syntax, manual editing of YAML fields.",
        "metadata": {
          "source": "GeneFEAST_run.yml_config",
          "page": 29
        }
      },
      {
        "content": "MAFin Usage Instructions must include:Step1: Navigate to Output Directory. Description: Before executing the MAFin command, change the working directory to the expected output path using `cd`. MAFin will write output files to the current working directory. If your MAF input file is specified using a relative path (e.g., `./data/xxx`), adjust it relative to your new working directory, such as `../../data/xxx`. Step2: Run MAFin command. Description: Execute the MAFin tool with the desired parameters. The command-line must specify exactly one search type (`--regexes`, `--kmers`, or `--jaspar_file`). Additional options include strand orientation, number of processes, and report format. Example Command: \n\n```bash\nMAFin ../../data/sample.maf \\\n      --regexes \"CTGCCCGCA\" \"AGT\" \\\n      --search_in reference \\\n      --reverse_complement no \\\n      --processes 4 \\\n      --detailed_report\n```\n\nExpected Output: All result files will be written to the current directory, including:\n- `regex_search_motif_hits.bed`\n- `ref_genome_regex_search_AGT_motif_hits.json`\n- `ref_genome_regex_search_AGT_motif_hits.csv` (if `--detailed_report` is used).\n\nCommand-Line Arguments:\n- `maf_file` (Required): Path to the uncompressed MAF file.\n- `--genome_ids`: Path to a genome IDs file (optional).\n- `--search_in`: Genome to search motifs in ('reference' [default] or 'all').\n- `--reverse_complement`: Search both strands if 'yes'. Choices: 'yes' or 'no' (default: 'no').\n- `--pvalue_threshold`: P-value threshold for PWM matches (default: 1e-4).\n- `--processes`: Number of parallel processes (default: 1).\n- `--background_frequencies`: Four space-separated floats summing to 1 (e.g., 0.25 0.25 0.25 0.25).\n- `--purge_results_dir`: Purge the 'tmp' directory before execution.\n- `--verbose, -v`: Enable verbose logging to `debug.log`.\n- `--detailed_report, -d`: Generate merged JSON and CSV analytical output.\n\nSearch Type Flags (Mutually Exclusive):\n- `--regexes`: Provide one or more regex motifs as arguments.\n- `--kmers`: Path to a text file containing K-mers (one per line).\n- `--jaspar_file`: Path to a JASPAR-formatted PWM file.\n\nTools Used: MAFin command-line interface, with support for regular expression, k-mer, and PWM motif scanning, parallel computing, and flexible output formats.",
        "metadata": {
          "source": "MAFin_execution",
          "page": 30
        }
      },
      {

            "content": "Transcriptome Annotation Comparison (Gencode vs Chess) using rnalib must include:Step1: Set test data directory. Description: Define the global test data path for rnalib by monkey-patching `rna.__RNALIB_TESTDATA__`. Generate necessary resources if missing by calling `rna.testdata.create_testdata`. Python code snippet:\n\ncat << 'EOF' > setup_testdata.py\nimport rnalib as rna\nrna.__RNALIB_TESTDATA__ = \"rnalib_testdata/\"\nrna.testdata.create_testdata(rna.__RNALIB_TESTDATA__, (rna.testdata.test_resources, rna.testdata.large_test_resources))\nEOF\n\nStep2: Load transcriptome data. Description: Instantiate `rna.Transcriptome` objects for Gencode and Chess annotations using canonical GRCh38 chromosomes and gene-type filtering ('protein_coding'). Use latest gene symbols via `gene_name_alias_file`. Example code:\n\ncat << 'EOF' > load_transcriptomes.py\nimport rnalib as rna\nalias_txt = rna.get_resource(\"gene_aliases\")\ngencode_gff = rna.get_resource(\"full_gencode_gff\")\nchess_gff = rna.get_resource(\"full_chess_gtf\")\n\ntxfilter = rna.TranscriptFilter(). \\\n    include_gene_types({'protein_coding'}). \\\n    include_chromosomes(rna.CANONICAL_CHROMOSOMES['GRCh38'])\n\nt_gc = rna.Transcriptome(\n    annotation_gff=gencode_gff,\n    annotation_flavour='gencode',\n    copied_fields=['gene_type', 'transcript_type'],\n    gene_name_alias_file=alias_txt,\n    feature_filter=txfilter)\n\nt_ch = rna.Transcriptome(\n    annotation_gff=chess_gff,\n    annotation_flavour='chess',\n    copied_fields=['gene_type'],\n    gene_name_alias_file=alias_txt,\n    feature_filter=txfilter)\nEOF\n\nStep3: Perform annotation comparison. Description: Use `rna.cmp_sets` to identify shared and unique genes. Example snippet:\n\ncat << 'EOF' > compare_annotations.py\nimport rnalib as rna\nshared, uniq_gc, uniq_ch = rna.cmp_sets({x.gene_name for x in t_gc.genes}, {x.gene_name for x in t_ch.genes})\nEOF\n\nStep4: Detailed gene coordinate comparison. Description: Utilize `rna.AnnotationIterator` to systematically identify differences in gene coordinates and names. Compute histograms for differences in start/end coordinates and visualize using matplotlib. Example code:\n\ncat << 'EOF' > detailed_comparison.py\nimport rnalib as rna\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nstats, hist_start, hist_end = Counter(), Counter(), Counter()\nwith rna.AnnotationIterator(\n    rna.TranscriptomeIterator(t_gc, feature_types='gene'),\n    rna.TranscriptomeIterator(t_ch, feature_types='gene')) as it:\n    for gene_gc, dat in it:\n        # coordinate comparison logic\n        pass  # full implementation omitted for brevity\nEOF\n\nTools Used: rnalib package, Python, matplotlib",
            "metadata": {
          "source": "rnalib_annotation_workflow",
          "page": 31
        }
      },
      {

            "content": "miRNA Annotation Comparison (MirGeneDB vs FlyBase) using rnalib must include:Step1: Load transcriptome data from MirGeneDB and FlyBase. Description: Instantiate transcriptomes for MirGeneDB and FlyBase annotations. Example snippet:\n\ncat << 'EOF' > load_mirna_transcriptomes.py\nimport rnalib as rna\n\nt_mg = rna.Transcriptome(\n    annotation_gff=rna.get_resource(\"mirgendb_dme_gff\"),\n    annotation_flavour='mirgenedb')\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    res = {\"flybase_full\": {\n        \"uri\": \"https://ftp.flybase.net/releases/FB2023_06/dmel_r6.55/gtf/dmel-all-r6.55.gtf.gz\",\n        \"filename\": \"dmel-all-r6.55.gtf.gz\"}}\n    rna.testdata.create_testdata(tmpdir, res)\n    t_fb = rna.Transcriptome(\n        annotation_gff=rna.get_resource(\"flybase_full\", tmpdir, res),\n        annotation_flavour='flybase')\nEOF\n\nStep2: Exact and overlap-based annotation mapping. Description: Map MirGeneDB and FlyBase annotations based on exact location and overlap using rnalib iterators. Example code snippet:\n\ncat << 'EOF' > mirna_annotation_mapping.py\nimport rnalib as rna\n\n# Exact mapping\nmapping_exact = {name: t_mg_mir.get(mir, None) for mir, name in t_fb_mir.items()}\n\n# Overlap mapping\nmapping_overlap = {}\nwith rna.it(rna.it(t_fb_mir), anno_its=rna.it(t_mg_mir)) as it:\n    for loc, (fb_mir, mg_mir) in it:\n        mapping_overlap[fb_mir] = [x.data for x in mg_mir]\nEOF\n\nStep3: Visualize annotations with IGV browser. Description: Export selected annotations to BED format and display in IGV browser. Example snippet:\n\ncat << 'EOF' > igv_browser_display.py\nimport rnalib as rna\nimport tempfile, igv_notebook\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    bed_file = f\"{tmpdir}/annotations.bed\"\n    with open(bed_file, 'wt') as out:\n        rna.it({\n            'hpRNA:CR46343': t_fb['hpRNA:CR46343'],\n            'gene_Dme-Mir-997_3p*': t_mg['gene_Dme-Mir-997_3p*'],\n            'gene_Dme-Mir-997_5p': t_mg['gene_Dme-Mir-997_5p']\n        }).to_bed(out)\n    igv_notebook.init()\n    browser = igv_notebook.Browser({\n        \"genome\": \"dm6\",\n        \"tracks\": [{\"name\": \"BED\", \"url\": bed_file, \"format\": \"bed\"}]\n    }).search('hpRNA:CR46343')\nEOF\n\nTools Used: rnalib, Python, IGV Browser (igv_notebook)",
            "metadata": {
        "source": "rnalib_annotation_workflow",
        "page": 32
        }
    },
    {
        "content": "import scanpy as sc; import nichepca as npc; # Run nichePCA on raw counts with default normalization pipeline (norm -> log1p -> agg -> pca); npc.wf.nichepca(adata, knn=25); # Use nichePCA result for neighbor graph and Leiden clustering; sc.pp.neighbors(adata, use_rep=\"X_npca\"); sc.tl.leiden(adata, resolution=0.5); # If analyzing multiple samples, specify the sample key; npc.wf.nichepca(adata, knn=25, sample_key=\"sample\"); # If cell type labels exist, run nichePCA using cell-type defined niches; npc.wf.nichepca(adata, knn=25, obs_key='cell_type'); # Customize the normalization pipeline, e.g., skip median normalization; npc.wf.nichepca(adata, knn=25, pipeline=[\"log1p\", \"agg\", \"pca\"]); # Apply PCA before aggregation step; npc.wf.nichepca(adata, knn=25, pipeline=[\"norm\", \"log1p\", \"pca\", \"agg\"]); # Run nichePCA without PCA step; npc.wf.nichepca(adata, knn=25, pipeline=[\"norm\", \"log1p\", \"agg\"]); # Higher knn values (e.g., 25) work well in brain data; lower knn values (e.g., 10) suit kidney data; # Install the latest version via GitHub; pip install git+https://github.com/imsb-uke/nichepca.git@main; # Example: testing Leiden clustering with multiple resolutions; import numpy as np; import pandas as pd; import scanpy as sc; from utils import generate_dummy_adata; import nichepca as npc; def test_leiden_multires(): adata = generate_dummy_adata(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); sc.pp.pca(adata); sc.pp.neighbors(adata, use_rep=\"X_pca\"); resolutions = np.linspace(0.1, 1.0, 2); results_parallel = npc.cl.leiden_multires(adata, resolutions, parallel=True, n_jobs=2); results_sequential = npc.cl.leiden_multires(adata, resolutions, parallel=False); assert isinstance(results_parallel, pd.DataFrame); assert results_parallel.shape == (adata.shape[0], len(resolutions)); assert results_parallel.equals(results_sequential); npc.cl.leiden_multires(adata, resolutions, parallel=False, return_leiden=False); assert f\"leiden_{resolutions[0]}\" in adata.obs.columns; assert f\"leiden_{resolutions[1]}\" in adata.obs.columns; # Example: constrain number of clusters; def test_leiden_with_nclusters(): adata = generate_dummy_adata(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); sc.pp.pca(adata); sc.pp.neighbors(adata, use_rep=\"X_pca\"); n_clusters = 5; npc.cl.leiden_with_nclusters(adata, n_clusters); assert adata.obs[\"leiden\"].nunique() == n_clusters",
        "metadata": {
        "source": "NichePCA_tutorial",
        "page": 33
        }
    },
    {
        "content": "Tutorial for wgatools: wgatools is a cross-platform and ultrafast toolkit for manipulating Whole Genome Alignment files (MAF, PAF, CHAIN formats). Basic usage starts with 'wgatools <COMMAND>' and each command supports '-h' for help. Common conversions include: 'wgatools maf2paf test.maf > test.paf', or reverse with 'wgatools paf2maf test.paf --target target.fa --query query.fa > test.maf'. Piping is supported: 'cat test.maf | wgatools maf2paf | wgatools paf2maf -g target.fa -q query.fa > test.maf'. For visualizations, 'wgatools dotplot -f paf test.paf > out.html' generates base-level dotplots, or use '-m overview' for summary plots: 'wgatools dotplot test.maf -m overview > overview.html'. Extract specific genomic regions from MAF using indexed access: 'wgatools maf-index test.maf' and 'wgatools maf-ext test.maf -r chr1:1-100'. To browse MAF interactively in terminal: 'wgatools tview test.maf'. Variant calling from MAF or PAF (with genome fasta) is supported: 'wgatools call test.maf -s -l0' or 'wgatools call test.paf -s -l0 --target target.fa --query query.fa -f paf'. Splitting large MAF blocks by length: 'wgatools chunk -l 100 test.maf -o chunked.maf'. Generate alignment statistics: 'wgatools stat test.maf' or 'wgatools stat -f paf test.paf'. Validate or fix PAF inconsistencies: 'wgatools validate wrong.paf' and optionally output fixed version: 'wgatools validate wrong.paf -f happy.paf'. Filter alignment blocks by block length or query size: 'wgatools filter test.maf -q 1000000 > filt.maf' or filter all-to-all alignments: 'wgatools filter all2all.paf -a 1000000 > filt.paf'. Rename sequences to distinguish target/query: 'wgatools rename --prefixs REF.,QUERY. input.maf > rename.maf'. Compute coverage for all-to-all alignments: 'wgatools pafcov all.paf > all.cov.beds'. Generate pseudo-MAF for divergence analysis: 'wgatools pafpseudo -f all.fa.gz all.paf -o out_dir -t 10'. Enable auto-completion: 'wgatools gen-completion --shell fish > ~/.config/fish/completions/wgatools.fish'. Version: 0.1.0, Author: Wenjie Wei.",
        "metadata": {
        "source": "NichePCA_tutorial",
        "page": 34
        }
    },
    {
        "content": "Squidpy:# Step 1: Load the example dataset\nadata = sq.datasets.slideseqv2()\n\n# Step 2: Preprocess the data\nsc.pp.filter_genes(adata, min_cells=3)\nsc.pp.normalize_total(adata)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat\", n_top_genes=2000)\nadata = adata[:, adata.var.highly_variable]\n\n# Step 3: Calculate spatial neighbors\nsq.gr.spatial_neighbors(adata)\n\n# Step 4: Calculate neighborhood enrichment\nsq.gr.nhood_enrichment(adata, cluster_key=\"cluster\")\n\n# Step 5: Visualize the neighborhood enrichment results\nsq.pl.nhood_enrichment(adata, cluster_key=\"cluster\")\n\n# Step 6: Calculate Ripley’s K statistic\nsq.gr.ripley(adata, cluster_key=\"cluster\", mode=\"K\", max_dist=500)\nsq.pl.ripley(adata, cluster_key=\"cluster\", mode=\"K\")\n\n# Step 7: Calculate ligand - receptor interactions\nsq.gr.ligrec(adata, n_perms=100, cluster_key=\"cluster\", clusters=[\"Polydendrocytes\", \"Oligodendrocytes\"])\nsq.pl.ligrec(adata, cluster_key=\"cluster\", source_groups=\"Oligodendrocytes\", target_groups=[\"Polydendrocytes\"], pvalue_threshold=0.05, swap_axes=True)\n\n# Step 8: Calculate spatial autocorrelation\nsq.gr.spatial_autocorr(adata, mode=\"moran\")\nprint(adata.uns[\"moranI\"].head(10))",
        "metadata":{
            "source": "Squidpy",
            "page":35
        }
    },
    {
        "content": "Scanpy:# Step 1: Load the example dataset\nadata = sc.datasets.pbmc3k()\n\n# Step 2: Preprocess the data\nsc.pp.filter_genes(adata, min_cells=3)\nsc.pp.normalize_total(adata)\nsc.pp.log1p(adata)\nsc.pp.highly_variable_genes(adata, flavor=\"seurat\", n_top_genes=2000)\nadata = adata[:, adata.var.highly_variable]\n\n# Step 3: Calculate spatial neighbors\nsc.pp.neighbors(adata)\n\n# Step 4: Calculate neighborhood enrichment\nsc.tl.neighborhood_enrichment(adata, cluster_key=\"louvain\")\n\n# Step 5: Visualize the neighborhood enrichment results\nsc.pl.neighborhood_enrichment(adata, cluster_key=\"louvain\")\n\n# Step 6: Calculate Ripley’s K statistic\nsc.tl.ripley(adata, cluster_key=\"louvain\", mode=\"K\", max_dist=500)\nsc.pl.ripley(adata, cluster_key=\"louvain\", mode=\"K\")\n\n# Step 7: Calculate ligand - receptor interactions\nsc.tl.ligrec(adata, n_perms=100, cluster_key=\"louvain\", clusters=[\"CD14+ Monocytes\", \"CD4+ T cells\"])\nsc.pl.ligrec(adata, cluster_key=\"louvain\", source_groups=\"CD4+ T cells\", target_groups=[\"CD14+ Monocytes\"], pvalue_threshold=0.05, swap_axes=True)\n\n# Step 8: Calculate spatial autocorrelation\nsc.tl.spatial(adata)\nprint(adata.uns[\"spatial\"].head(10))",
        "metadata":{
            "source": "Scanpy",
            "page":36
        }
    },
    {
        "content":"spaceranger:spaceranger is a set of analysis pipelines that process 10x Genomics Visium data with brightfield or fluorescence microscope images, allowing users to map the whole transcriptome in a variety of tissues. This tool has been installed,just use spaceranger command dierectly,do not download spaceranger repeatly.e.g., spaceranger count --id=Visium_FFPE_Mouse_Brain_RUN1 \n--create-bam=false \n --transcriptome=/path/to/refdata-gex-mm10-2020-A \n—-probe-set=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_probe_set.csv \n --fastqs=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_fastqs \n--image=/path/to/adult_mouse_brain_FFPE/Visium_FFPE_Mouse_Brain_image.jpg \n--slide=V11J26-127 \n--area=B1",
        "metadata":{
            "source": "spaceranger",
            "page":37
        }
    },
    {
        "content": "To install this package run one of the following:conda install bioconda::rmats conda install bioconda/label/cf201901::rmats ,rMATS: rMATS (replicate Multivariate Analysis of Transcript Splicing) is a computational tool designed for detecting differential alternative splicing events from RNA-Seq data with biological replicates. It quantifies splicing variations by computing inclusion levels and applying a likelihood-ratio test for statistical significance. \n\n### Supported Alternative Splicing Events:\n- **Skipped exon (SE)** - Exon skipping events.\n- **Alternative 5' splice site (A5SS)** - Variable splice site at the 5' end.\n- **Alternative 3' splice site (A3SS)** - Variable splice site at the 3' end.\n- **Mutually exclusive exons (MXE)** - Selection between two alternative exons.\n- **Retained intron (RI)** - Intron retention events.\n\n### Dependencies:\n- Python (>=3.6.12 or 2.7.15)\n- Cython (0.29.21 or 0.29.15 for Python 2)\n- numpy (>=1.16.6)\n- BLAS, LAPACK\n- GNU Scientific Library (GSL 2.5)\n- GCC (>=5.4.0)\n- gfortran (Fortran 77)\n- CMake (3.15.4)\n- PAIRADISE (optional)\n- Samtools (optional)\n- STAR (optional)\n\n### Installation & Setup:\n```bash\n# Build rMATS with required dependencies\n./build_rmats --conda\n\n# Running rMATS with conda environment\n./run_rmats {arguments}\n```\n\n### Usage:\n#### **Running with FASTQ files**\n1. Prepare sample lists in a text file:\n```bash\n# Sample 1 FASTQ list (paired-end reads separated by ':', replicates by ',')\necho '/path/to/1_1.R1.fastq:/path/to/1_1.R2.fastq,/path/to/1_2.R1.fastq:/path/to/1_2.R2.fastq' > s1.txt\n\necho '/path/to/2_1.R1.fastq:/path/to/2_1.R2.fastq,/path/to/2_2.R1.fastq:/path/to/2_2.R2.fastq' > s2.txt\n```\n2. Run rMATS with FASTQ input:\n```bash\npython rmats.py --s1 s1.txt --s2 s2.txt --gtf annotation.gtf --bi STAR_index/ -t paired --readLength 50 --nthread 4 --od results/ --tmp temp/\n```\n\n#### **Running with BAM files**\n1. Prepare BAM file lists:\n```bash\necho '/path/to/1_1.bam,/path/to/1_2.bam' > b1.txt\necho '/path/to/2_1.bam,/path/to/2_2.bam' > b2.txt\n```\n2. Run rMATS with BAM input:\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od results/ --tmp temp/\n```\n\n### **Running rMATS in Steps:**\n- **Prep phase** (extract and process alignments):\n```bash\npython rmats.py --b1 b1.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_prep --task prep\n```\n- **Post phase** (calculate statistical significance):\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_post --task post\n```\n\n### **Statistical Model Options:**\n- **Enable paired statistics model**:\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_post --paired-stats\n```\n- **Skip statistical testing (only generate raw splicing events):**\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_post --statoff\n```\n\n### **Output Files:**\n- `[AS_Event].MATS.JC.txt` - Results including only junction-spanning reads.\n- `[AS_Event].MATS.JCEC.txt` - Results including junction and exon-spanning reads.\n- `fromGTF.[AS_Event].txt` - Identified alternative splicing events from annotation.\n- `JC.raw.input.[AS_Event].txt` - Raw count data for junction counts.\n- `JCEC.raw.input.[AS_Event].txt` - Raw count data including exon counts.\n\n### **Additional Features:**\n- **Detect novel splice sites:**\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_post --novelSS\n```\n- **Use predefined event set:**\n```bash\npython rmats.py --b1 b1.txt --b2 b2.txt --gtf annotation.gtf -t paired --readLength 50 --nthread 4 --od output/ --tmp tmp_post --fixed-event-set events_dir/\n```\n\n### **Reference Documentation:**\nFor more details, refer to the rMATS official documentation and command-line help:\n```bash\npython rmats.py -h\n```\n",
        "metadata": {
            "source": "rMATS",
            "page": 38
        }
    },
    {
        "content": "To install this package run one of the following:\nconda install -c bioconda bedtools\n\nBEDTools: BEDTools is a powerful suite of utilities for comparing, manipulating, and analyzing genomic intervals in the popular BED, GFF, and VCF formats. It is widely used for a variety of tasks including interval manipulation, intersection analysis, and genomic file format conversion. BEDTools supports many types of genomic file formats, allowing users to easily perform complex analyses on genomic data.\n\n### Supported Operations:\n- **Intersect**: Find common regions between multiple files.\n- **Merge**: Combine overlapping or adjacent intervals into a single interval.\n- **Closest**: Find the closest feature from a set of intervals.\n- **Subtract**: Subtract one set of intervals from another.\n- **Shuffle**: Randomly shuffle intervals while preserving their length.\n- **Map**: Map a set of intervals to another file to extract values.\n- **Split**: Split intervals into smaller chunks based on specific criteria.\n\n### Dependencies:\n- Python (>=3.6)\n- GCC (>=5.4.0)\n- CMake (>=3.10.2)\n\n### Installation & Setup:\n```bash\n# Install BEDTools using conda\nconda install -c bioconda bedtools\n```\n\n### Usage:\n#### **Intersecting two BED files**\n```bash\nbedtools intersect -a file1.bed -b file2.bed > output.bed\n```\n\n#### **Merging overlapping intervals**\n```bash\nbedtools merge -i input.bed > merged.bed\n```\n\n#### **Find the closest feature**\n```bash\nbedtools closest -a file1.bed -b file2.bed > closest.bed\n```\n\n#### **Subtract one set of intervals from another**\n```bash\nbedtools subtract -a file1.bed -b file2.bed > result.bed\n```\n\n#### **Shuffle intervals randomly**\n```bash\nbedtools shuffle -i input.bed -g genome.txt > shuffled.bed\n```\n\n#### **Map intervals to another file**\n```bash\nbedtools map -a input.bed -b values.bed > mapped.bed\n```\n\n### **Advanced Features:**\n- **Multi-threading support**: BEDTools allows parallel processing for large genomic datasets.\n- **Custom genome definitions**: Users can specify custom genome definitions for operations like shuffling or calculating closest distances.\n\n### **Output Files:**\n- **output.bed**: The resulting file after performing an operation, typically in BED format.\n- **merged.bed**: The output file containing merged intervals from the input file.\n\n### **Reference Documentation:**\nFor more details, refer to the official BEDTools documentation and command-line help:\n```bash\nbedtools --help\n```",
        "metadata": {
            "source": "BEDTools",
            "page": 39
        }
    },
    {
        "content": "To install Bowtie2, run one of the following commands:\n\n### Installation Instructions:\n```bash\n# Install Bowtie2 using conda\nconda install -c bioconda bowtie2\n\n# Or install via package manager on Linux\nsudo apt-get install bowtie2\n```\n\n### Bowtie2 Overview:\nBowtie2 is a fast and efficient aligner that aligns short reads to a reference genome. It can handle paired-end and single-end read files in FASTQ format, and produces output in the widely used SAM format. It is optimized for aligning short DNA reads with high accuracy.\n\n### Supported Features:\n- **Single-End Read Alignment**: Aligns single-end reads to a reference genome.\n- **Paired-End Read Alignment**: Aligns paired-end reads to a reference genome.\n- **Output in SAM format**: SAM format is widely used for storing sequence alignments.\n- **Multithreading support**: Bowtie2 supports multithreading to speed up the alignment process.\n\n### Installation & Setup:\n```bash\n# Install Bowtie2 using conda\nconda install -c bioconda bowtie2\n```\n\n### Basic Usage:\n#### **Aligning single-end reads**\n```bash\nbowtie2 -x reference_genome -U reads.fastq -S output.sam\n```\n#### **Aligning paired-end reads**\n```bash\nbowtie2 -x reference_genome -1 reads_1.fastq -2 reads_2.fastq -S output.sam\n```\n\n#### **Aligning with multithreading (using 4 threads)**\n```bash\nbowtie2 -x reference_genome -U reads.fastq -S output.sam -p 4\n```\n\n### Output Files:\n- **output.sam**: The resulting alignment file in SAM format. It contains the aligned reads and their respective positions in the reference genome.\n\n### **Advanced Features:**\n- **Bowtie2 Indexing**: Before aligning, you need to index the reference genome with the following command:\n  ```bash\n  bowtie2-build reference_genome.fa reference_genome\n  ```\n- **Handling Large Datasets**: Bowtie2 is optimized for large datasets, and its multithreading support helps improve performance on large genome alignments.\n\n### **Reference Documentation:**\nFor more details on Bowtie2, refer to the official documentation and help command:\n```bash\nbowtie2 --help\n```",
        "metadata": {
          "source": "Bowtie2",
          "page": 40
        }
    },
    {
        "content": "To run PureCLIP in basic mode, it requires BAM and BAI files, the reference genome and a specified output file:\n\n```bash\npureclip -i aligned.prepro.R2.bam -bai aligned.prepro.R2.bam.bai -g ref.hg19.fa -iv 'chr1;chr2;chr3;' -nt 10 -o PureCLIP.crosslink_sites.bed\n```\n\nWith `-iv` the chromosomes (or transcripts) can be specified that will be used to learn the parameters of PureCLIP’s HMM. This reduces the memory consumption and runtime. \n\n### Input Control Mode\nTo run PureCLIP with input control data:\n\n```bash\npureclip -i aligned.prepro.R2.bam -bai aligned.prepro.R2.bam.bai -g ref.hg19.fa -iv 'chr1;chr2;chr3;' -nt 10 -o PureCLIP.crosslink_sites\n```\n\n### CL-Motif Scores\nTo address crosslinking sequence bias, CL-motif scores can be incorporated.\n\n#### Compute CL-Motif Scores:\n```bash\nwget -O motifs.txt https://raw.githubusercontent.com/skrakau/PureCLIP_data/master/common_CL-motifs/dreme.w10.k4.txt\nwget -O motifs.xml https://raw.githubusercontent.com/skrakau/PureCLIP_data/master/common_CL-motifs/dreme.w10.k4.xml\ncompute_CLmotif_scores.sh ref.hg19.fa aligned.prepro.R2.bam motifs.xml motifs.txt fimo_clmotif_occurences.bed\n```\n\n#### Run PureCLIP with CL-Motif:\n```bash\npureclip -i aligned.prepro.R2.bam -bai aligned.prepro.R2.bam.bai -g ref.hg19.fa -o PureCLIP.crosslink_sites.cov_CLmotifs.bed -nt 10 -iv 'chr1;chr2;chr3;' -nim 4 -fis fimo_clmotif_occurences.bed\n```\n\n### Output\n- **Crosslink sites**: BED6 with chr, start, end, state, score, strand.\n- **Binding regions** (optional): merged crosslink sites with scores if `-or` and `-dm` provided.",
        "metadata": {
          "source": "pureclip",
          "page": 41
        }
    },
    {
        "content": "ribotricer: Accurate detection of short and long active ORFs using Ribo-seq data\n\nribotricer is a tool that helps in detecting actively translating open reading frames (ORFs) from Ribo-seq data. It works by assessing the periodicity of candidate ORFs. To run ribotricer, the following files are required:\n\n1. **Genome annotation file** in GTF format (handles variations of GTFs including GENCODE and Ensembl)\n2. **Reference genome file** in FASTA format\n3. **Alignment file** in BAM format\n\n### Installation\nTo install ribotricer using conda, execute:\n\n```bash\nconda create -n ribotricer_env -c bioconda ribotricer\nconda activate ribotricer_env\nribotricer --help\n```\n\nAlternatively, to install locally, download the source code and run:\n\n```bash\nmake install\n```\n\n### Workflow\n1. **Preparing candidate ORFs**\n\n   Run ribotricer to find all candidate ORFs from the GTF and FASTA files:\n   ```bash\n   ribotricer prepare-orfs --gtf {GTF} --fasta {FASTA} --prefix {RIBOTRICER_INDEX_PREFIX}\n   ```\n   This generates a file with candidate ORFs: `{PREFIX}_candidate_orfs.tsv`.\n\n2. **Detecting translating ORFs**\n\n   Detect active translation from BAM file:\n   ```bash\n   ribotricer detect-orfs --bam {BAM} --ribotricer_index {RIBOTRICER_INDEX_PREFIX}_candidate_ORFs.tsv --prefix {OUTPUT_PREFIX}\n   ```\n   This will generate output such as `{OUTPUT_PREFIX}_translating_ORFs.tsv`.\n\n### Additional Features\n- **Learn cutoff empirically from data** using both Ribo-seq and RNA-seq BAM files.\n- **Visualization** of ribotricer output available in the provided notebooks.\n\nFor more details and troubleshooting, refer to the [documentation](https://github.com/smithlabcode/ribotricer/issues).\n\n### Dependencies\n- pyfaidx>=0.5.0\n- pysam>=0.11.2.2\n- numpy>=1.11.0\n- pandas>=0.20.3\n- scipy>=0.19.1\n- matplotlib>=2.1.0\n",
        "metadata": {
            "source": "ribotricer",
            "page": 42
        }
    },
    {
        "content": "### 1. Genome Index Generation\nTo use STAR, you first need to generate genome indexes from reference genome FASTA and annotation GTF files:\n\n```bash\nSTAR --runThreadN 8 \\\n     --runMode genomeGenerate \\\n     --genomeDir /path/to/genomeDir \\\n     --genomeFastaFiles /path/to/genome.fa \\\n     --sjdbGTFfile /path/to/annotations.gtf \\\n     --sjdbOverhang 99\n```\n\n**Option explanations:**\n- `--runThreadN`: Number of threads to use. Should match the available CPU cores.\n- `--runMode genomeGenerate`: Tells STAR to generate genome indices.\n- `--genomeDir`: Directory where the genome index will be saved. Must be created beforehand and have write permission.\n- `--genomeFastaFiles`: Reference genome file(s) in FASTA format.\n- `--sjdbGTFfile`: Gene annotations in GTF format (optional but highly recommended).\n- `--sjdbOverhang`: ReadLength - 1. For example, for 100 bp reads, use 99.\n\nMake sure the chromosome names in the GTF and FASTA files match exactly (e.g. both use `chr1`, `chr2`, etc.).\n\n### 2. Read Mapping\nAfter index generation, map reads using the following command:\n\n```bash\nSTAR --runThreadN 8 \\\n     --genomeDir /path/to/genomeDir \\\n     --readFilesIn /path/to/read1.fq /path/to/read2.fq \\\n     --outFileNamePrefix /path/to/output/sample_\n```\n\n**Optional flags:**\n- `--readFilesCommand zcat`: For gzipped files.\n- `--outSAMtype BAM SortedByCoordinate`: Direct output to sorted BAM file.\n- `--outSAMunmapped Within`: Include unmapped reads in BAM.\n- `--quantMode GeneCounts`: Count reads per gene.\n\n### 3. Output Files\n- `Aligned.out.sam`: default alignment in SAM format.\n- `Aligned.sortedByCoord.out.bam`: coordinate-sorted BAM output (if specified).\n- `ReadsPerGene.out.tab`: gene-level read count (if `--quantMode GeneCounts` used).\n- `SJ.out.tab`: detected splice junctions.\n- `Log.out`, `Log.progress.out`, `Log.final.out`: run logs and summary statistics.\n\n### 4. Example Workflow (Paired-End gzipped FASTQ):\n```bash\nSTAR --runThreadN 12 \\\n     --genomeDir ./GenomeDir \\\n     --readFilesIn sample_R1.fastq.gz sample_R2.fastq.gz \\\n     --readFilesCommand zcat \\\n     --outFileNamePrefix ./output/sample_ \\\n     --outSAMtype BAM SortedByCoordinate \\\n     --quantMode GeneCounts\n```\n\nThis will map reads to genome, output a sorted BAM file, and generate gene count summary.",
        "metadata": {
          "source": "STAR",
          "page": 43
        }
    },
    {
        "content": "Small RNA sequencing and miRNA prediction workflow consists of the following key steps:\n\nStep 1: Quality Control and Trimming\nDescription: Perform quality control on raw sequencing reads, remove low-quality reads, and trim sequencing adapters.\nCommands:\n  fastqc ./data/*.fastq.gz -o ./output/010/qc_reports/\n  fastp -i ./data/C1.fastq.gz -o ./output/010/trimmed_C1.fastq.gz\n\nStep 2: Genome Alignment\nDescription: Build index from genome and align reads with Bowtie1. Convert to sorted BAM.\nCommands:\n  bowtie-build ./data/genome.fa ./output/010/genome_index\n  bowtie -S -p 16 ./output/010/genome_index ./output/010/trimmed_C1.fastq.gz > ./output/010/C1.sam\n  samtools view -bS ./output/010/C1.sam | samtools sort -o ./output/010/aligned_C1.bam\n  samtools index ./output/010/aligned_C1.bam\nCheckpoint: Use `samtools flagstat` to verify alignments are not empty.\n\nStep 3: miRNA Quantification\nDescription: Use featureCounts with a SAF annotation file.\nCommands:\n  python convert_fasta_to_saf.py -i ./data/miRBase_mature.fasta -o ./data/miRBase_mature.saf\n  featureCounts -T 4 -F SAF -a ./data/miRBase_mature.saf -o ./output/010/miRNA_expression_matrix.txt ./output/010/aligned_C1.bam\nCheckpoint: Ensure expression matrix has non-zero counts.\n\nStep 4: Novel miRNA Discovery\nDescription: Collapse reads and predict novel miRNAs with miRDeep2.\nCommands:\n  mapper.pl ./output/010/trimmed_C1.fastq.gz -e -h -m -l 18 -s ./output/010/reads_collapsed.fa -t ./output/010/reads.arf -r ./data/genome.fa\n  miRDeep2.pl ./output/010/reads_collapsed.fa ./data/genome.fa ./output/010/reads.arf ./data/miRBase_mature.fasta ./data/miRBase_hairpin.fasta -t Human > ./output/010/miRDeep2_results.txt\nCheckpoint: Ensure result file contains predicted miRNAs.",
        "metadata": {
          "source": "workflow",
          "page": 44
        }
    },
    {
        "content": "Step 3: miRNA Quantification\nDescription: Quantify miRNA expression using featureCounts and SAF-format annotation derived from known miRNA sequences.\nInput Required: Aligned BAM files, miRNA annotation in SAF format.\nExpected Output: Expression matrix (miRNA counts).\nTools Used: featureCounts.\nCommands:\n  ```bash\n  # Convert FASTA to SAF\n  python ./scripts/convert_fasta_to_saf.py -i ./data/miRBase_mature.fasta -o ./data/miRBase_mature.saf\n  \n  # Run featureCounts\n  featureCounts -F SAF -T 4 -a ./data/miRBase_mature.saf -o ./output/010/miRNA_expression_matrix.txt ./output/010/aligned_C1.bam\n  ```\n**Checkpoint:** Check that `miRNA_expression_matrix.txt` has non-zero counts.",
        "metadata": {
          "source": "workflow",
          "page": 45
        }
    },
    {
        "content": "If miRBase_mature.fasta is available, convert it to SAF format using the provided script. Command: `python ./scripts/convert_fasta_to_saf.py -i ./data/miRBase_mature.fasta -o ./data/miRBase_mature.saf`. Alternatively, the user may prepare SAF manually before running featureCounts. Required SAF fields are: GeneID, Chr, Start, End, Strand.",
        
        "metadata": {
          "source": "task",
          "page": 46
        }
    },
    {
        "content": "Tool: STAR\nPurpose: Align 3′ end-seq reads to a reference genome with high precision.\nInput: Trimmed FASTQ file and genome index\nOutput: Sorted BAM file\nCommand Example:\n  STAR --runThreadN 4 --genomeDir ./genome_index --readFilesIn trimmed.fastq.gz --readFilesCommand zcat --outFileNamePrefix ./output/sample --outSAMtype BAM SortedByCoordinate",
        "metadata": {
          "source": "task",
          "page": 47
        }
    },
    {
        "content": "Tool: bedtools genomecov\nPurpose: Generate base-level coverage profile from BAM file to infer PAS sites.\nInput: BAM file\nOutput: Coverage profile\nCommand Example:\n  bedtools genomecov -ibam aligned.bam -d > coverage.txt",
        "metadata": {
          "source": "task",
          "page": 48
        }
    },
    {
        "content": "Tool: filter_pas_peaks.py\nPurpose: Custom Python script to detect PAS peaks from coverage profile. Automatically selects regions with local maxima at 3′ ends.\nInput: Coverage profile (text)\nOutput: PAS_sites.bed\nCommand Example:\n  python filter_pas_peaks.py --input coverage.txt --output PAS_sites.bed",
        "metadata": {
          "source": "task",
          "page": 49
        }
    },
    {
        "content": "Tool: bedtools genomecov\nPurpose: Generate base-level coverage profile from BAM file to infer PAS sites.\nInput: BAM file\nOutput: Coverage profile\nCommand Example:\n  bedtools genomecov -ibam aligned.bam -d > coverage.txt",
        "metadata": {
          "source": "task",
          "page": 50
        }
    },
    {
        "content": "Tool: extract_3p_end\nPurpose: Extract the last 20 bases (3′ end) from each read in a trimmed FASTQ file without using a reference genome.\nInput: Trimmed FASTQ file\nOutput: FASTA file containing 3′ end sequences\nNote: This is an internal logic operation for extracting sequence tails, not a script call.",
        "metadata": {
          "source": "extract_3p_end",
          "page": 51
        }
    },
    {
        "content": "Tool: featureCounts\nPurpose: Count reads mapped to genomic features such as genes or exons for isoform or gene expression quantification.\nInput: Sorted and indexed BAM file, GTF annotation file\nOutput: Read count matrix\nCommand Example: featureCounts -a annotation.gtf -o counts.txt aligned.bam\nNote: Supports long-read BAM input as long as it's properly aligned and sorted.",
        "metadata": {
          "source": "featureCounts",
          "page": 52
        }
    },
    {
        "content": "The single-cell data marker gene identification workflow includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using scanpy.pp.calculate_qc_metrics, scanpy.pp.filter_cells, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbors, output an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, outputting cell cluster labels and visualization plots (.png); Step 6: Whole-Cell Differential Expression Analysis, applying tools to compare group differences at the whole-cell level, inputting normalized expression matrices and grouping information, using Scanpy's rank_genes_groups, sc.tl.rank_genes_groups(adata, groupby='leiden', method='t-test'),Extract the differential expression results, outputting a table of differentially expressed genes (DEGs) with with colums of log2FC, p-values, and adjusted p-values, and respectively rename the columns to logFC, pvals, and adj_pvals, df.rename(columns={'pvals_adj':'adj_pvals','logfoldchanges':'logFC'}, inplace=True); Step 7: Result Filtering, filtering marker genes based on significance thresholds adj_pvals, logFC, output high-confidence marker gene lists with adj_pvals < 0.05 and logFC > 1", 
               "metadata": {
            "source": "single-cell data marker gene identification",
            "page": 53
        }
    },
    {
        "content": "Single-cell full-process analysis with top marker genes includes Data Reading and Initialization, Quality Control (QC), Normalization and Highly Variable Gene Selection, Dimensionality Reduction and Visualization, Clustering Analysis, Whole-Cell Differential Expression Analysis, Result Filtering and Top Marker Gene Extraction, Cell Type Annotation & Functional Enrichment Analysis, Purity Analysis, Functional Heterogeneity Analysis, Regulatory Network Analysis includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using scanpy.pp.calculate_qc_metrics, scanpy.pp.filter_cells, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbors, output an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, outputting cell cluster labels and visualization plots (.png); Step 6: Whole-Cell Differential Expression Analysis, applying tools to compare group differences at the whole-cell level, inputting normalized expression matrices and grouping information, using Scanpy's rank_genes_groups, sc.tl.rank_genes_groups(adata, groupby='leiden', method='t-test'),Extract the differential expression results, outputting a table of differentially expressed genes (DEGs) with with colums of log2FC, p-values, and adjusted p-values, and respectively rename the columns to logFC, pvals, and adj_pvals, df.rename(columns={'pvals_adj':'adj_pvals','logfoldchanges':'logFC'}, inplace=True); Step 7: Result Filtering, filtering marker genes based on significance thresholds adj_pvals, logFC, output high-confidence marker gene lists with adj_pvals < 0.05 and logFC > 1; Step 8 Cell Type Annotation, input high-confidence marker gene list (.csv with logFC and adj_pval fields) and a raw AnnData object (containing a normalized expression matrix and cluster labels). import scanpy as sc, import pandas as pd, import celltypist, from celltypist import models, load the clustered .h5ad and csv data. for example, adata = sc.read_h5ad(.h5ad), marker_genes = pd.read_csv('.csv', header=0) , usually the gene name is the second column of the csv file; then Automated annotation (based on CellTypist): models.download_models(), predictions = celltypist.annotate(adata, model=), adata = predictions.to_adata(), then verification by UMAP annotation overlay sc.pl.umap(adata, color=['cell_type', 'leiden'], legend_loc='on data', frameon=False), output adata.write('annotated_singlecellexpress.h5ad') and umap plot files; step 9 Pseudotime trajectory analysis, input the Annotated AnnData (with cell_type column) and subpopulation labels (e.g., leiden_clusters), then use scanpy tool to deal with it. sc.tl.diffmap(adata), sc.pl.diffmap(adata, color='leiden_clusters'), Output: UMAP plot overlaid with Diffusion Map trajectory(.png); step 10 Purity Analysis (Subpopulation Homogeneity Evaluation), import scanpy as sc, import numpy as np, from scipy.stats import gini, Calculate the Gini coefficient of each subpopulation (measuring gene expression dispersion), adata.var['gini_index'] = adata.to_df().apply(gini, axis=0), output gini_scores.csv, Filter genes with Gini > 0.7 and mark them as highly heterogeneous genes, cluster_gini = adata.obs.groupby('leiden_clusters').apply(lambda x: (x[:, adata.var['gini_index']>0.7].mean(axis=1))), cluster_gini.to_csv('cluster_purity.csv'), output cluster_purity.csv; step 11 Functional Heterogeneity Analysis, Differential gene screening Use the built-in method of Scanpy to quickly screen subpopulation marker genes, sc.tl.rank_genes_groups(adata, groupby='leiden_clusters', method='wilcoxon'),markers = sc.get.rank_genes_groups_df(adata, group=None), markers.to_csv('cluster_markers.csv'); step 12 Regulatory Network Analysis, Goal Predict core transcription factors (TF). first make sure the python version is 3.10.16, if not then conda install python==3.10.16, and then prepare proper version of pakage pip install scanpy==1.9.0 numba==0.56.4 numpy==1.23.5 anndata==0.8.0, TF activity score Quick scoring using a precompiled TF target gene database (such as DoRothEA), import decoupler as dc, net = dc.get_dorothea(organism='human', levels=['A','B']) , Infer TF activity using a multivariate linear model, dc.run_mlm(adata,dorothea_net, source='source', target='target', use_raw=False), do not put the key_added in dc.run_mlm ! Extract TF activity matrix, tf_activity_df = pd.DataFrame(adata.obsm['X_diffmap'],index=adata.obs.index), tf_activity_df.to_csv(), import pandas as pd, Correlate TF activity with gene expression, genes_expr_df = adata.to_df(), big_df = pd.concat([tf_activity_df, genes_expr_df], axis=1), corr_matrix = big_df.corr().loc[tf_activity_df.columns, genes_expr_df.columns], Save the regulatory network (TF-gene correlation) to CSV, corr_matrix.to_csv(), Output: tf_activity.csv (TF activity matrix for each cell), tf_gene_corr.csv (TF-gene correlation, |r|>0.3 is considered associated)", 
               "metadata": {
            "source": "Single cell full-process analysis with top marker genes",
            "page": 54
        }
    },
    {
        "content": "Single-cell perform clustering analysis includes the following steps: Step 1: Data Reading and Initialization, reading .h5ad or 10x Genomics matrix.mtx files using scanpy.read_10x_mtx or scanpy.read_h5ad to generate an AnnData object containing the raw gene expression matrix (rows=genes, columns=cells) and metadata; Step 2: Quality Control (QC), filtering low-quality cells based on mitochondrial gene percentage (≤20%), total UMIs (≥500), and detected genes using scanpy.pp.calculate_qc_metrics, scanpy.pp.filter_cells, outputting filtered high-quality data;Step 3: Normalization and Highly Variable Gene Selection, standardizing QC-processed data via scanpy.pp.normalize_total, scanpy.pp.log1p, and scanpy.pp.highly_variable_genes to obtain a normalized expression matrix and a list of highly variable genes (HVGs); Step 4: Dimensionality Reduction and Visualization, performing PCA dimensionality reduction using scanpy.tl.pca, constructing a KNN neighborhood graph with scanpy.pp.neighbors, output an AnnData object with dimension-reduced results; Step 5: Clustering Analysis, applying Leiden algorithm via scanpy.tl.leiden and UMAP/t-SNE visualization with scanpy.tl.umap based on dimension-reduced data, sc.tl.leiden(adata, resolution=), sc.pl.umap(adata, color='leiden', show=False, save='cluster_plots.png'), adata.write('./output/019/adata_clustered.h5ad'), outputting cell cluster labels and visualization plots (.png);", 
               "metadata": {
            "source": "Single-cell perform clustering analysis",
            "page": 55
        }
    },
    {
        "content": "Before performing nanopore data process (except for modification identification, for example, identifying methylation) from nanopore files. If basecalling file (fastq/bam) is not supplied, firstly using basecaller.sh command to do simplex basecalling to generate basecalling file before other steps.\nInput : a file folder containing nanopore files in fast5/pod5 format and a reference genome fa file\nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/basecaller.sh <pod5dir> <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# out : name of output bam file.",
            "metadata": {
            "source": "dorado",
            "page": 56
        }
        
    },
     {
        "content": "To extract signal from nanopore files. Each step of the process has been organized into specific executable scripts. The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order.",
            "metadata": {
            "source": "nanopore extract signal",
            "page": 57
        }
        
    },
    {
        "content": "1. map_fastq2_ref.sh:\nMap combined fastq file to reference genome(fa file). Generates sam file.\nInput : a combined nanopore fastq file and a reference genome fa file\nOutput : a sam file \nUsage\nRun the following in the container.\n./scripts/map_fastq2_ref.sh <fq> <output_sam> <ref> <nThreads>\n# fq : input fastq file.\n# output_sam : name of output sam file.\n# ref : reference genome fa file\n# nThreads : number of threads\n",
            "metadata": {
            "source": "map_fastq2_ref.sh",
            "page": 58
        }
        
    },
    {
        "content": "2. indexbam.sh:\nindex bam and generate statistics - coverage, error rate, number of reads.\nInput: a sam file\nOutput: a sorted bam file\nUsage\nRun the following in the container\n./scripts/indexbam.sh <nthread> <input_sam> <outputbam>\n# nthread : number of threads to use\n# input_sam : an input sam file\n# outputbam : name of output bam file\n",
            "metadata": {
            "source": "indexbam.sh",
            "page": 59
        }
        
    },
    {
        "content": "3. eventalign.sh:\nindex fast5 to fastq with nanopolish index tool and run nanopolish for fastq files to compute an improved consensus sequence from the draft genome assembly produced by minimap2, thus, get the signal.\nInput: a combined nanopore fastq file , a file folder containing nanopore files in fast5 format and a reference genome fa file\nOutput: a summary file in format sum and a output txt file\nUsage\nRun the following in the container\n./scripts/eventalign.sh <fq> <fast5dir> <sortedBam> <ref> <npsummary> <nThreads> <np_out>\n# fq: a combined fastq file.\n# fast5dir: Path of a file folder containing nanopore files in fast5 format.\n# sortedBam: a sorted bam file.\n# ref : reference genome fa file\n# nThreads : number of threads.\n# npsummary : name of merged summary file .\n# np_out : name of output txt file.",
            "metadata": {
            "source": "eventalign.sh",
            "page": 60
        }
    },
    {
        "content": "IF you: Please use this data to identify methylation from nanopore files. Each step of the process has been organized into specific executable scripts(2 steps). The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order.",
            "metadata": {
            "source": "nanopore methylation identification",
            "page": 61
        }
    },
    {
        "content": "1. methy_iden.sh:\n Do modified basecalling from nanopore pod5 files. \nInput : a file folder containing nanopore files in fast5/pod5 format and a reference genome fa file\nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/methy_iden.sh <pod5dir> <ref> <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# ref : reference genome fa file\n# out : name of output bam file.",
            "metadata": {
            "source": "methy_iden.sh",
            "page": 62
        }
    },
    {
        "content": "2. basecall_sum.sh:\n output a tab-separated file with read level sequencing information from the bam file. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n./scripts/basecall_sum.sh <bam> <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.",
            "metadata": {
            "source": "basecall_sum.sh",
            "page": 63
        }
    },
    {
        "content": "IF you: Please use this data to do complete quantitative analysis for nanopore files. Each step of the process has been organized into specific executable scripts(13 steps). The bash tool is used. All the scripts are in the folder ./scripts/. The steps must be generated in strict order.",
            "metadata": {
            "source": "nanopore complete quantitative analysis",
            "page": 64
        }
    },
    {
        "content": "1. basecaller.sh:\ndo simplex basecalling. \nInput : a file folder containing nanopore files in fast5/pod5 format\nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/basecaller.sh <pod5dir> <out>\n# pod5dir: Path of a file folder containing nanopore files in fast5/pod5 format.\n# out : name of output bam file.\n ",
            "metadata": {
            "source": "basecaller.sh",
            "page": 65
        }
    },
    {
        "content": "2. basecall_sum.sh:\noutput a tab-separated file with read level sequencing information from the bam file generated during basecalling. \nInput : basecaller’s output bam file \nOutput : a summary tsv file.\nUsage\nRun the following in the container\n./scripts/basecall_sum.sh <bam> <out>  \n# bam : basecaller’s output bam file.\n# out : name of output tsv file.\n",
            "metadata": {
            "source": "basecall_sum.sh",
            "page": 66
        }
    },
    {
        "content": "3. bam2fq.sh:\n transfer bam file to fastq format.\nInput : basecaller’s output bam file \nOutput : a fastq file.\nUsage\nRun the following in the container\n./scripts/bam2fq.sh <numcore> <bam> <out>  \n# numcore : number of threads\n# bam : basecaller’s output bam file.\n# out : name of output fastq file. \n",
            "metadata": {
            "source": "bam2fq.sh",
            "page": 67
        }
    },
    {
        "content": "4. raw_plot.sh:\n plot QC imformation from the basecall fastq file(raw_plot).\nInput : basecall fastq file \nOutput : sevaral png file and a report html file containing QC imformation.\nUsage\nRun the following in the container\n./scripts/raw_plot.sh <fastq> <outdir>  \n# fastq : basecall fastq file.\n# outdir : directory to store QC imformation files. \n",
            "metadata": {
            "source": "raw_plot.sh",
            "page": 68
        }
    },
    {
        "content": "5. filter.sh:\n filter low_quality reads in basecall fastq file.\nInput : basecall fastq file \nOutput : a filtered fastq file.\nUsage\nRun the following in the container\n./scripts/filter.sh <fastq> <out>  \n# fastq : basecall fastq file.\n# out : name of filtered fastq file. \n",
            "metadata": {
            "source": "filter.sh",
            "page": 68
        }
    },
    {
        "content": "6. host_removal.sh:\n remove host gene in filtered fastq file.\nInput : filtered fastq file \nOutput : a bam file.\nUsage\nRun the following in the container\n./scripts/host_removal.sh <fa> <fastq> <out>  # fa:  host gene fasta file.\n# fastq : filtered fastq file.\n# out : name of output bam file. \n",
            "metadata": {
            "source": "host_removal.sh",
            "page": 69
        }
    },
    {
        "content": "7. bam2fq.sh:\n transfer bam file to fastq format.\nInput : host-gene-removal bam file \nOutput : a fastq file.\nUsage\nRun the following in the container\n./scripts/bam2fq.sh <numcore> <bam> <out>  \n# numcore : number of threads\n# bam : host-gene-removal bam file.\n# out : name of output fastq file.  \n",
            "metadata": {
            "source": "bam2fq.sh",
            "page": 70
        }
    },
    {
        "content": "8. filtered_plot.sh:\n plot QC imformation from the host-gene-removal fastq file(filtered_plot).\nInput : host-gene-removal fastq file \nOutput : sevaral png file and a report html file containing QC imformation.\nUsage\nRun the following in the container\n./scripts/filtered_plot.sh <fastq> <outdir>  \n# fastq : host-gene-removal fastq file.\n# outdir : directory to store QC imformation files.  \n",
            "metadata": {
            "source": "filtered_plot.sh",
            "page": 71
        }
    },
    {
        "content": "9. map_fastq2_ref.sh:\nMap host-gene-removal fastq file to reference genome(fa file). Generates sam file.\nInput : a  host-gene-removal fastq file and a reference genome fa file\nOutput : a sam file \nUsage\nRun the following in the container.\n./scripts/map_fastq2_ref.sh <fq> <output_sam> <ref> <nThreads>\n# fq :  host-gene-removal fastq file.\n# output_sam : name of output sam file.\n# ref : reference genome fa file\n# nThreads : number of threads\n",
            "metadata": {
            "source": "map_fastq2_ref.sh",
            "page": 72
        }
    },
    {
        "content": "10. indexbam.sh:\nindex bam and generate statistics - coverage, error rate, number of reads.\nInput: a sam file\nOutput: a sorted bam file\nUsage\nRun the following in the container\n./scripts/indexbam.sh <nthread> <input_sam> <outputbam>\n# nthread : number of threads to use\n# input_sam : an input sam file\n# outputbam : name of output bam file\n",
            "metadata": {
            "source": "indexbam.sh",
            "page": 73
        }
    },
    {
        "content": "11. alignment_stats.sh:\nGet the alignment_stats report.\nInput : the sorted bam file\nOutput : a alignment_stats txt file \nUsage\nRun the following in the container.\n./scripts/alignment_stats.sh <bam> <out> \n# bam :  the sorted bam file.\n# out : name of alignment_stats report txt file.\n# ref : reference genome fa file\n# nThreads : number of threads \n",
            "metadata": {
            "source": "alignment_stats.sh",
            "page": 74
        }
    },
    {
        "content": "12. quantififation.sh:\nGenerate transcripts and  gene_abundance tsv file from the sorted bam file.\nInput : the sorted bam file\nOutput : a transcripts gtf file and a gene_abundance tsv file. \nUsage\nRun the following in the container.\n./scripts/quantififation.sh <bam> <transcripts_gtf> <out> \n# bam :  the sorted bam file. \n# transcripts_gtf :  name of output transcripts gtf file.\n# out : name of output gene_abundance tsv file.\n",
            "metadata": {
            "source": "quantififation.sh",
            "page": 75
        }
    },
    {
        "content": "13. result_sum.sh:\n make a new diroctory to store all the readable result. \nInput : path to raw_plot dir, filterd_plot dir, alignment_stats report txt file, gene_abundance tsv file and basecall summary tsv file, a new diroctory path.  \nUsage\nRun the following in the container.\n./scripts/result_sum.sh <dir1> <dir2> <alignment_stats> <gene_abundance> <basecall> <report_dir>\n# dir1 : path to raw_plot QC imformation dir \n# dir2 : path to filtered_plot QC imformation dir\n# alignment_stats : path to alignment_stats report txt file.\n# gene_abundance : path to gene_abundance tsv file.\n# basecall : path to  basecall summary tsv file. .\n# report_dir : path of new diroctory to store the results.",
            "metadata": {
            "source": "result_sum.sh",
            "page": 76
        }
    }           
]